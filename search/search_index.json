{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HelloDATA BE \ud83d\udc4b\ud83c\udffb","text":"<p>This is the open documentation about HelloDATA BE. We hope you enjoy it.</p> <p>Contribute</p> <p>In case something is missing or you'd like to add something, below is how you can contribute:</p> <ul> <li>Star our\u00a0GitHub \u2b50</li> <li>Want to discuss, contribute, or need help, create a GitHub Issue, create a Pull Request or open a Discussion.</li> </ul>"},{"location":"#what-is-hellodata-be","title":"What is HelloDATA BE?","text":"<p>HelloDATA BE is an\u00a0enterprise data platform\u00a0built on top of open-source tools based on the modern data stack. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p> <p>Each of these components is carefully chosen and additional tools can be added in a later stage.</p>"},{"location":"#why-do-you-need-an-open-enterprise-data-platform-hellodata-be","title":"Why do you need an Open Enterprise Data Platform (HelloDATA BE)?","text":"<p>These days the amount of data grows yearly more than the entire lifetime before. Each fridge, light bulb, or anything really starts to produce data. Meaning there is a growing need to make sense of more data. Usually, not all data is necessary and valid, but due to the nature of growing data, we must be able to collect and store them easily. There is a great need to be able to analyze this data. The result can be used for secondary usage and thus create added value.</p> <p>That is what this open data enterprise platform is all about. In the old days, you used to have one single solution provided; think of SAP or Oracle. These days that has completely changed. New SaaS products are created daily, specializing in a tiny little niche. There are also many open-source tools to use and get going with minutes freely.</p> <p>So why would you need a HelloDATA BE? It's simple. You want the best of both worlds. You want\u00a0open source\u00a0to not be locked-in, to use the strongest, collaboratively created product in the open. People worldwide can fix a security bug in minutes, or you can even go into the source code (as it's available for everyone) and fix it yourself\u2014compared to an extensive vendor where you solely rely on their update cycle.</p> <p>But let's be honest for a second if we use the latest shiny thing from open source. There are a lot of bugs, missing features, and independent tools. That's precisely where HelloDATA BE comes into play. We are building the\u00a0missing platform\u00a0that combines the best-of-breed open-source technologies into a\u00a0single portal, making it enterprise-ready by adding features you typically won't get in an open-source product. Or we fix bugs that were encountered during our extensive tests.</p> <p>Sounds too good to be true? Give it a try. Do you want to know the best thing? It's open-source as well. Check out our\u00a0GitHub HelloDATA BE.</p>"},{"location":"#quick-start-for-developers","title":"Quick Start for Developers","text":"<p>Want to run HelloDATA BE and test it locally? Run the following command in the docker-compose directory to deploy all components:</p> <pre><code>cd hello-data-deployment/docker-compose\ndocker-compose up -d\n</code></pre> <p>Note: Please refer to our docker-compose README for more information; there are some must presets you need to configure.</p>"},{"location":"architecture/architecture/","title":"Architecture","text":""},{"location":"architecture/architecture/#components","title":"Components","text":"<p>This chapter will explain the core architectural component, its context views, and how HelloDATA works under the hood.</p>"},{"location":"architecture/architecture/#domain-view","title":"Domain View","text":"<p>We separate between two main domains, \"Business and Data Domain.</p>"},{"location":"architecture/architecture/#business-vs-data-domain","title":"Business\u00a0vs. Data Domain","text":"<ul> <li>\"Business\" domain: This domain holds one customer or company with general services (portal, orchestration, docs, monitoring, logging). Every business domain represents a business tenant. HelloDATA is running on a Kubernetes cluster. A business domain is treated as a dedicated namespace within that cluster; thus, multi-tenancy is set up by various namespaces in the Kubernetes cluster.</li> <li>Data Domain: This is where actual data is stored (db-schema). We combine it with a superset instance (related dashboards) and the documentation about these data. Currently, a business domain relates 1 - n to its Data Domains. Within an existing Business Domain a Data Domain can be spawned using Kubernetes deployment features and scripts to set up the database objects.</li> </ul> <p>Resources encapsulated inside a\u00a0Data Domain\u00a0can be:</p> <ul> <li>Schema of the Data Domain</li> <li>Data mart tables of the Data Domain</li> <li>The entire DWH environment of the Data Domain</li> <li>Data lineage documents of the DBT projects of the Data Domain.     Dashboards, charts, and datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> <li>JupyterHub</li> </ul> <p>On top, you can add\u00a0subsystems. This can be seen as extensions that make HelloDATA pluggable with additional tools. We now support\u00a0CloudBeaver\u00a0for viewing your Postgres databases, RtD, and Gitea. You can imagine adding almost infinite tools with capabilities you'd like to have (data catalog, semantic layer, specific BI tool, Jupyter Notebooks, etc.).</p> <p>Read more about Business and Data Domain access rights in\u00a0Roles / Authorization Concept.</p> <p></p>"},{"location":"architecture/architecture/#data-domain","title":"Data Domain","text":"<p>Zooming into several Data Domains that can exist within a Business domain, we see an example of Data Domain A-C. Each Data Domain has a persistent storage, in our case, Postgres (see more details in the\u00a0Infrastructure Storage\u00a0chapter below).</p> <p>Each data domain might import different source systems; some might even be used in several data domains, as illustrated. Each Data Domain is meant to have its data model with straightforward to, in the best case, layered data models as shown on the image with:</p>"},{"location":"architecture/architecture/#landingstaging-area","title":"Landing/Staging Area","text":"<p>Data from various source systems is first loaded into the Landing/Staging Area.</p> <ul> <li>In this first area, the data is stored as it is delivered; therefore, the stage tables' structure corresponds to the interface to the source system.</li> <li>No relationships exist between the individual tables.</li> <li>Each table contains the data from the final delivery, which will be deleted before the next delivery.</li> <li>For example, in a grocery store, the Staging Area corresponds to the loading dock where suppliers (source systems) deliver their goods (data). Only the latest deliveries are stored there before being transferred to the next area.</li> </ul>"},{"location":"architecture/architecture/#data-storage-cleansing-area","title":"Data Storage (Cleansing Area)","text":"<p>It must be cleaned before the delivered data is loaded into the Data Processing (Core). Most of these cleaning steps are performed in this area.</p> <ul> <li>Faulty data must be filtered, corrected, or complemented with singleton (default) values.</li> <li>Data from different source systems must be transformed and integrated into a unified form.</li> <li>This layer also contains only the data from the final delivery.</li> <li>For example, In a grocery store, the Cleansing Area can be compared to the area where the goods are commissioned for sale. The goods are unpacked, vegetables and salad are washed, the meat is portioned, possibly combined with multiple products, and everything is labeled with price tags. The quality control of the delivered goods also belongs in this area.</li> </ul>"},{"location":"architecture/architecture/#data-processing-core","title":"Data Processing\u00a0(Core)","text":"<p>The data from the different source systems are brought together in a central area, the Data Processing (Core), through the Landing and Data Storage and stored there for extended periods, often several years.\u00a0</p> <ul> <li>A primary task of this layer is to integrate the data from different sources and store it in a thematically structured way rather than separated by origin.</li> <li>Often, thematic sub-areas in the Core are called \"Subject Areas.\"</li> <li>The data is stored in the Core so that historical data can be determined at any later point in time.\u00a0</li> <li>The Core should be the only data source for the Data Marts.</li> <li>Direct access to the Core by users should be avoided as much as possible.</li> </ul>"},{"location":"architecture/architecture/#data-mart","title":"Data Mart","text":"<p>Subsets of the data from the Core are stored in a form suitable for user queries.\u00a0</p> <ul> <li>Each Data Mart should only contain the data relevant to each application or a unique view of the data. This means several Data Marts are typically defined for different user groups and BI applications.</li> <li>This reduces the complexity of the queries, increasing the acceptance of the DWH system among users.</li> <li>For example, The Data Marts are the grocery store's market stalls or sales points. Each market stand offers a specific selection of goods, such as vegetables, meat, or cheese. The goods are presented so that they are accepted, i.e., purchased, by the respective customer group.</li> </ul> <p>Between the layers, we have lots of\u00a0Metadata</p> <p>Different types of metadata are needed for the smooth operation of the Data Warehouse. Business metadata contains business descriptions of all attributes, drill paths, and aggregation rules for the front-end applications and code designations. Technical metadata describes, for example, data structures, mapping rules, and parameters for ETL control. Operational metadata contains all log tables, error messages, logging of ETL processes, and much more. The metadata forms the infrastructure of a DWH system and is described as \"data about data\".</p> <p></p>"},{"location":"architecture/architecture/#example-multiple-superset-dashboards-within-a-data-domain","title":"Example: Multiple Superset Dashboards within a Data Domain","text":"<p>Within a Data Domain, several users build up different dashboards. Think of a dashboard as a specific use case e.g., Covid, Sales, etc., that solves a particular purpose. Each of these dashboards consists of individual charts and data sources in superset. Ultimately, what you see in the HelloDATA portal are the dashboards that combine all of the sub-components of what Superset provides.</p> <p></p>"},{"location":"architecture/architecture/#portalui-view","title":"Portal/UI View","text":"<p>As described in the intro. The portal is the heart of the HelloDATA application, with access to all critical applications.</p> <p>Entry page of helloDATA: When you enter the portal for the first time, you land on the dashboard where you have</p> <ol> <li>Navigation to jump to the different capabilities of helloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance, and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in users.\u00a0</li> <li>Choosing the data domain you want to work within your business domain</li> <li>Overview of your dashboards</li> <li>dbt linage docs</li> <li>Data marts of your Postgres database</li> <li>Answers to freuqently asked questions</li> </ol> <p>More technical details are in the \"Module deployment view\" chapter below.</p> <p></p>"},{"location":"architecture/architecture/#module-view-and-communication","title":"Module View and Communication","text":""},{"location":"architecture/architecture/#modules","title":"Modules","text":"<p>Going one level deeper, we see that we use different modules to make the portal and helloDATA work.\u00a0</p> <p>We have the following modules:</p> <ul> <li>Keycloak: Open-source identity and access management. This handles everything related to user permissions and roles in a central place that we integrate into helloDATA.</li> <li>Redis: open-source, in-memory data store that we use for persisting technical values for the portal to work.\u00a0</li> <li>NATS: Open-source connective technology for the cloud. It handles communication with the different tools we use.</li> <li>Data Stack: We use the open-source data stack with dbt, Airflow, and Superset. See more information in the intro chapters above. Subsystems can be added on demand as extensible plugins.</li> </ul> <p></p>"},{"location":"architecture/architecture/#what-is-keycloak-and-how-does-it-work","title":"What is Keycloak and how does it work?","text":"<p>At the center are two components, NATS and Keycloak.\u00a0Keycloak, together with the HelloDATA portal, handles the authentication, authorization, and permission management of HelloDATA components. Keycloak is a powerful open-source identity and access management system. Its primary benefits include:</p> <ol> <li>Ease of Use: Keycloak is easy to set up and use and can be deployed on-premise or in the cloud.</li> <li>Integration: It integrates seamlessly with existing applications and systems, providing a secure way of authenticating users and allowing them to access various resources and services with a single set of credentials.</li> <li>Single Sign-On: Keycloak takes care of user authentication, freeing applications from having to handle login forms, user authentication, and user storage. Users can log in once and access all applications linked to Keycloak without needing to re-authenticate. This extends to logging out, with Keycloak offering single sign-out across all linked applications.</li> <li>Identity Brokering and Social Login: Keycloak can authenticate users with existing OpenID Connect or SAML 2.0 Identity Providers and easily enable social network logins without requiring changes to your application's code.</li> <li>User Federation: Keycloak has the capacity to connect to existing LDAP or Active Directory servers and can support custom providers for users stored in other databases.</li> <li>Admin Console: Through the admin console, administrators can manage all aspects of the Keycloak server, including features, identity brokering, user federation, applications, services, authorization policies, user permissions, and sessions.</li> <li>Account Management Console: Users can manage their own accounts, update profiles, change passwords, setup two-factor authentication, manage sessions, view account history, and link accounts with additional identity providers if social login or identity brokering has been enabled.</li> <li>Standard Protocols: Keycloak is built on standard protocols, offering support for OpenID Connect, OAuth 2.0, and SAML.</li> <li>Fine-Grained Authorization Services: Beyond role-based authorization, Keycloak provides fine-grained authorization services, enabling the management of permissions for all services from the Keycloak admin console. This allows for the creation of specific policies to meet unique needs. Within HelloDATA, the HelloDATA portal manages authorization, yet if required by upcoming subsystems, this KeyCloak feature can be utilized in tandem.</li> <li>Two-Factor Authentication (2FA): This optional feature of KeyCloak enhances security by requiring users to provide two forms of authentication before gaining access, adding an extra layer of protection to the authentication process.</li> </ol>"},{"location":"architecture/architecture/#what-is-nats-and-how-does-it-work","title":"What is NATS and how does it work?","text":"<p>On the other hand, NATS is central for handling communication between the different modules. Its power comes from integrating modern distributed systems. It is the glue between microservices, making and processing statements, or stream processing.</p> <p>NATS focuses on hyper-connected moving parts and additional data each module generates. It supports location independence and mobility, whether the backend process is streaming or otherwise, and securely handles all of it.</p> <p>NATs let you connect mobile frontend or microservice to connect flexibly. There is no need for static 1:1 communication with a hostname, IP, or port. On the other hand, NATS lets you m:n connectivity based on subject instead. Still, you can use 1:1, but on top, you have things like load balancers, logs, system and network security models, proxies, and, most essential for us,\u00a0sidecars. We use sidecars heavily in connection with NATS.</p> <p>NATS can be\u00a0deployed\u00a0nearly anywhere: on bare metal, in a VM, as a container, inside K8S, on a device, or in whichever environment you choose. And all fully secure.</p>"},{"location":"architecture/architecture/#subsystem-communication","title":"Subsystem communication","text":"<p>Here is an example of subsystem communication. NATS, obviously at the center, handles these communications between the HelloDATA platform and the subsystems with its workers, as seen in the image below.</p> <p>The HelloDATA portal has workers.\u00a0These workers are deployed as extra containers with sidecars, called \"Sidecar Containers\". Each module needing communicating needs a sidecar with these workers deployed to communicate with NATS. Therefore, the subsystem itself has its workers to share with NATS as well.</p> <p></p>"},{"location":"architecture/architecture/#messaging-component-workers","title":"Messaging component workers","text":"<p>Everything starts with a\u00a0web browser\u00a0session. The HelloDATA user accesses the\u00a0HelloDATA Portal\u00a0through HTTP. Before you see any of your modules or components, you must authorize yourself again, Keycloak. Once logged in, you have a Single Sign-on Token that will give access to different business domains or data domains depending on your role.</p> <p>The HelloDATA portal sends an event to the EventWorkers via JDBC to the Portal database.\u00a0The\u00a0portal database\u00a0persists settings from the portal and necessary configurations.</p> <p>The\u00a0EventWorkers, on the other side communicate with the different\u00a0HelloDATA Modules\u00a0discussed above (Keycloak, NATS, Data Stack with dbt, Airflow, and Superset) where needed. Each module is part of the domain view, which persists their data within their datastore.</p> <p></p>"},{"location":"architecture/architecture/#flow-chart","title":"Flow Chart","text":"<p>In this flow chart, you see again what we discussed above in a different way. Here, we\u00a0assign a new user role. Again, everything starts with the HelloDATA Portal and an existing session from Keycloak. With that, the portal worker will publish a JSON message via UserRoleEvent to NATS. As\u00a0the\u00a0communication hub for HelloDATA, NATS knows what to do with each message and sends it to the respective subsystem worker.</p> <p>Subsystem workers will execute that instruction and create and populate roles on, e.g., Superset and Airflow, and once done, inform the spawned subsystem worker that it's done. The worker will push it back to NATS, telling the portal worker, and at the end, will populate a message on the HelloDATA portal.</p> <p></p>"},{"location":"architecture/architecture/#building-block-view","title":"Building Block View","text":""},{"location":"architecture/data-stack/","title":"Data Stack","text":"<p>We'll explain which data stack is behind HelloDATA BE.</p>"},{"location":"architecture/data-stack/#control-pane-portal","title":"Control Pane Portal","text":"<p>The\u00a0differentiator of HelloDATA\u00a0lies in the Portal. It combines all the loosely open-source tools into a single control pane.</p> <p>The portal lets you see:</p> <ul> <li>Data models with a dbt lineage: You see the sources of a given table or even column.</li> <li>You can check out the latest runs. Gives you when the dashboards have been updated.</li> <li>Create and view all company-wide reports and dashboards.</li> <li>View your data tables as Data Marts: Accessing physical tables, columns, and schemas.</li> <li>Central Monitoring of all processes running in the portal.</li> <li>Manage and control all your user access and role permission and authorization.</li> </ul> <p>You can find more about the navigation and the features in the\u00a0User Manual.</p>"},{"location":"architecture/data-stack/#data-modeling-with-sql-dbt","title":"Data Modeling with SQL - dbt","text":"<p>dbt\u00a0is a small database toolset that has gained immense popularity and is the facto standard for working with SQL. Why, you might ask? SQL is the most used language besides Python for data engineers, as it is\u00a0declarative and easy to learn the basics, and many business analysts or people working with Excel or similar tools might know a little already.</p> <p>The declarative approach is handy as you only define the\u00a0what, meaning you determine what columns you want in the SELECT and which table to query in the FROM statement. You can do more advanced things with WHERE, GROUP BY, etc., but you do not need to care about the\u00a0how. You do not need to watch which database, which partition it is stored, what segment, or what storage. You do not need to know if an index makes sense to use. All of it is handled by the\u00a0query optimizer\u00a0of Postgres (or any database supporting SQL).</p> <p>But let's face it: SQL also has its downside. If you have worked extensively with SQL, you know the spaghetti code that usually happens when using it. It's an issue because of the repeatability\u2014no\u00a0variable\u00a0we can set and reuse in an SQL. If you are familiar with them, you can achieve a better structure with\u00a0CTEs, which allows you to define specific queries as a block to reuse later. But this is only within one single query and handy if the query is already log.</p> <p>But what if you'd like to define your facts and dimensions as a separate query and reuse that in another query? You'd need to decouple the queries from storage, and we would persist it to disk and use that table on disk as a FROM statement for our following query. But what if we change something on the query or even change the name we won't notice in the dependent queries? And we will need to find out which queries depend on each other. There is no\u00a0lineage\u00a0or dependency graph.</p> <p>It takes a lot of work to be organized with SQL. There is also not a lot of support if you use a database, as they are declarative. You need to make sure how to store them in git or how to run them.</p> <p>That's where dbt comes into play. dbt lets you\u00a0create these dependencies within SQL. You can declaratively build on each query, and you'll get errors if one changes but not the dependent one. You get a lineage graph (see an\u00a0example), unit tests, and more. It's like you have an assistant that helps you do your job. It's added software engineering practice that we stitch on top of SQL engineering.</p> <p>The danger we need to be aware of, as it will be so easy to build your models, is not to make 1000 of 1000 tables. As you will get lots of errors checked by the pre-compiling dbt, \u00a0good data modeling techniques are essential to succeed.</p> <p>Below, you see dbt docs, lineage, and templates: 1. Project Navigation 2. Detail Navigation 3. SQL Template 4. SQL Compiled (practical SQL that gets executed) 5. Full Data lineage where with the source and transformation for the current object</p> <p></p> <p>Or zoom dbt lineage (when clicked): </p>"},{"location":"architecture/data-stack/#task-orchestration-airflow","title":"Task Orchestration - Airflow","text":"<p>Airflow\u00a0is the natural next step. If you have many SQLs representing your business metrics, you want them to run on a daily or hourly schedule triggered by events. That's where Airflow comes into play. Airflow is, in its simplest terms, a task or workflow scheduler, which tasks or\u00a0DAGs\u00a0(how they are called) can be written programatically with Python. If you know\u00a0cron\u00a0jobs, these are the lowest task scheduler in Linux (think <code>* * * * *</code>), but little to no customization beyond simple time scheduling.</p> <p>Airflow is different. Writing the DAGs in Python allows you to do whatever your business logic requires before or after a particular task is started. In the past, ETL tools like Microsoft SQL Server Integration Services (SSIS) and others were widely used. They were where your data transformation, cleaning and normalisation took place. In more modern architectures, these tools aren\u2019t enough anymore. Moreover, code and data transformation logic are much more valuable to other data-savvy people (data anlysts, data scientists, business analysts) in the company instead of locking them away in a propreitary format.</p> <p>Airflow or a general Orchestrator ensures correct execution of depend tasks. It is very flexibile and extensible with operators from the community or in-build capabiliities of the framework itself.</p>"},{"location":"architecture/data-stack/#default-view","title":"Default View","text":"<p>Airflow DAGs - Entry page which shows you the status of all your DAGs - what's the schedule of each job - are they active, how often have they failed, etc.</p> <p>Next, you can click on each of the DAGs and get into a detailed view: </p>"},{"location":"architecture/data-stack/#airflow-operations-overview-for-one-dag","title":"Airflow operations overview for one DAG","text":"<ol> <li>General visualization possibilities which you prefer to see (here Grid view)</li> <li>filter your DAG runs</li> <li>see details on each run status in one view\u00a0</li> <li>Check details in the table view</li> <li>Gantt view for another example to see how long each sub-task had of the DAG</li> </ol>"},{"location":"architecture/data-stack/#graph-view-of-dag","title":"Graph view of DAG","text":"<p>It shows you the dependencies of your business's various tasks, ensuring that the order is handled correctly.</p> <p></p>"},{"location":"architecture/data-stack/#dashboards-superset","title":"Dashboards - Superset","text":"<p>Superset\u00a0is the entry point to your data. It's a popular open-source business intelligence dashboard tool that visualizes your data according to your needs.\u00a0It's able to handle all the latest chart types. You can combine them into dashboards filtered and drilled down as expected from a BI tool. The access to dashboards is restricted to authenticated users only. A user can be given view or edit rights to individual dashboards using roles and permissions. Public access to dashboards is not supported.</p>"},{"location":"architecture/data-stack/#example-dashboard","title":"Example dashboard","text":""},{"location":"architecture/data-stack/#supported-charts","title":"Supported Charts","text":"<p>(see live in action)</p> <p></p>"},{"location":"architecture/data-stack/#ide-juypter-notebooks-with-jupyter-hub","title":"IDE - Juypter Notebooks with Jupyter Hub","text":"<p>Jupyter Notebooks and Jupyter Hub is an interactive IDE where you can code in Python or R (mainly) and implement your data science models, wrangling and cleaning your data, or visualize the data with charts. You are free to use what the Python or R libraries offer. It's a great tool to work with data interactively and share your results with others.</p> <p>Jupyter Hub is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. If you haven't heard of Jupyter Hub, you most certanily have seen or heard of Jupyter Notebooks, which turns your web browser into a interactive IDE.</p> <p>Jupyter Hub is encapsulating Jupyter Notebooks into a multi-user enviroment with lots of additonal features. In this part, we mostly focus on the feature of Jupyter Notebooks, as these are the once you and users will interact.</p> <p>]</p>"},{"location":"architecture/data-stack/#features","title":"Features","text":"<p>To name a few of Jupyter Notebooks features:</p> <ul> <li>Language of Choice: Jupyter Notebooks support over 40 programming languages, including Python, R, Julia, and Scala.</li> <li>Interactive Data Science: Jupyter Notebooks are a great tool for interactive data science. You can write code, visualize data, and share your results in a single document. It allows with most prominent libraries like Pandas, NumPy, Matplotlib, Apache Spark and many more.</li> <li>Share notebooks: In notebooks you can document your code along side with visualizations. When done, you can share your Jupyter Notebooks with others via link or by exporting them to HTML, PDF, or slideshows.</li> </ul> <p>What Jupyter Hub adds on top:</p> <ul> <li>Customizable: JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more.</li> <li>Flexible: JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub).</li> <li>Scalable: JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users.</li> <li>Portable: JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware.</li> </ul>"},{"location":"architecture/data-stack/#storage-layer-postgres","title":"Storage Layer - Postgres","text":"<p>Let's start with the storage layer. We use Postgres, the currently\u00a0most used and loved database. Postgres is versatile and simple to use. It's a\u00a0relational database\u00a0that can be customized and scaled extensively.</p>"},{"location":"architecture/infrastructure/","title":"Infrastructure","text":"<p>Infrastructure is the part where we go into depth about how to run HelloDATA and its components on\u00a0Kubernetes. </p>"},{"location":"architecture/infrastructure/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes and its platform allow you to run and orchestrate container workloads. Kubernetes has become\u00a0popular\u00a0and is the\u00a0de-facto standard\u00a0for your cloud-native apps to (auto-)\u00a0scale-out\u00a0and deploy the various open-source tools fast, on any cloud, and locally. This is called cloud-agnostic, as you are not locked into any cloud vendor (Amazon, Microsoft, Google, etc.).</p> <p>Kubernetes is\u00a0infrastructure as code, specifically as YAML, allowing you to version and test your deployment quickly. All the resources in Kubernetes, including Pods, Configurations, Deployments, Volumes, etc., can be expressed in a YAML file using Kubernetes tools like HELM. Developers quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down and using any programming language running with a simple Dockerfile. Its management makes it accessible through its modularity and abstraction; also, with the use of Containers, you can monitor all your applications in one place.</p> <p>Kubernetes\u00a0Namesspaces\u00a0provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace but not across namespaces. Namespace-based scoping is applicable only for namespaced\u00a0objects (e.g. Deployments, Services, etc)\u00a0and not for cluster-wide objects\u00a0(e.g., StorageClass, Nodes, PersistentVolumes, etc).</p> <ul> <li>Namespaces provide a mechanism for isolating groups of resources within a single cluster (separation of concerns). Namespaces also lets you easily wramp up several HelloDATA instances on demand.\u00a0<ul> <li>Names of resources need to be unique within a namespace but not across namespaces.</li> </ul> </li> <li>We get central monitoring and logging solutions with\u00a0Grafana,\u00a0Prometheus, and the\u00a0ELK stack (Elasticsearch, Logstash, and Kibana). As well as the Keycloak single sign-on.</li> <li>Everything runs in a single Kubernetes Cluster but can also be deployed on-prem on any Kubernetes Cluster.</li> <li>Persistent data will run within the \"Data Domain\" and must run on a\u00a0Persistent Volume\u00a0on Kubernetes or a central Postgres service (e.g., on Azure or internal).</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#module-deployment-view","title":"Module deployment view","text":"<p>Here, we have a look at the module view with an inside view of accessing the\u00a0HelloDATA Portal.</p> <p>The Portal API serves with\u00a0SpringBoot,\u00a0Wildfly\u00a0and\u00a0Angular.</p> <p></p> <p></p>"},{"location":"architecture/infrastructure/#storage-data-domain","title":"Storage (Data Domain)","text":"<p>Following up on how storage is persistent for the\u00a0Domain View\u00a0introduced in the above chapters.\u00a0</p>"},{"location":"architecture/infrastructure/#data-domain-storage-view","title":"Data-Domain Storage View","text":"<p>Storage is an important topic, as this is where the business value and the data itself are stored.</p> <p>From a Kubernetes and deployment view, everything is encapsulated inside a Namespace. As explained in the above \"Domain View\", we have different layers from one Business domain (here Business Domain) to n (multiple) Data Domains.\u00a0</p> <p>Each domain holds its data on\u00a0persistent storage, whether Postgres for relational databases, blob storage for files or file storage on persistent volumes within Kubernetes.</p> <p>GitSync is a tool we added to allow\u00a0GitOps-type deployment. As a user, you can push changes to your git repo, and GitSync will automatically deploy that into your cluster on Kubernetes.</p> <p></p>"},{"location":"architecture/infrastructure/#business-domain-storage-view","title":"Business-Domain Storage View","text":"<p>Here is another view that persistent storage within Kubernetes (K8s) can hold data across the Data Domain. If these\u00a0persistent volumes\u00a0are used to store Data Domain information, it will also require implementing a backup and restore plan for these data.</p> <p>Alternatively, blob storage on any\u00a0cloud vendor or services\u00a0such as Postgres service can be used, as these are typically managed and come with features such as backup and restore.</p> <p></p>"},{"location":"architecture/infrastructure/#k8s-jobs","title":"K8s Jobs","text":"<p>HelloDATA uses Kubernetes jobs to perform certain activities</p>"},{"location":"architecture/infrastructure/#cleanup-jobs","title":"Cleanup Jobs","text":"<p>Contents:</p> <ul> <li>Cleaning up user activity logs</li> <li>Cleaning up logfiles</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#deployment-platforms","title":"Deployment Platforms","text":"<p>HelloDATA can be operated as different platforms, e.g. development, test, and/or production platforms. The deployment is based on common CICD principles. It uses GIT and flux internally to deploy its resources onto the specific Kubernetes clusters. In case of resource shortages, the underlying platform can be extended with additional resources upon request. Horizontal scaling of the infrastructure can be done within the given resources boundaries (e. g. multiple pods for Superset.)</p>"},{"location":"architecture/infrastructure/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>See at\u00a0Roles and authorization concept.</p>"},{"location":"concepts/data-publisher/","title":"Data Publisher","text":"<p>With the Data Publisher, OGD data (Open Government Data) can be made available on the public Internet as CSV, XML, or JSON. This data can then be downloaded via a URL on a public website. </p>"},{"location":"concepts/data-publisher/#benefits-of-data-publisher","title":"Benefits of Data Publisher","text":"<p>The Data Publisher can be used to create greater transparency and make public data easily available. This fulfills the growing need for Open Government Data (OGD). </p>"},{"location":"concepts/data-publisher/#data-publisher-architecture","title":"Data Publisher architecture","text":"<p>The architecture of the Data Publisher is structured as follows: </p> <p></p> <ul> <li>Synchroniser:\u202fThe Synchroniser reads the data from the Datamart and converts it into a structured format (CSV, XLM, JSON). In a further step, this is stored on a filestorage.\u202f </li> <li>Filestorage:\u202f The filestorage serves as storage for various structured file formats. The filestorage is created within a Kubernetes cluster.\u202f </li> <li>Web server:\u202f The web server accesses the data stored on the filestorage. It also ensures that the data can be accessed via the https format and displayed/downloaded via a browser.\u202f </li> </ul>"},{"location":"concepts/data-publisher/#data-publisher-process","title":"Data Publisher process","text":"<p>The end-to-end process of the Data Publisher is as follows: </p> <p></p> <p>The blue boxes represent the current process that is already provided by HelloData BE with the help of DAGs. The orange boxes represent the tasks that are performed by the Data Publisher. </p> <ol> <li>Read out data: This process is done via Airflow and Python. </li> <li>Transforming data: Data transformation is carried out via DBT, and the data is integrated into a predefined target schema (e.g., UDM layer). </li> <li>Extract data from the datamart: All data is extracted from the defined target schema. </li> <li>Transform data into a file format: The data is transformed into one or more file formats (e.g. JSON, XML). </li> <li>Save data to a public repository: The data is stored on a publicly accessible repository on the Internet and a URL is provided for downloads. </li> </ol>"},{"location":"concepts/data-publisher/#configure-the-data-publisher","title":"Configure the data publisher","text":"<p>To configure the data publisher, the data owner must define the required tables and columns to be made available as OGD data. Once these have been defined, the data engineer or data analyst must configure the DAG so that the defined OGD data is written to a separate schema (usually the UDM layer). Once this has been set up, the Synchroniser can be configured as to which schema the tables should be exported from. This process with the corresponding responsibilities is shown in the illustration:  </p> <p></p>"},{"location":"concepts/data-publisher/#technical-process","title":"Technical process","text":"<p>In the first step, the DAG is executed either manually or in a planned manner. This process takes place on the HelloDATA platform, whereby the OGD data is written to a defined target schema, usually the UDM layer.  </p> <p>The Data Publisher process runs in parallel and is executed automatically at defined intervals. The timing of these queries can be customised by the administrator. In this step, the Data Publisher reads the data from the target schema, transforms it into one or more formats such as XML, JSON, or CSV, and saves it to the file store. The export formats can also be defined by the administrator.  </p> <p>Using a web server, the data can then be made available on the file repository via HTTPS. This allows the end user to be provided with a URL via which they can download the data. </p> <p></p>"},{"location":"concepts/showcase/","title":"Showcase: Animal Statistics (Switzerland)","text":""},{"location":"concepts/showcase/#what-is-the-showcase","title":"What is the Showcase?","text":"<p>It's the demo cases of HD-BE, it's importing animal data from an external source and loading them with Airflow, modeled with dbt, and visualized in Superset.</p> <p>It hopefully will show you how the platform works and it comes pre-installed with the docker-compose installation.</p>"},{"location":"concepts/showcase/#how-can-i-get-started-and-explore-it","title":"How can I get started and explore it?","text":"<p>Click on the data-domain <code>showcase</code> and you can explore pre-defined dashboards with below described Airflow job and dbt models.</p>"},{"location":"concepts/showcase/#how-does-it-look","title":"How does it look?","text":"<p>Below the technical details of the showcase are described. How the airflow pipeline is collecting the data from an open API and modeling it with dbt.</p>"},{"location":"concepts/showcase/#airflow-pipeline","title":"Airflow Pipeline","text":"<ul> <li>data_download   The source files, which are in CSV format, are queried via the data_download task and stored in the file system.</li> <li>create_tables   Based on the CSV files, tables are created in the LZN database schema of the project.</li> <li>insert_data   After the tables have been created, in this step, the source data from the CSV file is copied into the corresponding tables in the LZN database schema.</li> <li>dbt_run   After the preceding steps have been executed and the data foundation for the DBT framework has been established, the data processing steps in the database can be initiated using   DBT scripts. (described in the DBT section)</li> <li>dbt_docs   Upon completion of generating the tables in the database, a documentation of the tables and their dependencies is generated using DBT.</li> <li>dbt_docs_serve   For the visualization of the generated documentation, it is provided in the form of a website.</li> </ul>"},{"location":"concepts/showcase/#dbt-data-modeling","title":"DBT: Data modeling","text":""},{"location":"concepts/showcase/#fact_breeds_long","title":"fact_breeds_long","text":"<p>The fact table fact_breeds_long describes key figures, which are used to derive the stock of registered, living animals, divided by breeds over time.</p> <p>The following tables from the [lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cats_breeds</li> <li>cattle_breeds</li> <li>dogs_breeds</li> <li>equids_breeds</li> <li>goats_breeds</li> <li>sheep_breeds</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_beefiness_fattissue","title":"fact_cattle_beefiness_fattissue","text":"<p>The fact table fact_catle_beefiness_fattissue describes key figures, which are used to derive the number of slaughtered cows by year and month. Classification is done according to CH-TAX (Trading Class Classification CHTAX System | VIEGUT AG)</p> <p>The following tables from the [lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cattle_evolbeefiness</li> <li>cattle_evolfattissue</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_popvariations","title":"fact_cattle_popvariations","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the increase and decrease of the cattle population in the Animal Traffic Database (https://www.agate.ch/) over time (including reports from Liechtenstein). The key figures are grouped according to the following types of reports:</p> <ul> <li>Birth</li> <li>Slaughter</li> <li>Death</li> </ul> <p>The following table from the [lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_popvariations</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_pyr_wide-fact_cattle_pyr_long","title":"fact_cattle_pyr_wide\u00a0&amp;\u00a0fact_cattle_pyr_long","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the distribution of registered living cattle by age class and gender.</p> <p>The following table from the [lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_pyr</li> </ul> <p>The fact table fact_cattle_pyr_long pivots all key figures from fact_cattle_pyr_wide.</p> <p></p>"},{"location":"concepts/showcase/#superset","title":"Superset","text":""},{"location":"concepts/showcase/#database-connection","title":"Database Connection","text":"<p>The data foundation of the Superset visualizations in the form of Datasets, Dashboards, and Charts is realized through a Database Connection.</p> <p>In this case, a database connection to a database is established, which refers to a PostgreSQL database in which the above-described DBT scripts were executed.</p>"},{"location":"concepts/showcase/#datasets","title":"Datasets","text":"<p>Datasets are used to prepare the data foundation in a suitable form, which can then be visualized in charts in an appropriate way.</p> <p>Essentially, modeled fact tables from the UDM database schema are selected and linked with dimension tables.</p> <p>This allows facts to be calculated or evaluated at different levels of professional granularity.</p> <p></p>"},{"location":"concepts/showcase/#interfaces","title":"Interfaces","text":""},{"location":"concepts/showcase/#tierstatistik","title":"Tierstatistik","text":"Source Description https://tierstatistik.identitas.ch/de/ Website of the API provider https://tierstatistik.identitas.ch/de/docs.html Documentation of the platform and description of the data basis and API tierstatistik.identitas.ch/tierstatistik.rdf API and data provided by the website"},{"location":"concepts/workspaces-troubleshoot/","title":"Workspace Troubleshooting","text":""},{"location":"concepts/workspaces-troubleshoot/#kubernetes","title":"Kubernetes","text":"<p>If you haven't turned on Kubernetes, you'll get an error similar to this: <code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='kubernetes.docker.internal', port=6443): Max retries exceeded with url: /api/v1/namespaces/default/pods?labelSelector=dag_id%3Drun_boiler_example%2Ckubernetes_pod_operator%3DTrue%2Cpod-label-test%3Dlabel-name-test%2Crun_id%3Dmanual__2024-01-29T095915.2491840000-f3be8d87f%2Ctask_id%3Drun_duckdb_query%2Calready_checked%21%3DTrue%2C%21airflow-worker (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0xffff82c2ab10&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</code></p> <p>Full log: <pre><code>[2024-01-29, 09:48:49 UTC] {pod.py:1017} ERROR - 'NoneType' object has no attribute 'metadata'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n                       ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 363, in connect\n    self.sock = conn = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPSConnection object at 0xffff82db3650&gt;: Failed to establish a new connection: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 583, in execute_sync\n    self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 545, in get_or_create_pod\n    pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n\n....\n\n\nairflow.exceptions.AirflowException: Pod airflow-running-dagster-workspace-jdkqug7h returned a failure.\nremote_pod: None\n[2024-01-29, 09:48:49 UTC] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=run_boiler_example, task_id=run_duckdb_query, execution_date=20210501T000000, start_date=20240129T094849, end_date=20240129T094849\n[2024-01-29, 09:48:49 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 3 for task run_duckdb_query (Pod airflow-running-dagster-workspace-jdkqug7h returned a failure.\nremote_pod: None; 225)\n[2024-01-29, 09:48:49 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1\n[2024-01-29, 09:48:49 UTC] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check\n</code></pre></p>"},{"location":"concepts/workspaces-troubleshoot/#docker-image-not-build-locally-or-missing","title":"Docker image not build locally or missing","text":"<p>If your name or image is not available locally (check <code>docker image ls</code>), you'll get an error on Airflow like this:</p> <pre><code>[2024-01-29, 10:10:14 UTC] {pod.py:961} INFO - Building pod airflow-running-dagster-workspace-64ngbudj with labels: {'dag_id': 'run_boiler_example', 'task_id': 'run_duckdb_query', 'run_id': 'manual__2024-01-29T101013.7029880000-328a76b5e', 'kubernetes_pod_operator': 'True', 'try_number': '1'}\n[2024-01-29, 10:10:14 UTC] {pod.py:538} INFO - Found matching pod airflow-running-dagster-workspace-64ngbudj with labels {'airflow_kpo_in_cluster': 'False', 'airflow_version': '2.7.1-astro.1', 'dag_id': 'run_boiler_example', 'kubernetes_pod_operator': 'True', 'pod-label-test': 'label-name-test', 'run_id': 'manual__2024-01-29T101013.7029880000-328a76b5e', 'task_id': 'run_duckdb_query', 'try_number': '1'}\n[2024-01-29, 10:10:14 UTC] {pod.py:539} INFO - `try_number` of task_instance: 1\n[2024-01-29, 10:10:14 UTC] {pod.py:540} INFO - `try_number` of pod: 1\n[2024-01-29, 10:10:14 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:15 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:16 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:17 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:18 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:12:15 UTC] {pod.py:823} INFO - Deleting pod: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:12:15 UTC] {taskinstance.py:1935} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 594, in execute_sync\n    self.await_pod_start(pod=self.pod)\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 556, in await_pod_start\n    self.pod_manager.await_pod_start(pod=pod, startup_timeout=self.startup_timeout_seconds)\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py\", line 354, in await_pod_start\n    raise PodLaunchFailedException(msg)\nairflow.providers.cncf.kubernetes.utils.pod_manager.PodLaunchFailedException: Pod took longer than 120 seconds to start. Check the pod events in kubernetes to determine why.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 578, in execute\n    return self.execute_sync(context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 617, in execute_sync\n    self.cleanup(\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 746, in cleanup\n    raise AirflowException(\nairflow.exceptions.AirflowException: Pod airflow-running-dagster-workspace-64ngbudj returned a failure.\n\n\n...\n\n[2024-01-29, 10:12:15 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1\n[2024-01-29, 10:12:15 UTC] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check\n</code></pre> <p>If you open a kubernetes Monitoring tool such as Lens or k9s, you'll also see the pod struggling to pull the image:</p> <p></p> <p>Another cause, in case you haven't created the local PersistentVolume, you'd see something like \"my-pvc\" does not exist. Then you'd need to create the pvc first.</p>"},{"location":"concepts/workspaces/","title":"Data Engineering Workspaces","text":"<p>On this page, we'll explain what workspaces in the context of HelloDATA-BE are and how to use them, and you'll create your own based on a prepared starter repo.</p> <p>Info</p> <p>Also see the step-by-step video we created that might help you further.</p>"},{"location":"concepts/workspaces/#what-is-a-workspace","title":"What is a Workspace?","text":"<p>Within the context of HelloDATA-BE, data, engineers, or technical people can\u00a0develop their dbt, airflow, or even bring their tool, all packed into a separate git-repo and run as part of HelloDATA-BE where they enjoy the benefits of persistent storage, visualization tools, user management, monitoring, etc.</p> <p><pre><code>graph TD\n    subgraph \"Business Domain (Tenant)\"\n        BD[Business Domain]\n        BD --&gt;|Services| SR1[Portal]\n        BD --&gt;|Services| SR2[Orchestration]\n        BD --&gt;|Services| SR3[Lineage]\n        BD --&gt;|Services| SR5[Database Manager]\n        BD --&gt;|Services| SR4[Monitoring &amp; Logging]\n    end\n    subgraph \"Workspaces\"\n        WS[Workspaces] --&gt;|git-repo| DE[Data Engineering]\n        WS[Workspaces] --&gt;|git-repo| ML[ML Team]\n        WS[Workspaces] --&gt;|git-repo| DA[Product Analysts]\n        WS[Workspaces] --&gt;|git-repo| NN[...]\n    end\n    subgraph \"Data Domain (1-n)\"\n        DD[Data Domain] --&gt;|Persistent Storage| PG[Postgres]\n        DD[Data Domain] --&gt;|Data Modeling| DBT[dbt]\n        DD[Data Domain] --&gt;|Visualization| SU[Superset]\n    end\n\n    BD --&gt;|Contains 1-n| DD\n    DD --&gt;|n-instances| WS\n\n    %% Colors\n    class BD business\n    class DD data\n    class WS workspace\n    class SS,PGA subsystem\n    class SR1,SR2,SR3,SR4 services\n\n    classDef business fill:#96CD70,stroke:#333,stroke-width:2px;\n    classDef data fill:#A898D8,stroke:#333,stroke-width:2px;\n    classDef workspace fill:#70AFFD,stroke:#333,stroke-width:2px;\n    %% classDef subsystem fill:#F1C40F,stroke:#333,stroke-width:2px;\n    %% classDef services fill:#E74C3C,stroke:#333,stroke-width:1px;</code></pre> A schematic overview of workspaces are embedded into HelloDATA-BE.</p> <p>A workspace can have n-instances within a data domain. What does it mean? Each team can deal with its requirements to develop and build their project independently.</p> <p>Think of an ML engineer who needs heavy tools such as Tensorflow, etc., as an analyst might build simple dbt models. In contrast, another data engineer uses a specific tool from the Modern Data Stack.</p>"},{"location":"concepts/workspaces/#when-to-use-workspaces","title":"When to use Workspaces","text":"<p>Workspaces are best used for development, implementing custom business logic, and modeling your data. But there is no limit to what you build as long as it can be run as a DAG as an Airflow data pipeline.</p> <p>Generally speaking, a workspace is used whenever someone needs to create a custom logic yet to be integrated within the HelloDATA BE Platform.</p> <p>As a second step - imagine you implemented a critical business transformation everyone needs - that code and DAG could be moved and be a default DAG within a data domain. But the development always happens within the workspace, enabling self-serve.</p> <p>Without workspaces, every request would need to go over the HelloDATA BE Project team. Data engineers need a straightforward way isolated from deployment where they can add custom code for their specific data domain pipelines.</p>"},{"location":"concepts/workspaces/#how-does-a-workspace-work","title":"How does a Workspace work?","text":"<p>When you create your workspace, it will be deployed within HelloDATA-BE and run by an Airflow DAG. The Airflow DAG is the integration into HD. You'll define things like how often it runs, what it should run, the order of it, etc.</p> <p>Below, you see an example of two different Airflow DAGs deployed from two different Workspaces (marked red arrow): </p>"},{"location":"concepts/workspaces/#how-do-i-create-my-own-workspace","title":"How do I create my own Workspace?","text":"<p>To implement your own Workspace, we created a hellodata-be-workspace-starter. This repo contains a minimal set of artefacts in order to be deployed on HD.</p>"},{"location":"concepts/workspaces/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Install latest Docker Desktop</li> <li>Activate Kubernetes feature in Docker Desktop (needed to run Airflow DAG as an Docker-Image): <code>Settings -&gt; Kubernetes -&gt; Enable Kubernetes</code></li> </ul>"},{"location":"concepts/workspaces/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Clone hellodata-be-workspace-starter.</li> <li>Add your own custom logic to the repo, update Dockerfile with relevant libraries and binaries you need.</li> <li>Create one or multiple Airflow DAGs for running within HelloDATA-BE.</li> <li>Build the image with <code>docker build -t hellodata-ws-boilerplate:0.1.0-a.1 .</code> (or the name of choice)</li> <li>Start up Airflow locally with Astro CLI (see more below) and run/test the pipeline</li> <li>Define needed ENV-Variables and deployments needs (to be set-up by HD-Team initially once)</li> <li>Push the image to a DockerHub of choice</li> <li>Ask HD Team to deploy initially</li> </ol> <p>From now on whenever you have a change, you just build a new image and that will be deployed on HelloDATA-BE automatically. Making you and your team independent.</p>"},{"location":"concepts/workspaces/#boiler-plate-example","title":"Boiler-Plate Example","text":"<p>Below you find an example structure that help you understand how to configure workspaces for your needs.</p>"},{"location":"concepts/workspaces/#boiler-plate-repo","title":"Boiler-Plate repo","text":"<p>The repo helps you to build your workspace by simply clone the whole repo and adding your changes.</p> <p>We generally have these boiler plate files: <pre><code>\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 build-and-push.sh\n\u251c\u2500\u2500 deployment\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 deployment-needs.yaml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 airflow\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 .astro\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Dockerfile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Makefile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 README.md\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 airflow_settings.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .airflowignore\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 boiler-example.py\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 include\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 .kube\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 packages.txt\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 plugins\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 duckdb\n        \u2514\u2500\u2500 query_duckdb.py\n</code></pre></p>"},{"location":"concepts/workspaces/#important-files-business-logic-dag","title":"Important files: Business logic (DAG)","text":"<p>Where as <code>query_duckdb.py</code> and the <code>boiler-example.py</code> DAG are in this case are my custom code that you'd change with your own code. </p> <p>Although the Airflow DAG can be re-used as we use <code>KubernetesPodOperator</code> that works works within HD and locally (check more below). Essentially you change the name and the schedule to your needs, the image name and your good to go.</p> <p>Example of a Airflow DAG: <pre><code>from pendulum import datetime\nfrom airflow import DAG\nfrom airflow.configuration import conf\nfrom airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (\n    KubernetesPodOperator,\n)\nfrom kubernetes.client import models as k8s\nimport os\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_past\": False,\n    \"start_date\": datetime(2021, 5, 1),\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 1,\n}\n\nworkspace_name = os.getenv(\"HD_WS_BOILERPLATE_NAME\", \"ws-boilerplate\")\nnamespace = os.getenv(\"HD_NAMESPACE\", \"default\")\n\n# This will use .kube/config for local Astro CLI Airflow and ENV variable for k8s deployment\nif namespace == \"default\":\n    config_file = \"include/.kube/config\"  # copy your local kube file to the include folder: `cp ~/.kube/config include/.kube/config`\n    in_cluster = False\nelse:\n    in_cluster = True\n    config_file = None\n\nwith DAG(\n    dag_id=\"run_boiler_example\",\n    schedule=\"@once\",\n    default_args=default_args,\n    description=\"Boiler Plate for running a hello data workspace in airflow\",\n    tags=[workspace_name],\n) as dag:\n    KubernetesPodOperator(\n        namespace=namespace,\n        image=\"my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\",\n        image_pull_secrets=[k8s.V1LocalObjectReference(\"regcred\")],\n        labels={\"pod-label-test\": \"label-name-test\"},\n        name=\"airflow-running-dagster-workspace\",\n        task_id=\"run_duckdb_query\",\n        in_cluster=in_cluster,  # if set to true, will look in the cluster, if false, looks for file\n        cluster_context=\"docker-desktop\",  # is ignored when in_cluster is set to True\n        config_file=config_file,\n        is_delete_operator_pod=True,\n        get_logs=True,\n        # please add/overwrite your command here\n        cmds=[\"/bin/bash\", \"-cx\"],\n        arguments=[\n            \"python query_duckdb.py &amp;&amp; echo 'Query executed successfully'\",  # add your command here\n        ],\n    )\n</code></pre></p>"},{"location":"concepts/workspaces/#dag-how-to-test-or-run-a-dag-locally-before-deploying","title":"DAG: How to test or run a DAG locally before deploying","text":"<p>To run locally, the easiest way is to use the Astro CLI (see link for installation). With it, we can simply <code>astro start</code> or <code>astro stop</code> to start up/down.</p> <p>For local deployment we have these requirements:</p> <ul> <li>Local Docker installed (either native or Docker-Desktop)</li> <li>make sure Kubernetes is enabled</li> <li>copy you local kube-file to astro: <code>cp ~/.kube/config src/dags/airflow/include/.kube/</code></li> <li>attention, under Windows you find that file most probably under: <code>C:\\Users\\[YourIdHere]\\.kube\\config</code> </li> <li>make sure docker image is available locally (for Airflow to use it) -&gt; <code>docker build</code> must have run (check with <code>docker image ls</code></li> </ul> <p>The <code>config</code> file is used from astro to run on local Kubernetes. Se more infos on Run your Astro project in a local Airflow environment.</p>"},{"location":"concepts/workspaces/#install-requirements-dockerfile","title":"Install Requirements: <code>Dockerfile</code>","text":"<p>Below is the example how to install requirements (here <code>duckdb</code>) and copy my custom code <code>src/duckdb/query_duckdb.py</code> to the image.</p> <p>Boiler-plate example: <pre><code>FROM python:3.10-slim\n\nRUN mkdir -p /opt/airflow/airflow_home/dags/\n\n# Copy your airflow DAGs which will be copied into bussiness domain Airflow (These DAGs will be executed by Airflow)\nCOPY ../src/dags/airflow/dags/* /opt/airflow/airflow_home/dags/\n\nWORKDIR /usr/src/app\n\nRUN pip install --upgrade pip\n\n# Install DuckDB (example - please add your own dependencies here)\nRUN pip install duckdb\n\n# Copy the script into the container\nCOPY src/duckdb/query_duckdb.py ./\n\n# long-running process to keep the container running \nCMD tail -f /dev/null\n</code></pre></p>"},{"location":"concepts/workspaces/#deployment-deployment-needsyaml","title":"Deployment: <code>deployment-needs.yaml</code>","text":"<p>Below you see an an example of a deployment needs in <code>deployment-needs.yaml</code>, that defines:</p> <ul> <li>Docker image</li> <li>Volume mounts you need</li> <li>a command to run </li> <li>container behaviour</li> <li>extra ENV variables and values that HD-Team needs to provide for you</li> </ul> <p>This part is the one that will change most likely</p> <p>All of which will be eventually more automated. Also let us know or just add missing specs to the file and we'll add the functionallity on the deployment side. </p> <pre><code>spec:\n  initContainers:\n    copy-dags-to-bd:\n      image:\n        repository: my-docker-registry.com/hellodata-ws-boilerplate\n        pullPolicy: IfNotPresent\n        tag: \"0.1.0\"\n      resources: {}\n\n      volumeMounts:\n        - name: storage-hellodata\n          type: external\n          path: /storage\n      command: [ \"/bin/sh\",\"-c\" ]\n      args: [ \"mkdir -p /storage/${datadomain}/dags/${workspace}/ &amp;&amp; rm -rf /storage/${datadomain}/dags/${workspace}/* &amp;&amp; cp -a /opt/airflow/airflow_home/dags/*.py /storage/${datadomain}/dags/${workspace}/\" ]\n\n  containers:\n    - name: ws-boilerplate\n      image: my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\n      imagePullPolicy: Always\n\n\n#needed envs for Airflow\nairflow:\n\n  extraEnv: |\n    - name: \"HD_NAMESPACE\"\n      value: \"${namespace}\"\n    - name: \"HD_WS_BOILERPLATE_NAME\"\n      value: \"dd01-ws-boilerplate\"\n</code></pre>"},{"location":"concepts/workspaces/#example-with-airflow-and-dbt","title":"Example with Airflow and dbt","text":"<p>We've added another demo dag called <code>showcase-boiler.py</code> which is an DAG that download data from the web (animal statistics, ~150 CSVs), postgres tables are created, data inserted and a dbt run and docs is ran at the end.</p> <p></p> <p>In this case we use multiple task in a DAG, these have all the same image, but you could use different one for each step. Meaning you could use Python for download, R for transformatin and Java for machine learning. But as long as images are similar, I'd suggest to use the same image.</p>"},{"location":"concepts/workspaces/#volumes-pvc","title":"Volumes / PVC","text":"<p>Another addition is the use of voulmes. These are a persistent storage also called <code>pvs</code> in Kubernetes, which allow to store intermediate storage outside of the container. Downloaded CSVs are stored there for the next task to pick up from that storage.</p> <p>Locally you need to create such a storage once, there is a script in case you want to apply it to you local Docker-Desktop setup. Run this command: <pre><code>kubectl apply -f src/volume_mount/pvc.yaml\n</code></pre></p> <p>Be sure to use the same name, in this example we use <code>my-pvc</code> in your DAGs as well. See in the <code>showcase-boiler.py</code> how the volumnes are mounted like this: <pre><code>volume_claim = k8s.V1PersistentVolumeClaimVolumeSource(claim_name=\"my-pvc\")\nvolume = k8s.V1Volume(name=\"my-volume\", persistent_volume_claim=volume_claim)\nvolume_mount = k8s.V1VolumeMount(name=\"my-volume\", mount_path=\"/mnt/pvc\")\n</code></pre></p>"},{"location":"concepts/workspaces/#conclusion","title":"Conclusion","text":"<p>I hope this has illustrated how to create your own workspace. Otherwise let us know in the discussions or create an issue/PR.</p>"},{"location":"concepts/workspaces/#troubleshooting","title":"Troubleshooting","text":"<p>If you enconter errors, we collect them in Troubleshooting.</p>"},{"location":"manuals/role-authorization-concept/","title":"Roles and authorization concept","text":""},{"location":"manuals/role-authorization-concept/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>Authentication and authorizations within the various logical contexts or domains of the HelloDATA system are handled as follows.\u00a0 Authentication is handled via the OAuth 2 standard. In the case of the Canton of Bern, this is done via the central KeyCloak server. Authorizations to the various elements within a subject or Data Domain are handled via authorization within the HelloDATA portal. To keep administration simple, a role concept is applied. Instead of defining the authorizations for each user, roles receive the authorizations and the users are then assigned to the roles. The roles available in the portal have fixed defined permissions.</p>"},{"location":"manuals/role-authorization-concept/#business-domain","title":"Business Domain","text":"<p>In order for a user to gain access to a Business Domain, the user must be authenticated for the Business Domain. Users without authentication who try to access a Business Domain will receive an error message. The following two logical roles are available within a Business Domain:</p> <ul> <li>HELLODATA_ADMIN</li> <li>BUSINESS_DOMAIN_ADMIN</li> </ul>"},{"location":"manuals/role-authorization-concept/#hellodata_admin","title":"HELLODATA_ADMIN","text":"<ul> <li>Can act fully in the system.</li> </ul>"},{"location":"manuals/role-authorization-concept/#business_domain_admin","title":"BUSINESS_DOMAIN_ADMIN","text":"<ul> <li>Can manage users and assign roles (except HELLODATA_ADMIN).</li> <li>Can manage dashboard metadata.</li> <li>Can manage announcements.</li> <li>Can manage the FAQ.</li> <li>Can manage the external documentation links.</li> </ul> <p>BUSINESS_DOMAIN_ADMIN is automatically DATA_DOMAIN_ADMIN in all Data Domains within the Business Domain (see Data Domain Context).</p>"},{"location":"manuals/role-authorization-concept/#data-domain","title":"Data Domain","text":"<p>A Data Domain encapsulates all data elements and tools that are of interest for a specific issue. HalloDATA supports 1 - n Data Domains within a Business Domain.</p> <p>The resources to be protected within a Data Domain are:</p> <ul> <li>Schema of the Data Domain.</li> <li>Data mart tables of the Data Domain.</li> <li>The entire DWH environment of the Data Domain.</li> <li>Data lineage documents of the DBT projects of the Data Domain.</li> <li>Dashboards, charts, datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> </ul> <p>The following three logical roles are available within a Data Domain:</p> <ul> <li>DATA_DOMAIN_VIEWER \u00a0 \u00a0</li> <li>DATA_DOMAIN_EDITOR</li> <li>DATA_DOMAIN_ADMIN</li> </ul> <p>Depending on the role assigned, users are given different permissions to act in the Data Domain. A user who has not been assigned a role in a Data Domain will generally not be granted access to any resources of that Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#data_domain_viewer","title":"DATA_DOMAIN_VIEWER","text":"<ul> <li>The DATA_DOMAIN_VIEWER role is granted potential read access to dashboards of a Data Domain.</li> <li>Which dashboards of the Data Domain a DATA_DOMAIN_VIEWER user is allowed to see is administered within the user management of the HelloDATA portal.</li> <li>Only assigned dashboards are visible to a DATA_DOMAIN_VIEWER.</li> <li>Only dashboards in \"Published\" status are visible to a DATA_DOMAIN_VIEWER. A DATA_DOMAIN_VIEWER can view all data lineage documents of the Data Domain.</li> <li>A DATA_DOMAIN_VIEWER can access the links to external dashboards associated with its Data Domain. It is not checked whether the user has access in the systems outside the HelloDATA system boundary.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_editor","title":"DATA_DOMAIN_EDITOR","text":"<p>Same as DATA_DOMAIN_VIEWER plus:</p> <ul> <li>The DATA_DOMAIN_EDITOR role is granted read and write access to the dashboards of a Data Domain. All dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All charts used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All data sets used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR.</li> <li>A DATA_DOMAIN_EDITOR can create new dashboards.</li> <li>A DATA_DOMAIN_EDITOR can view the data marts of the Data Domain.</li> <li>A DATA_DOMAIN_EDITOR has access to the SQL lab in the superset.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_admin","title":"DATA_DOMAIN_ADMIN","text":"<p>Same as DATA_DOMAIN_EDITOR plus:</p> <p>The DATA_DOMAIN_ADMIN role can view the airflow DAGs of the Data Domain. A DATA_DOMAIN_ADMIN can view all database objects in the DWH of the Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#extra-data-domain","title":"Extra Data Domain","text":"<p>Beside the standard Data Domains there are also extra Data Domains An Extra Data Domain provides additional permissions, functions and database connections such as :</p> <ul> <li>CSV uploads to the Data Domain.</li> <li>Read permissions from one Data Domain to additional other Data Domain(s).</li> <li>Database connections to Data Domains of other databases.</li> <li>Database connections via AD group permissions.</li> <li>etc.</li> </ul> <p>These additional permissions, functions or database connections are a matter of negotiation per extra Data Domain. The additional permissions, if any, are then added to the standard roles mentioned above for the extra Data Domain.</p> <p>Row Level Security settings on Superset level can be used to additionally restrict the data that is displayed in a dashboard (e.g. only data of the own domain is displayed).</p>"},{"location":"manuals/role-authorization-concept/#system-role-to-portal-role-mapping","title":"System Role to Portal Role Mapping","text":"System Role Portal Role Portal Permission Menu / Submenu / Page in Portal Info HELLODATA_ADMIN SUPERUSER ROLE_MANAGEMENT Administration / Portal Rollenverwaltung MONITORING Monitoring DEVTOOLS Dev Tools USER_MANAGEMENT Administration / Benutzerverwaltung FAQ_MANAGEMENT Administration / FAQ Verwaltung EXTERNAL_DASHBOARDS_MANAGEMENT Unter External Dashboards Kann neue Eintr\u00e4ge erstellen und verwalten bei Seite External Dashboards DOCUMENTATION_MANAGEMENT Administration / Dokumentationsmanagement ANNOUNCEMENT_MANAGEMENT Administration/ Ank\u00fcndigungen DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration BUSINESS_DOMAIN_ADMIN BUSINESS_DOMAIN_ADMIN USER_MANAGEMENT Administration / Portal Rollenverwaltung FAQ_MANAGEMENT Dev Tools EXTERNAL_DASHBOARDS_MANAGEMENT Administration / Benutzerverwaltung DOCUMENTATION_MANAGEMENT Administration / FAQ Verwaltung ANNOUNCEMENT_MANAGEMENT Unter External Dashboards DASHBOARDS Administration / Dokumentationsmanagement Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Administration/ Ank\u00fcndigungen Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_ADMIN DATA_DOMAIN_ADMIN DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_EDITOR EDITOR DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DOMAIN_VIEWER VIEWER DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat"},{"location":"manuals/role-authorization-concept/#system-role-to-superset-role-mapping","title":"System Role to Superset Role Mapping","text":"System Role Superset Role Info No Data Domain role Public User should not get access to Superset functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER BI_VIEWER plus\u00a0roles forDashboards he was granted access to i. e. the slugified dashboard names with prefix \"D_\" Example: User is \"DATA_DOMAIN_VIEWER\" in a Data Domain. We grant the user acces to the \"Hello World\" dashboard. Then user gets the role \"BI_VIEWER\" plus the role \"D_hello_world\" in Superset. DATA_DOMAIN_EDITOR BI_EDITOR Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions. DATA_DOMAIN_ADMIN BI_EDITOR plus\u00a0BI_ADMIN Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions."},{"location":"manuals/role-authorization-concept/#system-role-to-airflow-role-mapping","title":"System Role to Airflow Role Mapping","text":"System Role Airflow Role Info HELLO_DATA_ADMIN Admin User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles.User additionally gets the Admin role. BUSINESS_DOMAIN_ADMIN User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles. No Data Domain role Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_EDITOR Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_ADMIN AF_OPERATOR plus\u00a0role corresponding to his Data Domain Key with prefix \"DD_\" Example: User is \"DATA_DOMAIN_ADMIN\" in a Data Domain with the key \"data_domain_one\". Then user gets the role \"AF_OPERATOR\" plus the role \"DD_data_domain_one\" in Airflow."},{"location":"manuals/user-manual/","title":"User Manual","text":""},{"location":"manuals/user-manual/#goal","title":"Goal","text":"<p>This use manual should enable you to use the HelloDATA platform and illustrate the features of the product and how to use them.</p> <p>\u2192 More about the Platform and its architecture you can find onArchitecture &amp; Concepts.</p>"},{"location":"manuals/user-manual/#navigation","title":"Navigation","text":""},{"location":"manuals/user-manual/#portal","title":"Portal","text":"<p>The entry page of HelloDATA is the Web Portal.</p> <ol> <li>Navigation to jump to the different capabilities of HelloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in user.4. Overview of your dashboards</li> </ol> <p></p>"},{"location":"manuals/user-manual/#business-data-domain","title":"Business &amp; Data Domain","text":"<p>As explained in Domain View, a key feature is to create business domains with n-data domains. If you have access to more than one data domain, you can switch between them by clicking the <code>drop-down</code> at the top and switch between them.</p> <p></p>"},{"location":"manuals/user-manual/#dashboards","title":"Dashboards","text":"<p>The most important navigation button is the dashboard links. If you hover over it, you'll see three options to choose from.</p> <p>You can either click the dashboard list in the hover menu (2) to see the list of dashboards with thumbnails, or directly choose your dashboard (3).</p> <p></p>"},{"location":"manuals/user-manual/#data-lineage","title":"Data-Lineage","text":"<p>To see the data lineage (dependencies of your data tables), you have the second menu option. Again, you chose the list or directly on \"data lineage\" (2).</p> <p>Button 2 will bring you to the project site, where you choose your project and load the lineage. </p> <p>Once loaded, you see all sources (1) and dbt Projects (2). On the detail page, you can see all the beautiful and helpful documentation such as:</p> <ul> <li>table name (3)</li> <li>columns and data types (4)</li> <li>which table and model this selected object depends on (5)</li> <li>the SQL code (6)<ul> <li>as a template or complied</li> </ul> </li> <li>and dependency graph (7)<ul> <li>which you can expand to full view (8) after clicking (7)</li> <li>interactive data lineage view (9)</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#data-marts-viewer","title":"Data Marts Viewer","text":"<p>This view let's you access the universaal data mart (udm) layer:</p> <p></p> <p>These are cleaned and modeled data mart tables. Data marts are the tables that have been joined and cleaned from the source tables. This is effectively the latest layer of HelloDATA BE, which the Dashboards are accessing. Dashboards should not access any layer before (landing zone, data storage, or data processing).</p> <p>We use CloudBeaver for this, same as the DWH Viewer later. </p>"},{"location":"manuals/user-manual/#data-engineering","title":"Data Engineering","text":""},{"location":"manuals/user-manual/#dwh-viewer","title":"DWH Viewer","text":"<p>This is essentially a database access layer where you see all your tables, and you can write SQL queries based on your access roles with a provided tool (CloudBeaver).</p>"},{"location":"manuals/user-manual/#create-new-sql-query","title":"Create new SQL Query","text":"<p>o</p>"},{"location":"manuals/user-manual/#choose-connection-and-stored-queries","title":"Choose Connection and stored queries","text":"<p>You can chose pre-defined connections and query your data warehouse. Also you can store queries that other user can see and use as well. Run your queries with (1).</p> <p></p>"},{"location":"manuals/user-manual/#settings-and-powerful-features","title":"Settings and Powerful features","text":"<p>You can set many settings, such as user status, and many more.</p> <p> Please find all setting and features in the CloudBeaver Documentation.</p>"},{"location":"manuals/user-manual/#orchestration","title":"Orchestration","text":"<p>The orchestrator is your task manager. You tellAirflow, our orchestrator, in which order the task will run. This is usually done ahead of time, and in the portal, you can see the latest runs and their status (successful, failed, etc.).</p> <ul> <li>You can navigate to DAGs (2) and see all the details (3) with the DAG name, owner, runs, schedules, next run and   recent.</li> <li>You can also dive deeper into Datasets, Security, Admin or similar (4)</li> <li>Airflow offers lots of different visualization modes, e.g. the Graph view (6), that allows you to see each step of   this task.<ul> <li>As you can see, you can choose calendar, task duration, Gantt, etc.</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#jupyter-notebooks-jupyter-hub","title":"Jupyter Notebooks (Jupyter Hub)","text":"<p>If you have one of the roles of <code>HELLODATA_ADMIN</code>, <code>BUSINESS_DOMAIN_ADMIN</code>, or <code>DATA_DOMAIN_ADMIN</code>, you can access Jupyter Hub and its notebooks with:</p> <p></p> <p>That opens up Jupyter Hub where you choose the base image you want to start with. E.g. you choose Data Science to do ML workloads, or R if you solely want to work with R. This could look like this:</p> <p></p> <p>After you can start creating notebooks with <code>File -&gt; New -&gt; Notebook</code>:</p> <p> After you choose the language (e.g. Python for Python notebooks, or R).</p> <p>After you can start running commands like you do in Jupyter Notebooks.</p> <p></p> <p>See the official documentation for help or functions.</p>"},{"location":"manuals/user-manual/#connect-to-hd-postgres-db","title":"Connect to HD Postgres DB","text":"<p>By default, a connection to your own Postgres DB can be made.</p> <p>The default session time is 24h as of now and can be changed with ENV <code>HELLODATA_JUPYTERHUB_TEMP_USER_PASSWORD_VALID_IN_DAYS</code>.</p>"},{"location":"manuals/user-manual/#how-to-connect-to-the-database","title":"How to connect to the database","text":"<p>This is how to get a db-connection:</p> <pre><code>from hello_data_scripts import connect # import the function\nconnection = connect() # use function, it fetches the temp user creds and establishes the connection\n</code></pre> <p><code>connection</code> can be used to read from postgres.</p>"},{"location":"manuals/user-manual/#example","title":"Example","text":"<p>This is a more extensive example of querying the Postgres database. Imagine <code>SELECT version();</code> as your custom query or logic you want to do.</p> <pre><code>import sys\n#import psycopg2 -&gt; this is imported through the below hello_data_scripts import\nfrom hello_data_scripts import connect  \n\n# Get the database connection\nconnection = connect()\n\nif connection is None:\n    print(\"Failed to connect to the database.\")\n    sys.exit(1)\n\ntry:\n    # Create a cursor object\n    cursor = connection.cursor()\n\n    # Example query to check the connection\n    cursor.execute(\"SELECT version();\")\n    db_version = cursor.fetchone()\n    print(f\"Connected to database. PostgreSQL version: {db_version}\")\n\nexcept psycopg2.Error as e:\n    print(f\"An error occurred while performing database operations: {e}\")\n\nfinally:\n    # Close the cursor and connection\n    cursor.close()\n    connection.close()\n    print(\"Database connection closed.\")\n</code></pre>"},{"location":"manuals/user-manual/#administration","title":"Administration","text":"<p>Here you manage the portal configurations such as user, roles, announcements, FAQs, and documentation management.</p> <p></p>"},{"location":"manuals/user-manual/#benutzerverwaltung-user-management","title":"Benutzerverwaltung / User Management","text":""},{"location":"manuals/user-manual/#adding-user","title":"Adding user","text":"<p>First type your email and hit enter. Then choose the drop down and click on it. </p> <p>Now type the Name and hit <code>Berechtigungen setzen</code> to add the user: </p> <p>You should see something like this:</p> <p></p>"},{"location":"manuals/user-manual/#changing-permissions","title":"Changing Permissions","text":"<ol> <li>Search the user you want to give or change permission</li> <li>Scroll to the right</li> <li>Click the green edit icon</li> </ol> <p>Now choose the <code>role</code> you want to give:</p> <p></p> <p>And or give access to specific data domains:</p> <p></p> <p>See more in role-authorization-concept.</p>"},{"location":"manuals/user-manual/#portal-rollenverwaltung-portal-role-management","title":"Portal Rollenverwaltung / Portal Role Management","text":"<p>In this portal role management, you can see all the roles that exist.</p> <p>Warning</p> <p>Creating new roles are not supported, despite the fact \"Rolle erstellen\" button exists. All roles are defined and hard coded.</p> <p></p>"},{"location":"manuals/user-manual/#creating-a-new-role","title":"Creating a new role","text":"<p>See how to create a new role below: </p>"},{"location":"manuals/user-manual/#ankundigung-announcement","title":"Ank\u00fcndigung / Announcement","text":"<p>You can simply create an announcement that goes to all users by <code>Ank\u00fcndigung erstellen</code>: </p> <p>Then you fill in your message. Save it.</p> <p> You'll see a success if everything went well: </p> <p>And this is how it looks to the users \u2014 It will appear until the user clicks the cross to close it. </p>"},{"location":"manuals/user-manual/#faq","title":"FAQ","text":"<p>The FAQ works the same as the announcements above. They are shown on the starting dashboard, but you can set the granularity of a data domain:</p> <p></p> <p>And this is how it looks: </p>"},{"location":"manuals/user-manual/#dokumentationsmanagement-documentation-management","title":"Dokumentationsmanagement / Documentation Management","text":"<p>Lastly, you can document the system with documentation management. Here you have one document that you can document everything in detail, and everyone can write to it. It will appear on the dashboard as well:</p> <p></p>"},{"location":"manuals/user-manual/#monitoring","title":"Monitoring","text":"<p>We provide two different ways of monitoring:</p> <ul> <li>Status:- Workspaces</li> </ul> <p></p>"},{"location":"manuals/user-manual/#status","title":"Status","text":"<p>It will show you details information on instances of HelloDATA, how is the situation for the Portal, is the monitoring running, etc. </p>"},{"location":"manuals/user-manual/#data-domains","title":"Data Domains","text":"<p>In Monitoring your data domains you see each system and the link to the native application. You can easily and quickly observer permission, roles and users by different subsystems (1). Click the one you want, and you can choose different levels (2) for each, and see its permissions (3).</p> <p></p> <p></p> <p>By clicking on the blue underlined <code>DBT Docs</code>, you will be navigated to the native dbt docs. Same is true if you click on a Airflow or Superset instance.</p>"},{"location":"manuals/user-manual/#devtools","title":"DevTools","text":"<p>DevTools are additional tools HelloDATA provides out of the box to e.g. send Mail (Mailbox) or browse files ( FileBrowser).</p> <p></p>"},{"location":"manuals/user-manual/#mailbox","title":"Mailbox","text":"<p>You can check in Mailbox (we useMailHog) what emails have been sending or what accounts are updated.|</p> <p></p>"},{"location":"manuals/user-manual/#filebrowser","title":"FileBrowser","text":"<p>Here you can browse all the documentation or code from the git repos as file browser. We useSFTPGohere. Please use with care, as some of the folder are system relevant.</p> <p>Log in</p> <p>Make sure you have the login credentials to log in. Your administrator should be able to provide these to you.</p> <p></p>"},{"location":"manuals/user-manual/#more-know-how","title":"More: Know-How","text":"<ul> <li>More help for Superset<ul> <li>Superset Documentation</li> </ul> </li> <li>More help for dbt:<ul> <li>dbt Documentation</li> <li>dbt Developer Hub</li> </ul> </li> <li>More about Airflow<ul> <li>Airflow Documentation</li> </ul> </li> <li>More about SFTPGo<ul> <li>SFTPGo Documentation</li> </ul> </li> </ul> <p>Find further important references, know-how, and best practices on HelloDATA Know-How.</p>"},{"location":"more/changelog/","title":"Changelog: HD-BE Documentation","text":""},{"location":"more/changelog/#2024-08-28-added-new-features","title":"2024-08-28 Added new features","text":"<ul> <li>Added Juypter Notebooks / Jupyter Hub on the data stack page.</li> <li>Added Data Publisher to the concept page.</li> <li>Updated User Manual related to the new features.</li> <li>Updated Roadmap.</li> </ul>"},{"location":"more/changelog/#2023-11-22-concepts","title":"2023-11-22 Concepts","text":"<ul> <li>Added workspaces on the concepts page.</li> <li>Added showcase main category to explain the demo that comes with HD-BE</li> </ul>"},{"location":"more/changelog/#2023-11-20-changed-corporate-design","title":"2023-11-20 Changed corporate design","text":"<ul> <li>Changed primary color to KAIO style guide: color red (#EE0F0F), and font: Roboto (was already default font)</li> </ul>"},{"location":"more/changelog/#2023-11-06-switched-architecture-over","title":"2023-11-06 Switched architecture over","text":"<ul> <li>Switched the architecture over to mkdocs</li> <li>Updated vision</li> <li>Updated user manual</li> </ul>"},{"location":"more/changelog/#2023-09-29-initial-version","title":"2023-09-29 Initial version","text":"<ul> <li>Created the template for documentation with mkdocs and the popular theme mkdocs-material.</li> </ul>"},{"location":"more/faq/","title":"FAQ","text":""},{"location":"more/glossary/","title":"Glossary","text":"<ul> <li>HD: HelloDATA</li> <li>KAIO: Amt f\u00fcr Informatik und Organisation des Kantons Bern (KAIO)</li> </ul>"},{"location":"test/examples/","title":"Examples","text":""},{"location":"test/examples/#code-annotation-examples","title":"Code Annotation Examples","text":""},{"location":"test/examples/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"test/examples/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"test/examples/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"test/examples/#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#admonitions-call-outs","title":"Admonitions / Call-outs","text":"<p>Note</p> <p>this is a note</p> <p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Supported types:</p> <ul> <li>note</li> <li>abstract</li> <li>info</li> <li>tip</li> <li>success</li> <li>question</li> <li>warning</li> <li>failure</li> <li>danger</li> <li>bug</li> <li>example</li> <li>quote</li> </ul>"},{"location":"test/examples/#diagrams","title":"Diagrams","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"test/examples/#sequence-diagram","title":"Sequence diagram","text":"<p>Sequence diagrams describe a specific scenario as sequential interactions between multiple objects or actors, including the messages that are exchanged between those actors:</p> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"test/examples/#state-diagram","title":"State diagram","text":"<p>State diagrams are a great tool to describe the behavior of a system, decomposing it into a finite number of states, and transitions between those states: <pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre></p>"},{"location":"test/examples/#class-diagram","title":"Class diagram","text":"<p>Class diagrams are central to object oriented programing, describing the structure of a system by modelling entities as classes and relationships between them:</p> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre>"},{"location":"test/examples/#entity-relationship-diagram","title":"Entity-relationship diagram","text":"<p>An entity-relationship diagram is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge:</p> <pre><code>erDiagram\n  CUSTOMER ||--o{ ORDER : places\n  ORDER ||--|{ LINE-ITEM : contains\n  LINE-ITEM {\n    string name\n    int pricePerUnit\n  }\n  CUSTOMER }|..|{ DELIVERY-ADDRESS : uses</code></pre>"},{"location":"test/examples/#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"vision/roadmap/","title":"Feature Roadmap HD-BE - August 2024","text":"Feature Roadmap"},{"location":"vision/vision-and-goal/","title":"Our Vision and Goal","text":"<p>The Open-Source Enterprise Data Platform in a Single Portal</p> <p>HelloDATA BE is an enterprise data platform built on top of open source. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p>"},{"location":"vision/vision-and-goal/#vision","title":"Vision","text":"<p>In a fast-moving data engineering world, where every device and entity becomes a data generator, the need for agile, robust, and transparent data platforms is more crucial. HelloDATA BE is not just any data platform; it's the bridge between open-source innovation and enterprise solutions' demanding reliability. </p> <p>HelloDATA BE handpicked the best tools like dbt, Airflow, Superset, and Postgres and integrated them into a seamless, enterprise-ready data solution. Empowering businesses with the agility of open-source and the dependability of a tested, unified platform. </p>"},{"location":"vision/vision-and-goal/#the-goal-of-hellodata-be","title":"The Goal of HelloDATA BE","text":"<p>Our goal at HelloDATA BE is clear: to democratize the power of data for enterprises. </p> <p>As digital transformation and data expand, the challenges with various SaaS solutions, vendor lock-ins, and fragmented data sources become apparent. </p> <p>HelloDATA BE trying to provide an answer to these challenges. We aim to merge the world's best open-source tools, refining them for enterprise standards ensuring that every organization, irrespective of size or niche, has access to top-tier data solutions. By fostering a community-driven approach through our open-source commitment, we envision a data future that's inclusive, robust, and open to innovation.</p>"}]}