{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HelloDATA BE \ud83d\udc4b\ud83c\udffb","text":"<p>This is the open documentation about HelloDATA BE. We hope you enjoy it.</p> <p>Contribute</p> <p>In case something is missing or you'd like to add something, below is how you can contribute:</p> <ul> <li>Star our\u00a0GitHub \u2b50</li> <li>Want to discuss, contribute, or need help, create a GitHub Issue, create a Pull Request or open a Discussion.</li> </ul>"},{"location":"#what-is-hellodata-be","title":"What is HelloDATA BE?","text":"<p>HelloDATA BE is an\u00a0enterprise data platform\u00a0built on top of open-source tools based on the modern data stack. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p> <p>Each of these components is carefully chosen and additional tools can be added in a later stage.</p>"},{"location":"#why-do-you-need-an-open-enterprise-data-platform-hellodata-be","title":"Why do you need an Open Enterprise Data Platform (HelloDATA BE)?","text":"<p>These days the amount of data grows yearly more than the entire lifetime before. Each fridge, light bulb, or anything really starts to produce data. Meaning there is a growing need to make sense of more data. Usually, not all data is necessary and valid, but due to the nature of growing data, we must be able to collect and store them easily. There is a great need to be able to analyze this data. The result can be used for secondary usage and thus create added value.</p> <p>That is what this open data enterprise platform is all about. In the old days, you used to have one single solution provided; think of SAP or Oracle. These days that has completely changed. New SaaS products are created daily, specializing in a tiny little niche. There are also many open-source tools to use and get going with minutes freely.</p> <p>So why would you need a HelloDATA BE? It's simple. You want the best of both worlds. You want\u00a0open source\u00a0to not be locked-in, to use the strongest, collaboratively created product in the open. People worldwide can fix a security bug in minutes, or you can even go into the source code (as it's available for everyone) and fix it yourself\u2014compared to an extensive vendor where you solely rely on their update cycle.</p> <p>But let's be honest for a second if we use the latest shiny thing from open source. There are a lot of bugs, missing features, and independent tools. That's precisely where HelloDATA BE comes into play. We are building the\u00a0missing platform\u00a0that combines the best-of-breed open-source technologies into a\u00a0single portal, making it enterprise-ready by adding features you typically won't get in an open-source product. Or we fix bugs that were encountered during our extensive tests.</p> <p>Sounds too good to be true? Give it a try. Do you want to know the best thing? It's open-source as well. Check out our\u00a0GitHub HelloDATA BE.</p>"},{"location":"#quick-start-for-developers","title":"Quick Start for Developers","text":"<p>Want to run HelloDATA BE and test it locally? Run the following command in the docker-compose directory to deploy all components:</p> <pre><code>cd hello-data-deployment/docker-compose\ndocker-compose up -d\n</code></pre> <p>Note: Please refer to our docker-compose README for more information; there are some must presets you need to configure.</p>"},{"location":"architecture/architecture/","title":"Architecture","text":""},{"location":"architecture/architecture/#components","title":"Components","text":"<p>This chapter will explain the core architectural component, its context views, and how HelloDATA works under the hood.</p>"},{"location":"architecture/architecture/#domain-view","title":"Domain View","text":"<p>We separate between two main domains, \"Business and Data Domain.</p>"},{"location":"architecture/architecture/#business-vs-data-domain","title":"Business\u00a0vs. Data Domain","text":"<ul> <li>\"Business\" domain: This domain holds one customer or company with general services (portal, orchestration, docs, monitoring, logging). Every business domain represents a business tenant. HelloDATA is running on a Kubernetes cluster. A business domain is treated as a dedicated namespace within that cluster; thus, multi-tenancy is set up by various namespaces in the Kubernetes cluster.</li> <li>Data Domain: This is where actual data is stored (db-schema). We combine it with a superset instance (related dashboards) and the documentation about these data. Currently, a business domain relates 1 - n to its Data Domains. Within an existing Business Domain a Data Domain can be spawned using Kubernetes deployment features and scripts to set up the database objects.</li> </ul> <p>Resources encapsulated inside a\u00a0Data Domain\u00a0can be:</p> <ul> <li>Schema of the Data Domain</li> <li>Data mart tables of the Data Domain</li> <li>The entire DWH environment of the Data Domain</li> <li>Data lineage documents of the DBT projects of the Data Domain.     Dashboards, charts, and datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> <li>JupyterHub</li> </ul> <p>On top, you can add\u00a0subsystems. This can be seen as extensions that make HelloDATA pluggable with additional tools. We now support\u00a0CloudBeaver\u00a0for viewing your Postgres databases, RtD, and Gitea. You can imagine adding almost infinite tools with capabilities you'd like to have (data catalog, semantic layer, specific BI tool, Jupyter Notebooks, etc.).</p> <p>Read more about Business and Data Domain access rights in\u00a0Roles / Authorization Concept.</p> <p></p>"},{"location":"architecture/architecture/#data-domain","title":"Data Domain","text":"<p>Zooming into several Data Domains that can exist within a Business domain, we see an example of Data Domain A-C. Each Data Domain has a persistent storage, in our case, Postgres (see more details in the\u00a0Infrastructure Storage\u00a0chapter below).</p> <p>Each data domain might import different source systems; some might even be used in several data domains, as illustrated. Each Data Domain is meant to have its data model with straightforward to, in the best case, layered data models as shown on the image with:</p>"},{"location":"architecture/architecture/#landingstaging-area","title":"Landing/Staging Area","text":"<p>Data from various source systems is first loaded into the Landing/Staging Area.</p> <ul> <li>In this first area, the data is stored as it is delivered; therefore, the stage tables' structure corresponds to the interface to the source system.</li> <li>No relationships exist between the individual tables.</li> <li>Each table contains the data from the final delivery, which will be deleted before the next delivery.</li> <li>For example, in a grocery store, the Staging Area corresponds to the loading dock where suppliers (source systems) deliver their goods (data). Only the latest deliveries are stored there before being transferred to the next area.</li> </ul>"},{"location":"architecture/architecture/#data-storage-cleansing-area","title":"Data Storage (Cleansing Area)","text":"<p>It must be cleaned before the delivered data is loaded into the Data Processing (Core). Most of these cleaning steps are performed in this area.</p> <ul> <li>Faulty data must be filtered, corrected, or complemented with singleton (default) values.</li> <li>Data from different source systems must be transformed and integrated into a unified form.</li> <li>This layer also contains only the data from the final delivery.</li> <li>For example, In a grocery store, the Cleansing Area can be compared to the area where the goods are commissioned for sale. The goods are unpacked, vegetables and salad are washed, the meat is portioned, possibly combined with multiple products, and everything is labeled with price tags. The quality control of the delivered goods also belongs in this area.</li> </ul>"},{"location":"architecture/architecture/#data-processing-core","title":"Data Processing\u00a0(Core)","text":"<p>The data from the different source systems are brought together in a central area, the Data Processing (Core), through the Landing and Data Storage and stored there for extended periods, often several years.\u00a0</p> <ul> <li>A primary task of this layer is to integrate the data from different sources and store it in a thematically structured way rather than separated by origin.</li> <li>Often, thematic sub-areas in the Core are called \"Subject Areas.\"</li> <li>The data is stored in the Core so that historical data can be determined at any later point in time.\u00a0</li> <li>The Core should be the only data source for the Data Marts.</li> <li>Direct access to the Core by users should be avoided as much as possible.</li> </ul>"},{"location":"architecture/architecture/#data-mart","title":"Data Mart","text":"<p>Subsets of the data from the Core are stored in a form suitable for user queries.\u00a0</p> <ul> <li>Each Data Mart should only contain the data relevant to each application or a unique view of the data. This means several Data Marts are typically defined for different user groups and BI applications.</li> <li>This reduces the complexity of the queries, increasing the acceptance of the DWH system among users.</li> <li>For example, The Data Marts are the grocery store's market stalls or sales points. Each market stand offers a specific selection of goods, such as vegetables, meat, or cheese. The goods are presented so that they are accepted, i.e., purchased, by the respective customer group.</li> </ul> <p>Between the layers, we have lots of\u00a0Metadata</p> <p>Different types of metadata are needed for the smooth operation of the Data Warehouse. Business metadata contains business descriptions of all attributes, drill paths, and aggregation rules for the front-end applications and code designations. Technical metadata describes, for example, data structures, mapping rules, and parameters for ETL control. Operational metadata contains all log tables, error messages, logging of ETL processes, and much more. The metadata forms the infrastructure of a DWH system and is described as \"data about data\".</p> <p></p>"},{"location":"architecture/architecture/#example-multiple-superset-dashboards-within-a-data-domain","title":"Example: Multiple Superset Dashboards within a Data Domain","text":"<p>Within a Data Domain, several users build up different dashboards. Think of a dashboard as a specific use case e.g., Covid, Sales, etc., that solves a particular purpose. Each of these dashboards consists of individual charts and data sources in superset. Ultimately, what you see in the HelloDATA portal are the dashboards that combine all of the sub-components of what Superset provides.</p> <p></p>"},{"location":"architecture/architecture/#portalui-view","title":"Portal/UI View","text":"<p>As described in the intro. The portal is the heart of the HelloDATA application, with access to all critical applications.</p> <p>Entry page of helloDATA: When you enter the portal for the first time, you land on the dashboard where you have</p> <ol> <li>Navigation to jump to the different capabilities of helloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance, and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in users.\u00a0</li> <li>Choosing the data domain you want to work within your business domain</li> <li>Overview of your dashboards</li> <li>dbt linage docs</li> <li>Data marts of your Postgres database</li> <li>Answers to freuqently asked questions</li> </ol> <p>More technical details are in the \"Module deployment view\" chapter below.</p> <p></p>"},{"location":"architecture/architecture/#module-view-and-communication","title":"Module View and Communication","text":""},{"location":"architecture/architecture/#modules","title":"Modules","text":"<p>Going one level deeper, we see that we use different modules to make the portal and helloDATA work.\u00a0</p> <p>We have the following modules:</p> <ul> <li>Keycloak: Open-source identity and access management. This handles everything related to user permissions and roles in a central place that we integrate into helloDATA.</li> <li>Redis: open-source, in-memory data store that we use for persisting technical values for the portal to work.\u00a0</li> <li>NATS: Open-source connective technology for the cloud. It handles communication with the different tools we use.</li> <li>Data Stack: We use the open-source data stack with dbt, Airflow, and Superset. See more information in the intro chapters above. Subsystems can be added on demand as extensible plugins.</li> </ul> <p></p>"},{"location":"architecture/architecture/#what-is-keycloak-and-how-does-it-work","title":"What is Keycloak and how does it work?","text":"<p>At the center are two components, NATS and Keycloak.\u00a0Keycloak, together with the HelloDATA portal, handles the authentication, authorization, and permission management of HelloDATA components. Keycloak is a powerful open-source identity and access management system. Its primary benefits include:</p> <ol> <li>Ease of Use: Keycloak is easy to set up and use and can be deployed on-premise or in the cloud.</li> <li>Integration: It integrates seamlessly with existing applications and systems, providing a secure way of authenticating users and allowing them to access various resources and services with a single set of credentials.</li> <li>Single Sign-On: Keycloak takes care of user authentication, freeing applications from having to handle login forms, user authentication, and user storage. Users can log in once and access all applications linked to Keycloak without needing to re-authenticate. This extends to logging out, with Keycloak offering single sign-out across all linked applications.</li> <li>Identity Brokering and Social Login: Keycloak can authenticate users with existing OpenID Connect or SAML 2.0 Identity Providers and easily enable social network logins without requiring changes to your application's code.</li> <li>User Federation: Keycloak has the capacity to connect to existing LDAP or Active Directory servers and can support custom providers for users stored in other databases.</li> <li>Admin Console: Through the admin console, administrators can manage all aspects of the Keycloak server, including features, identity brokering, user federation, applications, services, authorization policies, user permissions, and sessions.</li> <li>Account Management Console: Users can manage their own accounts, update profiles, change passwords, setup two-factor authentication, manage sessions, view account history, and link accounts with additional identity providers if social login or identity brokering has been enabled.</li> <li>Standard Protocols: Keycloak is built on standard protocols, offering support for OpenID Connect, OAuth 2.0, and SAML.</li> <li>Fine-Grained Authorization Services: Beyond role-based authorization, Keycloak provides fine-grained authorization services, enabling the management of permissions for all services from the Keycloak admin console. This allows for the creation of specific policies to meet unique needs. Within HelloDATA, the HelloDATA portal manages authorization, yet if required by upcoming subsystems, this KeyCloak feature can be utilized in tandem.</li> <li>Two-Factor Authentication (2FA): This optional feature of KeyCloak enhances security by requiring users to provide two forms of authentication before gaining access, adding an extra layer of protection to the authentication process.</li> </ol>"},{"location":"architecture/architecture/#what-is-nats-and-how-does-it-work","title":"What is NATS and how does it work?","text":"<p>On the other hand, NATS is central for handling communication between the different modules. Its power comes from integrating modern distributed systems. It is the glue between microservices, making and processing statements, or stream processing.</p> <p>NATS focuses on hyper-connected moving parts and additional data each module generates. It supports location independence and mobility, whether the backend process is streaming or otherwise, and securely handles all of it.</p> <p>NATs let you connect mobile frontend or microservice to connect flexibly. There is no need for static 1:1 communication with a hostname, IP, or port. On the other hand, NATS lets you m:n connectivity based on subject instead. Still, you can use 1:1, but on top, you have things like load balancers, logs, system and network security models, proxies, and, most essential for us,\u00a0sidecars. We use sidecars heavily in connection with NATS.</p> <p>NATS can be\u00a0deployed\u00a0nearly anywhere: on bare metal, in a VM, as a container, inside K8S, on a device, or in whichever environment you choose. And all fully secure.</p>"},{"location":"architecture/architecture/#subsystem-communication","title":"Subsystem communication","text":"<p>Here is an example of subsystem communication. NATS, obviously at the center, handles these communications between the HelloDATA platform and the subsystems with its workers, as seen in the image below.</p> <p>The HelloDATA portal has workers.\u00a0These workers are deployed as extra containers with sidecars, called \"Sidecar Containers\". Each module needing communicating needs a sidecar with these workers deployed to communicate with NATS. Therefore, the subsystem itself has its workers to share with NATS as well.</p> <p></p>"},{"location":"architecture/architecture/#messaging-component-workers","title":"Messaging component workers","text":"<p>Everything starts with a\u00a0web browser\u00a0session. The HelloDATA user accesses the\u00a0HelloDATA Portal\u00a0through HTTP. Before you see any of your modules or components, you must authorize yourself again, Keycloak. Once logged in, you have a Single Sign-on Token that will give access to different business domains or data domains depending on your role.</p> <p>The HelloDATA portal sends an event to the EventWorkers via JDBC to the Portal database.\u00a0The\u00a0portal database\u00a0persists settings from the portal and necessary configurations.</p> <p>The\u00a0EventWorkers, on the other side communicate with the different\u00a0HelloDATA Modules\u00a0discussed above (Keycloak, NATS, Data Stack with dbt, Airflow, and Superset) where needed. Each module is part of the domain view, which persists their data within their datastore.</p> <p></p>"},{"location":"architecture/architecture/#flow-chart","title":"Flow Chart","text":"<p>In this flow chart, you see again what we discussed above in a different way. Here, we\u00a0assign a new user role. Again, everything starts with the HelloDATA Portal and an existing session from Keycloak. With that, the portal worker will publish a JSON message via UserRoleEvent to NATS. As\u00a0the\u00a0communication hub for HelloDATA, NATS knows what to do with each message and sends it to the respective subsystem worker.</p> <p>Subsystem workers will execute that instruction and create and populate roles on, e.g., Superset and Airflow, and once done, inform the spawned subsystem worker that it's done. The worker will push it back to NATS, telling the portal worker, and at the end, will populate a message on the HelloDATA portal.</p> <p></p>"},{"location":"architecture/architecture/#building-block-view","title":"Building Block View","text":""},{"location":"architecture/data-stack/","title":"Data Stack","text":"<p>We'll explain which data stack is behind HelloDATA BE.</p>"},{"location":"architecture/data-stack/#control-pane-portal","title":"Control Pane Portal","text":"<p>Thedifferentiator of HelloDATAlies in the Portal. It combines all the loosely open-source tools into a single control pane.</p> <p>The portal lets you see:</p> <ul> <li>Data models with a dbt lineage: You see the sources of a given table or even column.</li> <li>You can check out the latest runs. Gives you when the dashboards have been updated.</li> <li>Create and view all company-wide reports and dashboards.</li> <li>View your data tables as Data Marts: Accessing physical tables, columns, and schemas.</li> <li>Central Monitoring of all processes running in the portal.</li> <li>Manage and control all your user access and role permission and authorization.</li> </ul> <p>You can find more about the navigation and the features in theUser Manual.</p>"},{"location":"architecture/data-stack/#data-modeling-with-sql-dbt","title":"Data Modeling with SQL - dbt","text":"<p>dbtis a small database toolset that has gained immense popularity and is the facto standard for working with SQL. Why, you might ask? SQL is the most used language besides Python for data engineers, as it is declarative and easy to learn the basics, and many business analysts or people working with Excel or similar tools might know a little already.</p> <p>The declarative approach is handy as you only define the_what_, meaning you determine what columns you want in the SELECT and which table to query in the FROM statement. You can do more advanced things with WHERE, GROUP BY, etc., but you do not need to care about the_how_. You do not need to watch which database, which partition it is stored, what segment, or what storage. You do not need to know if an index makes sense to use. All of it is handled by the query optimizer of Postgres (or any database supporting SQL).</p> <p>But let's face it: SQL also has its downside. If you have worked extensively with SQL, you know the spaghetti code that usually happens when using it. It's an issue because of the repeatability\u2014no_variable_we can set and reuse in an SQL. If you are familiar with them, you can achieve a better structure withCTEs, which allows you to define specific queries as a block to reuse later. But this is only within one single query and handy if the query is already log.</p> <p>But what if you'd like to define your facts and dimensions as a separate query and reuse that in another query? You'd need to decouple the queries from storage, and we would persist it to disk and use that table on disk as a FROM statement for our following query. But what if we change something on the query or even change the name we won't notice in the dependent queries? And we will need to find out which queries depend on each other. There is nolineageor dependency graph.</p> <p>It takes a lot of work to be organized with SQL. There is also not a lot of support if you use a database, as they are declarative. You need to make sure how to store them in git or how to run them.</p> <p>That's where dbt comes into play. dbt lets youcreate these dependencies within SQL. You can declaratively build on each query, and you'll get errors if one changes but not the dependent one. You get a lineage graph (see anexample), unit tests, and more. It's like you have an assistant that helps you do your job. It's added software engineering practice that we stitch on top of SQL engineering.</p> <p>The danger we need to be aware of, as it will be so easy to build your models, is not to make 1000 of 1000 tables. As you will get lots of errors checked by the pre-compiling dbt,good data modeling techniques are essential to succeed.</p> <p>Below, you see dbt docs, lineage, and templates:</p> <ol> <li>Project Navigation</li> <li>Detail Navigation</li> <li>SQL Template</li> <li>SQL Compiled (practical SQL that gets executed)</li> <li>Full Data lineage where with the source and transformation for the current object</li> </ol> <p></p> <p>Or zoom dbt lineage (when clicked): </p>"},{"location":"architecture/data-stack/#task-orchestration-airflow","title":"Task Orchestration - Airflow","text":"<p>Airflowis the natural next step. If you have many SQLs representing your business metrics, you want them to run on a daily or hourly schedule triggered by events. That's where Airflow comes into play. Airflow is, in its simplest terms, a task or workflow scheduler, which tasks orDAGs(how they are called) can be written programatically with Python. If you knowcronjobs, these are the lowest task scheduler in Linux (think <code>* * * * *</code>), but little to no customization beyond simple time scheduling.</p> <p>Airflow is different. Writing the DAGs in Python allows you to do whatever your business logic requires before or after a particular task is started. In the past, ETL tools like Microsoft SQL Server Integration Services (SSIS) and others were widely used. They were where your data transformation, cleaning and normalisation took place. In more modern architectures, these tools aren\u2019t enough anymore. Moreover, code and data transformation logic are much more valuable to other data-savvy people (data anlysts, data scientists, business analysts) in the company instead of locking them away in a propreitary format.</p> <p>Airflow or a general Orchestrator ensures correct execution of depend tasks. It is very flexibile and extensible with operators from the community or in-build capabiliities of the framework itself.</p>"},{"location":"architecture/data-stack/#default-view","title":"Default View","text":"<p>Airflow DAGs - Entry page which shows you the status of all your DAGs</p> <ul> <li>what's the schedule of each job</li> <li>are they active, how often have they failed, etc.</li> </ul> <p>Next, you can click on each of the DAGs and get into a detailed view: </p>"},{"location":"architecture/data-stack/#airflow-operations-overview-for-one-dag","title":"Airflow operations overview for one DAG","text":"<ol> <li>General visualization possibilities which you prefer to see (here Grid view)</li> <li>filter your DAG runs</li> <li>see details on each run status in one view4. Check details in the table view</li> <li>Gantt view for another example to see how long each sub-task had of the DAG</li> </ol>"},{"location":"architecture/data-stack/#graph-view-of-dag","title":"Graph view of DAG","text":"<p>It shows you the dependencies of your business's various tasks, ensuring that the order is handled correctly.</p> <p></p>"},{"location":"architecture/data-stack/#dashboards-superset","title":"Dashboards - Superset","text":"<p>Supersetis the entry point to your data. It's a popular open-source business intelligence dashboard tool that visualizes your data according to your needs. It's able to handle all the latest chart types. You can combine them into dashboards filtered and drilled down as expected from a BI tool. The access to dashboards is restricted to authenticated users only. A user can be given view or edit rights to individual dashboards using roles and permissions. Public access to dashboards is not supported.</p>"},{"location":"architecture/data-stack/#example-dashboard","title":"Example dashboard","text":""},{"location":"architecture/data-stack/#supported-charts","title":"Supported Charts","text":"<p>(see live in action)</p> <p></p>"},{"location":"architecture/data-stack/#ide-juypter-notebooks-with-jupyter-hub","title":"IDE - Juypter Notebooks with Jupyter Hub","text":"<p>Jupyter Notebooks and Jupyter Hub is an interactive IDE where you can code in Python or R (mainly) and implement your data science models, wrangling and cleaning your data, or visualize the data with charts. You are free to use what the Python or R libraries offer. It's a great tool to work with data interactively and share your results with others.</p> <p>Jupyter Hub is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. If you haven't heard of Jupyter Hub, you most certanily have seen or heard of Jupyter Notebooks, which turns your web browser into a interactive IDE.</p> <p>Jupyter Hub is encapsulating Jupyter Notebooks into a multi-user enviroment with lots of additonal features. In this part, we mostly focus on the feature of Jupyter Notebooks, as these are the once you and users will interact.</p> <p>]</p>"},{"location":"architecture/data-stack/#features","title":"Features","text":"<p>To name a few of Jupyter Notebooks features:</p> <ul> <li>Language of Choice: Jupyter Notebooks support over 40 programming languages, including Python, R, Julia, and   Scala.</li> <li>Interactive Data Science: Jupyter Notebooks are a great tool for interactive data science. You can write code,   visualize data, and share your results in a single document. It allows with most prominent libraries like Pandas,   NumPy, Matplotlib, Apache Spark and many more.</li> <li>Share notebooks: In notebooks you can document your code along side with visualizations. When done, you can share   your Jupyter Notebooks with others via link or by exporting them to HTML, PDF, or slideshows.</li> </ul> <p>What Jupyter Hub adds on top:</p> <ul> <li>Customizable: JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the   Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab,   RStudio, nteract, and more.</li> <li>Flexible: JupyterHub can be configured with authentication in order to provide access to a subset of users.   Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub).</li> <li>Scalable: JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs   on Kubernetes, and can run with up to tens of thousands of users.</li> <li>Portable: JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes   commercial cloud providers, virtual machines, or even your own laptop hardware.</li> </ul>"},{"location":"architecture/data-stack/#storage-layer-postgres","title":"Storage Layer - Postgres","text":"<p>Let's start with the storage layer. We use Postgres, the currentlymost used and loved database. Postgres is versatile and simple to use. It's arelational databasethat can be customized and scaled extensively.</p>"},{"location":"architecture/data-stack/#s3-and-sftpgo","title":"S3 and SFTPGo","text":""},{"location":"architecture/data-stack/#sftpgo-access-architecture-with-s3-integration","title":"SFTPGo Access Architecture with S3 Integration","text":"<p>The HelloDATA BE platform integrates secure file access and browsing through a modular architecture built on SFTPGo and Amazon S3 (or any other S3 alternative, like the MinIO).</p> <ul> <li>SFTPGo is deployed as a core component responsible for handling SFTP connections from users. It provides a secure   and standards-compliant interface for accessing data.</li> <li>An SFTPGo sidecar container is deployed alongside each SFTPGo instance. This sidecar dynamically configures   virtual folders based on the authenticated user\u2019s data domain access, retrieved from HelloDATA's access control   system.</li> <li>These virtual folders are mounted from backend object storage, such as Amazon S3 or MinIO, abstracting the   storage layer from the user.</li> <li>The integration ensures that:<ul> <li>Each user only sees folders and files relevant to their permissions.</li> <li>All data is accessed securely through the SFTP protocol.</li> <li>Underlying storage remains scalable, cloud-native, and cost-effective.</li> </ul> </li> </ul> <p>This architecture enables a consistent and user-specific file browsing experience within HelloDATA BE while maintaining secure, centralized data governance.</p> <p></p>"},{"location":"architecture/infrastructure/","title":"Infrastructure","text":"<p>Infrastructure is the part where we go into depth about how to run HelloDATA and its components on\u00a0Kubernetes. </p>"},{"location":"architecture/infrastructure/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes and its platform allow you to run and orchestrate container workloads. Kubernetes has become\u00a0popular\u00a0and is the\u00a0de-facto standard\u00a0for your cloud-native apps to (auto-)\u00a0scale-out\u00a0and deploy the various open-source tools fast, on any cloud, and locally. This is called cloud-agnostic, as you are not locked into any cloud vendor (Amazon, Microsoft, Google, etc.).</p> <p>Kubernetes is\u00a0infrastructure as code, specifically as YAML, allowing you to version and test your deployment quickly. All the resources in Kubernetes, including Pods, Configurations, Deployments, Volumes, etc., can be expressed in a YAML file using Kubernetes tools like HELM. Developers quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down and using any programming language running with a simple Dockerfile. Its management makes it accessible through its modularity and abstraction; also, with the use of Containers, you can monitor all your applications in one place.</p> <p>Kubernetes\u00a0Namesspaces\u00a0provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace but not across namespaces. Namespace-based scoping is applicable only for namespaced\u00a0objects (e.g. Deployments, Services, etc)\u00a0and not for cluster-wide objects\u00a0(e.g., StorageClass, Nodes, PersistentVolumes, etc).</p> <ul> <li>Namespaces provide a mechanism for isolating groups of resources within a single cluster (separation of concerns). Namespaces also lets you easily wramp up several HelloDATA instances on demand.\u00a0<ul> <li>Names of resources need to be unique within a namespace but not across namespaces.</li> </ul> </li> <li>We get central monitoring and logging solutions with\u00a0Grafana,\u00a0Prometheus, and the\u00a0ELK stack (Elasticsearch, Logstash, and Kibana). As well as the Keycloak single sign-on.</li> <li>Everything runs in a single Kubernetes Cluster but can also be deployed on-prem on any Kubernetes Cluster.</li> <li>Persistent data will run within the \"Data Domain\" and must run on a\u00a0Persistent Volume\u00a0on Kubernetes or a central Postgres service (e.g., on Azure or internal).</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#module-deployment-view","title":"Module deployment view","text":"<p>Here, we have a look at the module view with an inside view of accessing the\u00a0HelloDATA Portal.</p> <p>The Portal API serves with\u00a0SpringBoot,\u00a0Wildfly\u00a0and\u00a0Angular.</p> <p></p> <p></p>"},{"location":"architecture/infrastructure/#storage-data-domain","title":"Storage (Data Domain)","text":"<p>Following up on how storage is persistent for the\u00a0Domain View\u00a0introduced in the above chapters.\u00a0</p>"},{"location":"architecture/infrastructure/#data-domain-storage-view","title":"Data-Domain Storage View","text":"<p>Storage is an important topic, as this is where the business value and the data itself are stored.</p> <p>From a Kubernetes and deployment view, everything is encapsulated inside a Namespace. As explained in the above \"Domain View\", we have different layers from one Business domain (here Business Domain) to n (multiple) Data Domains.\u00a0</p> <p>Each domain holds its data on\u00a0persistent storage, whether Postgres for relational databases, blob storage for files or file storage on persistent volumes within Kubernetes.</p> <p>GitSync is a tool we added to allow\u00a0GitOps-type deployment. As a user, you can push changes to your git repo, and GitSync will automatically deploy that into your cluster on Kubernetes.</p> <p></p>"},{"location":"architecture/infrastructure/#business-domain-storage-view","title":"Business-Domain Storage View","text":"<p>Here is another view that persistent storage within Kubernetes (K8s) can hold data across the Data Domain. If these\u00a0persistent volumes\u00a0are used to store Data Domain information, it will also require implementing a backup and restore plan for these data.</p> <p>Alternatively, blob storage on any\u00a0cloud vendor or services\u00a0such as Postgres service can be used, as these are typically managed and come with features such as backup and restore.</p> <p></p>"},{"location":"architecture/infrastructure/#k8s-jobs","title":"K8s Jobs","text":"<p>HelloDATA uses Kubernetes jobs to perform certain activities</p>"},{"location":"architecture/infrastructure/#cleanup-jobs","title":"Cleanup Jobs","text":"<p>Contents:</p> <ul> <li>Cleaning up user activity logs</li> <li>Cleaning up logfiles</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#deployment-platforms","title":"Deployment Platforms","text":"<p>HelloDATA can be operated as different platforms, e.g. development, test, and/or production platforms. The deployment is based on common CICD principles. It uses GIT and flux internally to deploy its resources onto the specific Kubernetes clusters. In case of resource shortages, the underlying platform can be extended with additional resources upon request. Horizontal scaling of the infrastructure can be done within the given resources boundaries (e. g. multiple pods for Superset.)</p>"},{"location":"architecture/infrastructure/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>See at\u00a0Roles and authorization concept.</p>"},{"location":"concepts/dashboard-comments/","title":"Dashboard Comments","text":""},{"location":"concepts/dashboard-comments/#overview","title":"Overview","text":"<p>The Dashboard Comments feature allows users to create and manage comments directly within dashboards in the HelloDATA Portal. Comments support a full publishing workflow with draft and published states, versioning, and role-based access control.</p>"},{"location":"concepts/dashboard-comments/#architecture","title":"Architecture","text":""},{"location":"concepts/dashboard-comments/#components","title":"Components","text":""},{"location":"concepts/dashboard-comments/#backend-spring-boot","title":"Backend (Spring Boot)","text":"<ul> <li>DashboardCommentController - REST API controller (<code>/dashboards/{contextKey}/{dashboardId}/comments</code>)</li> <li>DashboardCommentService - Business logic and permission validation</li> <li>DashboardCommentPermissionService - Permission management and synchronization</li> <li>DashboardCommentRepository - JPA repository for comment database operations</li> <li>DashboardCommentPermissionRepository - JPA repository for permission database operations</li> <li>DashboardCommentEntity - JPA entity for comment metadata</li> <li>DashboardCommentVersionEntity - JPA entity for versioned content</li> <li>DashboardCommentPermissionEntity - JPA entity for user permissions per data domain</li> </ul>"},{"location":"concepts/dashboard-comments/#frontend-angular","title":"Frontend (Angular)","text":"<ul> <li>CommentsFeed - Main comments panel component</li> <li>CommentEntryComponent - Individual comment display with actions</li> <li>DomainDashboardCommentsComponent - Aggregated view of all comments for a domain</li> <li>DashboardCommentUtilsService - Shared utilities for comment operations</li> <li>NgRx Store - State management for comments</li> </ul>"},{"location":"concepts/dashboard-comments/#data-model","title":"Data Model","text":"<p>The data model consists of three main entities:</p> <ol> <li>DashboardCommentEntry - Immutable comment metadata</li> <li>DashboardCommentVersion - Versioned content with history</li> <li>DashboardCommentPermission - User permissions per data domain</li> </ol> <p>Key Design Points:</p> <ul> <li><code>pointerUrl</code> and <code>tags</code> are stored per-version and derived from the active version when displaying</li> <li>Permissions are independent of comment data and stored in a separate table</li> <li>Hierarchical permission model (REVIEW \u2283 WRITE \u2283 READ) is enforced at application layer</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    DashboardCommentEntry                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id: string (UUID)                                           \u2502\n\u2502 dashboardId: number                                         \u2502\n\u2502 dashboardUrl: string                                        \u2502\n\u2502 contextKey: string                                          \u2502\n\u2502 author: string                                              \u2502\n\u2502 authorEmail: string                                         \u2502\n\u2502 createdDate: number (timestamp)                             \u2502\n\u2502 deleted: boolean                                            \u2502\n\u2502 deletedDate?: number                                        \u2502\n\u2502 deletedBy?: string                                          \u2502\n\u2502 activeVersion: number                                       \u2502\n\u2502 hasActiveDraft?: boolean                                    \u2502\n\u2502 entityVersion: number (for optimistic locking)              \u2502\n\u2502 importedFromId?: string (original ID when imported)         \u2502\n\u2502 history: DashboardCommentVersion[]                          \u2502\n\u2502 pointerUrl?: string (derived from active version)           \u2502\n\u2502 tags?: string[] (derived from active version)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  DashboardCommentVersion                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 version: number                                             \u2502\n\u2502 text: string                                                \u2502\n\u2502 status: DashboardCommentStatus                              \u2502\n\u2502         (DRAFT | READY_FOR_REVIEW | PUBLISHED | DECLINED    \u2502\n\u2502          | DELETED)                                         \u2502\n\u2502 editedDate: number                                          \u2502\n\u2502 editedBy: string                                            \u2502\n\u2502 publishedDate?: number                                      \u2502\n\u2502 publishedBy?: string                                        \u2502\n\u2502 declineReason?: string                                      \u2502\n\u2502 deleted: boolean                                            \u2502\n\u2502 pointerUrl?: string (specific page/chart link for version)  \u2502\n\u2502 tags?: string[] (tags snapshot for this version)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               DashboardCommentPermission                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id: string (UUID)                                           \u2502\n\u2502 userId: string (UUID)                                       \u2502\n\u2502 contextKey: string                                          \u2502\n\u2502 readComments: boolean                                       \u2502\n\u2502 writeComments: boolean                                      \u2502\n\u2502 reviewComments: boolean                                     \u2502\n\u2502 createdDate?: number (timestamp)                            \u2502\n\u2502 createdBy?: string                                          \u2502\n\u2502 modifiedDate?: number (timestamp)                           \u2502\n\u2502 modifiedBy?: string                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Relationships:</p> <ul> <li><code>DashboardCommentEntry</code> \u2192 <code>DashboardCommentVersion</code>: One-to-Many (via <code>history</code> array)</li> <li><code>DashboardCommentPermission</code> \u2192 <code>User</code>: Many-to-One (via <code>userId</code>)</li> <li><code>DashboardCommentPermission</code> \u2192 <code>Context</code>: Many-to-One (via <code>contextKey</code>)</li> <li>Unique constraint: One permission record per <code>(userId, contextKey)</code> pair</li> </ul> <p>Note: The top-level <code>pointerUrl</code> and <code>tags</code> fields on <code>DashboardCommentEntry</code> are derived from the active version in history. They are not stored separately on the comment entity. This ensures that when admins switch versions (e.g., <code>restoreVersion</code>), the displayed <code>pointerUrl</code> and <code>tags</code> always match the active version's data.</p> <p>Permission Independence: The <code>DashboardCommentPermission</code> entity is completely independent from comment data, allowing administrators to manage commenting permissions separately from comment content. Permissions are automatically created during user synchronization and can be manually adjusted per user per data domain.</p>"},{"location":"concepts/dashboard-comments/#user-roles-and-permissions","title":"User Roles and Permissions","text":""},{"location":"concepts/dashboard-comments/#permission-system","title":"Permission System","text":"<p>The dashboard commenting system uses a three-level permission model that is independent of data domain roles:</p> Permission Level Description What it includes READ View published comments only Basic read-only access WRITE Create and manage own comments READ + create/edit own comments + view own drafts + delete own comments REVIEW Full comment moderation capabilities WRITE + view/manage all drafts + publish/decline + delete <p>Key Points:</p> <ul> <li>Permissions are stored per user per data domain in <code>dashboard_comment_permission</code> table</li> <li>Independent of data domain roles - a user's commenting permissions can differ from their data access level</li> <li>Hierarchical - REVIEW includes WRITE capabilities, WRITE includes READ capabilities</li> <li>Flexible - administrators can assign custom permission combinations for fine-grained control</li> </ul>"},{"location":"concepts/dashboard-comments/#permission-matrix","title":"Permission Matrix","text":"<p>The table below shows what each permission level allows:</p> Action READ WRITE REVIEW View published comments \u2714 \u2714 \u2714 View own drafts/declined \u2714 \u2714 View all drafts/declined \u2714 Create comment \u2714 \u2714 Edit own comment \u2714 \u2714 Edit any comment \u2714 Delete own version/entire \u2714 \u2714 Delete any version/entire \u2714 Send for review \u2714 \u2714 Publish comment \u2714 Decline comment \u2714 View metadata &amp; versions \u2714 Restore version* \u2714 <p>Legend:</p> <ul> <li>\u2714 = Action allowed</li> <li>(empty) = Action not allowed</li> <li>* = Cannot restore version when comment is in READY_FOR_REVIEW status</li> </ul> <p>Note for Administrators: HELLODATA_ADMIN, BUSINESS_DOMAIN_ADMIN, and DATA_DOMAIN_ADMIN roles act as templates for default permissions. Their permissions are synced to the <code>dashboard_comment_permission</code> table, which is the primary source of truth for all authorization checks.</p> <p>Note: Dashboard-level access (Superset RBAC) is also validated for restricted roles like <code>DATA_DOMAIN_VIEWER</code>. Even if they have READ permission for comments in the domain, they can only see comments for dashboards they can access in Superset.</p>"},{"location":"concepts/dashboard-comments/#automatic-permission-assignment","title":"Automatic Permission Assignment","text":"<p>When users are synchronized or new domains are created, default permissions are automatically assigned based on portal roles:</p> Portal Role Default Comment Permissions Scope HELLODATA_ADMIN READ + WRITE + REVIEW All data domains BUSINESS_DOMAIN_ADMIN READ + WRITE + REVIEW All data domains DATA_DOMAIN_ADMIN READ + WRITE + REVIEW Specific data domain Regular Users READ only All accessible domains NONE role No access Specific data domain <p>Important: These are default assignments. Administrators can manually adjust individual user permissions through the User Management UI or database to grant custom access levels.</p>"},{"location":"concepts/dashboard-comments/#permission-validation","title":"Permission Validation","text":""},{"location":"concepts/dashboard-comments/#backend-dashboardcommentservice","title":"Backend (DashboardCommentService)","text":"<p>The backend validates permissions solely by checking the <code>dashboard_comment_permission</code> table:</p> <pre><code>// Check user's comment permissions for a context\nprivate void checkHasAccessToComment(String contextKey) {\n    DashboardCommentPermissionEntity perm = getCommentPermissionForCurrentUser(contextKey);\n    if (perm == null || !perm.isWriteComments()) {\n        throw new ResponseStatusException(HttpStatus.FORBIDDEN, \"No write permission for comments\");\n    }\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#frontend-ngrx-selectors","title":"Frontend (NgRx Selectors)","text":"<p>Frontend selectors check permissions from the NgRx store state:</p> <pre><code>// Check if user can publish comments (requires REVIEW permission)\nexport const canPublishComment = createSelector(\n    selectCurrentDashboardContextKey,\n    selectCurrentUserCommentPermissions,\n    selectIsSuperuser,\n    selectIsBusinessDomainAdmin,\n    (state: AppState) =&gt; state.auth,\n    (contextKey, permissions, isSuperuser, isBusinessDomainAdmin, authState) =&gt; {\n        // Admins always have full access\n        if (isSuperuser || isBusinessDomainAdmin) {\n            return true;\n        }\n\n        // Check if user is data domain admin for this context\n        const isDataDomainAdmin = authState.contextRoles.some(role =&gt;\n            role.context.contextKey === contextKey &amp;&amp;\n            role.role.name === DATA_DOMAIN_ADMIN_ROLE\n        );\n\n        if (isDataDomainAdmin) {\n            return true;\n        }\n\n        // Check REVIEW permission from permission table\n        return permissions?.reviewComments || false;\n    }\n);\n\n// Check if user can create/edit comments (requires WRITE permission)\nexport const canEditComment = createSelector(\n    selectCurrentDashboardContextKey,\n    selectCurrentUserCommentPermissions,\n    selectCurrentUserEmail,\n    (contextKey, permissions, currentUserEmail) =&gt; (comment) =&gt; {\n        // User can edit their own comments with WRITE permission\n        if (comment.authorEmail === currentUserEmail &amp;&amp; permissions?.writeComments) {\n            return true;\n        }\n\n        // Or edit any comment with REVIEW permission\n        return permissions?.reviewComments || false;\n    }\n);\n</code></pre>"},{"location":"concepts/dashboard-comments/#comment-lifecycle","title":"Comment Lifecycle","text":""},{"location":"concepts/dashboard-comments/#states","title":"States","text":"<pre><code>  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   send for review   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    publish     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  DRAFT   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 READY_FOR_REVIEW \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 PUBLISHED \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u25b2                                   \u2502                                 \u2502\n       \u2502              decline              \u2502                                 \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n                                                                             \u2502\n       \u25b2                                                                     \u2502\n       \u2502                          edit published                             \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/dashboard-comments/#workflow","title":"Workflow","text":"<ol> <li>Create Comment - User creates a comment (status: DRAFT, version: 1)</li> <li>Edit Draft - Author or admin can edit the draft text</li> <li>Send for Review - Author sends the draft for moderation (status: READY_FOR_REVIEW)</li> <li>Decline - Reviewer declines the draft with a reason (status: DECLINED)</li> <li>Publish - Reviewer publishes the comment (status: PUBLISHED)</li> <li>Edit Published - Creates a new DRAFT version while keeping the published version active</li> <li>Delete - Soft deletes current version; restores last published if available. If <code>deleteEntire</code> is true, soft    deletes the entire comment entity.</li> </ol>"},{"location":"concepts/dashboard-comments/#versioning","title":"Versioning","text":"<p>Each edit to a published comment creates a new version:</p> <pre><code>Version 1 (PUBLISHED) \u2500\u25ba Version 2 (DRAFT) \u2500\u25ba Version 2 (PUBLISHED) \u2500\u25ba Version 3 (DRAFT)\n                                                       \u2502\n                                               (activeVersion = 3)\n</code></pre> <ul> <li>activeVersion - Points to the currently active version</li> <li>hasActiveDraft - True when a draft version exists</li> <li>Admins can restore any non-deleted PUBLISHED version</li> </ul>"},{"location":"concepts/dashboard-comments/#api-endpoints","title":"API Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/dashboards/{contextKey}/{dashboardId}/comments</code> Get all visible comments <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments</code> Create new comment <code>PUT</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}</code> Update draft comment <code>DELETE</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}</code> Delete comment version or entire <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}/send-for-review</code> Send draft for review <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}/decline</code> Decline draft with reason <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}/publish</code> Publish comment <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}/clone</code> Clone for edit (creates new version) <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/{commentId}/restore/{versionNumber}</code> Restore specific version <code>GET</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/tags</code> Get all tags for dashboard <code>GET</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/export</code> Export comments to JSON <code>POST</code> <code>/dashboards/{contextKey}/{dashboardId}/comments/import</code> Import comments from JSON"},{"location":"concepts/dashboard-comments/#requestresponse-examples","title":"Request/Response Examples","text":""},{"location":"concepts/dashboard-comments/#create-comment","title":"Create Comment","text":"<pre><code>// POST /dashboards/demo/5/comments\n// Request:\n{\n  \"text\": \"This dashboard shows sales metrics\",\n  \"dashboardUrl\": \"https://superset.example.com/superset/dashboard/5/\",\n  \"pointerUrl\": \"https://superset.example.com/superset/dashboard/5/?tab=sales\"\n}\n\n// Response:\n{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"dashboardId\": 5,\n  \"contextKey\": \"demo\",\n  \"author\": \"John Doe\",\n  \"authorEmail\": \"john.doe@example.com\",\n  \"createdDate\": 1705827600000,\n  \"activeVersion\": 1,\n  \"entityVersion\": 0,\n  \"history\": [\n    {\n      \"version\": 1,\n      \"text\": \"This dashboard shows sales metrics\",\n      \"status\": \"DRAFT\",\n      \"editedDate\": 1705827600000,\n      \"editedBy\": \"John Doe\",\n      \"deleted\": false\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#update-comment","title":"Update Comment","text":"<pre><code>// PUT /dashboards/demo/5/comments/550e8400-e29b-41d4-a716-446655440000\n// Request:\n{\n  \"text\": \"Updated comment text\",\n  \"entityVersion\": 0\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#optimistic-locking","title":"Optimistic Locking","text":"<p>To prevent concurrent edit conflicts, the system uses optimistic locking via <code>entityVersion</code>:</p> <ol> <li>Client sends <code>entityVersion</code> with update request</li> <li>Backend verifies <code>entityVersion</code> matches current value</li> <li>If mismatch, returns <code>409 Conflict</code> error</li> <li>Client receives notification and refreshes comments</li> </ol> <pre><code>private void checkEntityVersion(DashboardCommentEntity comment, Integer providedVersion, String userEmail) {\n    if (providedVersion != null &amp;&amp; !providedVersion.equals(comment.getEntityVersion())) {\n        log.warn(\"Optimistic lock conflict for comment {}, current: {}, provided: {}\",\n                comment.getId(), comment.getEntityVersion(), providedVersion);\n        throw new ResponseStatusException(HttpStatus.CONFLICT,\n                \"Comment was modified by another user. Please refresh and try again.\");\n    }\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#visibility-rules","title":"Visibility Rules","text":""},{"location":"concepts/dashboard-comments/#for-regular-users-read","title":"For Regular Users (READ)","text":"<ul> <li>See all PUBLISHED comments</li> <li>If current activeVersion is a DRAFT, READY_FOR_REVIEW or DECLINED by someone else \u2192 show last PUBLISHED version   instead</li> <li>If no PUBLISHED version exists \u2192 comment is hidden</li> </ul>"},{"location":"concepts/dashboard-comments/#for-authors-write","title":"For Authors (WRITE)","text":"<ul> <li>See all rules for Regular Users</li> <li>See own DRAFT, READY_FOR_REVIEW, and DECLINED versions</li> <li>Author can see their own declined version to read the decline reason</li> </ul>"},{"location":"concepts/dashboard-comments/#for-reviewers-review","title":"For Reviewers (REVIEW)","text":"<ul> <li>See all comments (DRAFT, READY_FOR_REVIEW, PUBLISHED, DECLINED)</li> <li>See complete version history</li> <li>Can switch between versions for any comment</li> </ul>"},{"location":"concepts/dashboard-comments/#dashboard-access-restriction","title":"Dashboard Access Restriction","text":"<p>Users with certain roles (e.g., <code>DATA_DOMAIN_VIEWER</code>, <code>BUSINESS_SPECIALIST</code>) are restricted to seeing comments only on dashboards they have explicit access to via Superset RBAC. This is enforced by the backend by matching user roles against the required roles for each dashboard.</p>"},{"location":"concepts/dashboard-comments/#consolidated-deletion-logic","title":"Consolidated Deletion Logic","text":"<p>The deletion process has been simplified into a single \"Delete\" action:</p> <ol> <li>Delete Version (Default): By default, the delete action soft-deletes the current active version (sets status to    <code>DELETED</code>).<ul> <li>If a previous <code>PUBLISHED</code> version exists, the comment is restored to that version.</li> <li>If no published version exists, the entire comment is soft-deleted.</li> </ul> </li> <li>Delete Entire Comment: If the user explicitly selects \"Delete entire comment\", the entire comment entity is    soft-deleted regardless of version history.</li> </ol> <p>Authors can delete their own DRAFT or DECLINED versions. Reviewers can delete any version or entire comments.</p>"},{"location":"concepts/dashboard-comments/#features","title":"Features","text":""},{"location":"concepts/dashboard-comments/#pointer-url","title":"Pointer URL","text":"<p>Comments can include an optional <code>pointerUrl</code> that links to a specific dashboard page, tab, or chart:</p> <ul> <li>Validated to ensure it points to the same Superset instance</li> <li>Clicking the link loads that specific view in the iframe</li> </ul>"},{"location":"concepts/dashboard-comments/#tags","title":"Tags","text":"<p>Comments can be tagged with short labels for better organization and filtering:</p> <ul> <li>Scope: Tags are scoped per dashboard - each dashboard has its own set of tags</li> <li>Length: Maximum 10 characters per tag</li> <li>Normalization: Tags are automatically trimmed, lowercased, and deduplicated</li> <li>Multiple tags: A comment can have multiple tags assigned</li> <li>Autocomplete: When adding tags, existing tags from the dashboard are suggested</li> <li>Filtering: Comments can be filtered by tag in the comments panel</li> <li>Permissions: Adding/editing tags follows the same permission rules as editing comments</li> <li>History tracking: Each version stores a snapshot of tags - changes to tags are visible in version history</li> </ul>"},{"location":"concepts/dashboard-comments/#tag-display","title":"Tag Display","text":"<ul> <li>Tags are displayed at the bottom of each comment entry</li> <li>Each tag shows with a tag icon (<code>fa-solid fa-tag</code>) followed by the tag name</li> <li>Tags have a distinctive blue chip styling for easy identification</li> <li>In version history, tags for each version are shown, allowing comparison of tag changes</li> </ul>"},{"location":"concepts/dashboard-comments/#tag-api","title":"Tag API","text":"<pre><code>GET /dashboards/{contextKey}/{dashboardId}/comments/tags\n</code></pre> <p>Returns all unique tags used in comments for a specific dashboard.</p> <ul> <li>Useful for referencing specific parts of complex dashboards</li> </ul>"},{"location":"concepts/dashboard-comments/#filtering","title":"Filtering","text":"<p>Domain comments view supports filtering by:</p> <ul> <li>Year</li> <li>Quarter (based on comment creation date)</li> <li>Tag (if any comments have tags assigned)</li> <li>Text search (searches comment text content)</li> </ul>"},{"location":"concepts/dashboard-comments/#auto-refresh","title":"Auto-refresh","text":"<p>Comments are automatically refreshed every 30 seconds when viewing a dashboard.</p>"},{"location":"concepts/dashboard-comments/#database-schema","title":"Database Schema","text":""},{"location":"concepts/dashboard-comments/#main-tables","title":"Main Tables","text":"<pre><code>CREATE TABLE dashboard_comment\n(\n    id               VARCHAR(36) PRIMARY KEY,\n    dashboard_id     INTEGER      NOT NULL,\n    dashboard_url    VARCHAR(2048),\n    context_key      VARCHAR(255) NOT NULL,\n    author           VARCHAR(255),\n    author_email     VARCHAR(255),\n    created_date     BIGINT,\n    deleted          BOOLEAN DEFAULT FALSE,\n    deleted_date     BIGINT,\n    deleted_by       VARCHAR(255),\n    active_version   INTEGER,\n    has_active_draft BOOLEAN DEFAULT FALSE,\n    entity_version   INTEGER DEFAULT 0,\n    imported_from_id VARCHAR(36) -- Original ID when imported from another dashboard\n);\n\nCREATE TABLE dashboard_comment_version\n(\n    id             BIGSERIAL PRIMARY KEY,\n    comment_id     VARCHAR(36) REFERENCES dashboard_comment (id),\n    version        INTEGER NOT NULL,\n    text           TEXT,\n    status         VARCHAR(20),\n    edited_date    BIGINT,\n    edited_by      VARCHAR(255),\n    published_date BIGINT,\n    published_by   VARCHAR(255),\n    deleted        BOOLEAN DEFAULT FALSE,\n    tags           TEXT,         -- Comma-separated tags snapshot for this version\n    pointer_url    VARCHAR(2000) -- Optional link to specific page/chart for this version\n);\n\nCREATE TABLE dashboard_comment_permission\n(\n    id              UUID PRIMARY KEY,\n    user_id         UUID         NOT NULL,\n    context_key     VARCHAR(100) NOT NULL,\n    read_comments   BOOLEAN      NOT NULL DEFAULT false,\n    write_comments  BOOLEAN      NOT NULL DEFAULT false,\n    review_comments BOOLEAN      NOT NULL DEFAULT false,\n    created_date    TIMESTAMP,\n    created_by      VARCHAR(255),\n    modified_date   TIMESTAMP,\n    modified_by     VARCHAR(255),\n    CONSTRAINT uq_dashboard_comment_permission_user_context UNIQUE (user_id, context_key),\n    CONSTRAINT fk_dashboard_comment_permission_user FOREIGN KEY (user_id)\n        REFERENCES user_ (id) ON DELETE CASCADE,\n    CONSTRAINT fk_dashboard_comment_permission_context FOREIGN KEY (context_key)\n        REFERENCES context (context_key) ON DELETE CASCADE\n);\n\n-- Indexes for comment lookups\nCREATE INDEX idx_dashboard_comment_context_dashboard\n    ON dashboard_comment (context_key, dashboard_id);\n\n-- Index for import duplicate detection\nCREATE INDEX idx_comment_imported_from\n    ON dashboard_comment (imported_from_id, context_key, dashboard_id);\n\n-- Indexes for permission lookups\nCREATE INDEX idx_comment_permission_user\n    ON dashboard_comment_permission (user_id);\n\nCREATE INDEX idx_comment_permission_context\n    ON dashboard_comment_permission (context_key);\n</code></pre>"},{"location":"concepts/dashboard-comments/#table-descriptions","title":"Table Descriptions","text":""},{"location":"concepts/dashboard-comments/#dashboard_comment","title":"dashboard_comment","text":"<p>Main comment entity storing metadata and reference to active version. Uses optimistic locking via <code>entity_version</code>.</p>"},{"location":"concepts/dashboard-comments/#dashboard_comment_version","title":"dashboard_comment_version","text":"<p>Stores complete version history including text, status, tags, and pointer URLs. Each version is immutable once created.</p>"},{"location":"concepts/dashboard-comments/#dashboard_comment_permission","title":"dashboard_comment_permission","text":"<p>Stores per-user per-data-domain commenting permissions. Independent of comment data, allowing flexible permission management.</p> <p>Note: <code>pointer_url</code> and <code>tags</code> are stored per version in <code>dashboard_comment_version</code>, not on the main <code>dashboard_comment</code> table. This design ensures each version snapshot captures the complete state (text, tags, pointerUrl) at that point in time. The application derives the current <code>pointerUrl</code> and <code>tags</code> from the active version when building DTOs.</p>"},{"location":"concepts/dashboard-comments/#permission-table-details","title":"Permission Table Details","text":"<p>The <code>dashboard_comment_permission</code> table provides fine-grained control over commenting capabilities:</p> <ul> <li>Unique constraint on <code>(user_id, context_key)</code> ensures one permission record per user per domain</li> <li>Cascade delete on user and context ensures automatic cleanup</li> <li>Boolean flags represent the three permission levels (read, write, review)</li> <li>Hierarchical permissions are enforced by the application layer (review \u2192 write \u2192 read)</li> <li>Audit fields track when and by whom permissions were created/modified</li> <li>Independent storage allows permission management without affecting comment data</li> </ul>"},{"location":"concepts/dashboard-comments/#permission-management-and-synchronization","title":"Permission Management and Synchronization","text":""},{"location":"concepts/dashboard-comments/#automatic-permission-initialization","title":"Automatic Permission Initialization","text":""},{"location":"concepts/dashboard-comments/#initial-setup-via-liquibase-migration","title":"Initial Setup via Liquibase Migration","text":"<p>When the system is first deployed or upgraded, a Liquibase migration (<code>55_initialize_dashboard_comment_permissions.sql</code>) automatically creates default permissions for all existing users:</p> <p>Migration Logic:</p> <ol> <li>Admins (HELLODATA_ADMIN, BUSINESS_DOMAIN_ADMIN) \u2192 Full access (READ + WRITE + REVIEW) in all domains</li> <li>Data Domain Admins (DATA_DOMAIN_ADMIN) \u2192 Full access (READ + WRITE + REVIEW) in their specific domain</li> <li>Regular users \u2192 Read-only access (READ) in all domains</li> </ol> <p>The migration uses SQL JOINs with <code>user_portal_role</code> and <code>user_context_role</code> tables to determine user roles and is * idempotent* - running it multiple times will not create duplicates.</p>"},{"location":"concepts/dashboard-comments/#user-synchronization-syncallusers","title":"User Synchronization (syncAllUsers)","text":"<p>The <code>syncAllUsers()</code> method in <code>UserService</code> automatically maintains comment permissions when users are synchronized:</p> <pre><code>// In UserService.syncAllUsers()\ndashboardCommentPermissionService.syncDefaultPermissionsForUser(userEntity);\n</code></pre> <p>When it runs:</p> <ul> <li>During scheduled user synchronization</li> <li>When new users are created</li> <li>When new data domains are added</li> </ul> <p>What it does:</p> <ul> <li>Checks if user has admin roles (HELLODATA_ADMIN or BUSINESS_DOMAIN_ADMIN)</li> <li>Creates missing permissions for any data domains where user doesn't have permissions yet</li> <li>Never modifies existing permissions - only creates new ones for new domains</li> <li>Logs the number of permissions created for each user</li> </ul> <p>Permission Assignment:</p> <pre><code>if(isAdmin){\n        // Admins get full access\n        permission.\n\nsetReadComments(true);\n    permission.\n\nsetWriteComments(true);\n    permission.\n\nsetReviewComments(true);\n}else{\n        // Regular users get read-only\n        permission.\n\nsetReadComments(true);\n    permission.\n\nsetWriteComments(false);\n    permission.\n\nsetReviewComments(false);\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#use-cases-for-automatic-synchronization","title":"Use Cases for Automatic Synchronization","text":""},{"location":"concepts/dashboard-comments/#1-new-data-domain-added","title":"1. New Data Domain Added","text":"<pre><code>Scenario: Administrator creates a new data domain \"finance\"\nResult:   syncAllUsers() runs automatically\n          - All admins receive READ+WRITE+REVIEW for \"finance\"\n          - All regular users receive READ for \"finance\"\n</code></pre>"},{"location":"concepts/dashboard-comments/#2-new-user-created","title":"2. New User Created","text":"<pre><code>Scenario: New user \"john@example.com\" is added to the system\nResult:   syncAllUsers() processes the new user\n          - Permissions created for all existing data domains\n          - Permission level based on user's portal role\n</code></pre>"},{"location":"concepts/dashboard-comments/#3-user-promoted-to-admin","title":"3. User Promoted to Admin","text":"<pre><code>Scenario: Regular user is promoted to BUSINESS_DOMAIN_ADMIN\nResult:   syncAllUsers() does NOT automatically upgrade existing permissions\nAction:   Administrator must manually update permissions via database\n          OR re-run migration to reset all permissions\n</code></pre>"},{"location":"concepts/dashboard-comments/#best-practices","title":"Best Practices","text":"<ol> <li>New Domains: Always run user sync after creating new data domains to ensure all users get appropriate permissions</li> <li>Role Changes: When promoting users to admin roles, consider manually updating their comment permissions if    immediate access is needed</li> <li>Permission Auditing: Use <code>created_by</code> and <code>modified_by</code> fields to track permission changes</li> <li>Regular Sync: Schedule regular <code>syncAllUsers()</code> execution to catch any missing permissions</li> </ol>"},{"location":"concepts/dashboard-comments/#troubleshooting","title":"Troubleshooting","text":"<p>Problem: User cannot see comment actions despite having correct domain role</p> <p>Solution:</p> <ol> <li>Check <code>dashboard_comment_permission</code> table for user's permissions</li> <li>Verify <code>context_key</code> matches the current dashboard's domain</li> <li>Run <code>syncAllUsers()</code> to create missing permissions</li> <li>Check application logs for permission validation errors</li> </ol> <p>SQL Query to Check Permissions:</p> <pre><code>SELECT u.email,\n       dcp.context_key,\n       dcp.read_comments,\n       dcp.write_comments,\n       dcp.review_comments\nFROM dashboard_comment_permission dcp\n         JOIN user_ u ON dcp.user_id = u.id\nWHERE u.email = 'user@example.com';\n</code></pre>"},{"location":"concepts/dashboard-comments/#error-handling","title":"Error Handling","text":"HTTP Status Scenario <code>403 Forbidden</code> User lacks permission for the action <code>404 Not Found</code> Comment not found <code>409 Conflict</code> Optimistic locking conflict (concurrent edit)"},{"location":"concepts/dashboard-comments/#importexport","title":"Import/Export","text":"<p>Comments can be exported and imported between dashboards for migration, backup, or copying comments across environments.</p>"},{"location":"concepts/dashboard-comments/#export","title":"Export","text":"<ul> <li>Format: JSON file</li> <li>Scope: Exports all non-deleted comments with complete version history (both DRAFT and PUBLISHED)</li> <li>Contents: Comment ID, text, author, creation date, status, tags, activeVersion, and full version history</li> <li>Permission: Only admins (superuser, business_domain_admin, data_domain_admin) can export</li> <li>File naming: <code>comments_{contextKey}_{dashboardTitle}_{date}.json</code> (title is sanitized for filesystem)</li> </ul>"},{"location":"concepts/dashboard-comments/#export-format","title":"Export Format","text":"<pre><code>{\n  \"exportVersion\": \"1.0\",\n  \"contextKey\": \"demo\",\n  \"dashboardId\": 5,\n  \"dashboardTitle\": \"Sales Dashboard\",\n  \"exportDate\": 1705827600000,\n  \"comments\": [\n    {\n      \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"text\": \"Current active version text\",\n      \"author\": \"John Doe\",\n      \"authorEmail\": \"john.doe@example.com\",\n      \"createdDate\": 1705827600000,\n      \"status\": \"PUBLISHED\",\n      \"activeVersion\": 2,\n      \"tags\": [\n        \"sales\",\n        \"kpi\"\n      ],\n      \"history\": [\n        {\n          \"version\": 1,\n          \"text\": \"First version text\",\n          \"status\": \"PUBLISHED\",\n          \"editedDate\": 1705827600000,\n          \"editedBy\": \"John Doe\",\n          \"publishedDate\": 1705828000000,\n          \"publishedBy\": \"Admin User\",\n          \"tags\": [\n            \"sales\"\n          ]\n        },\n        {\n          \"version\": 2,\n          \"text\": \"Current active version text\",\n          \"status\": \"PUBLISHED\",\n          \"editedDate\": 1705830000000,\n          \"editedBy\": \"Jane Smith\",\n          \"publishedDate\": 1705831000000,\n          \"publishedBy\": \"Admin User\",\n          \"tags\": [\n            \"sales\",\n            \"kpi\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/dashboard-comments/#import","title":"Import","text":"<ul> <li>Permission: Only admins (superuser, business_domain_admin, data_domain_admin) can import</li> <li>Status preservation: Comments are imported with their original status (DRAFT or PUBLISHED) preserved</li> <li>History preservation: Full version history is imported, allowing admins to review previous versions</li> <li>Pointer URL: Preserved per-version from export file (stored on each version in history)</li> <li>Dashboard URL: Automatically set to the target dashboard URL (not copied from source)</li> <li>Tags: Preserved per-version from export file</li> <li>Author: Preserved from export if available, otherwise uses importing user</li> </ul>"},{"location":"concepts/dashboard-comments/#import-scenarios","title":"Import Scenarios","text":""},{"location":"concepts/dashboard-comments/#1-first-import-same-dashboard-re-import","title":"1. First Import (Same Dashboard - Re-import)","text":"<p>When importing to the same dashboard where comments were originally exported:</p> <ul> <li>Comments with matching IDs are updated with imported data</li> <li>New comments (no matching ID) are created</li> <li>Prevents duplicate comments on re-import</li> </ul>"},{"location":"concepts/dashboard-comments/#2-import-from-different-dashboard","title":"2. Import from Different Dashboard","text":"<p>When importing to a different dashboard:</p> <ul> <li>New comments are created with new IDs</li> <li>The <code>importedFromId</code> field tracks the original comment ID</li> <li>On subsequent imports of the same file, comments are updated (not duplicated) based on <code>importedFromId</code></li> </ul>"},{"location":"concepts/dashboard-comments/#3-cross-environment-import","title":"3. Cross-Environment Import","text":"<p>When importing from another environment (e.g., DEV \u2192 PROD):</p> <ul> <li>Comments are created with new IDs</li> <li>Original author information is preserved</li> <li><code>importedFromId</code> enables re-import without duplicates</li> </ul>"},{"location":"concepts/dashboard-comments/#import-result","title":"Import Result","text":"<pre><code>{\n  \"imported\": 5,\n  \"updated\": 2,\n  \"skipped\": 1,\n  \"message\": \"Imported 5 new, updated 2 existing, skipped 1\"\n}\n</code></pre> <ul> <li>imported: Number of new comments created</li> <li>updated: Number of existing comments updated</li> <li>skipped: Number of invalid items (e.g., empty text)</li> </ul>"},{"location":"concepts/dashboard-comments/#import-validation","title":"Import Validation","text":"<ul> <li>File must be valid JSON</li> <li>Comments without text are skipped</li> <li>Invalid tags are normalized or skipped</li> <li>Dashboard URL is replaced with target dashboard URL</li> </ul>"},{"location":"concepts/dashboard-comments/#importexport-use-cases","title":"Import/Export Use Cases","text":"<ol> <li>Backup: Export comments before major changes</li> <li>Migration: Move comments when restructuring dashboards</li> <li>Template: Create standard comments that can be imported to multiple dashboards</li> <li>Cross-environment: Copy comments from development to production environment</li> <li>Disaster recovery: Restore comments from backup after data loss</li> </ol>"},{"location":"concepts/dashboard-comments/#import-tracking-importedfromid","title":"Import Tracking (importedFromId)","text":"<p>The <code>importedFromId</code> field in the database tracks the original comment ID from import. This enables:</p> <ul> <li>Detection of previously imported comments to prevent duplicates</li> <li>Traceability of comment origins</li> <li>Safe re-imports that update existing comments instead of creating duplicates</li> </ul>"},{"location":"concepts/dashboard-comments/#mobile-support","title":"Mobile Support","text":"<p>The comments panel adapts to mobile view:</p> <ul> <li>Comments panel opens as a drawer from the top</li> <li>Simplified UI for touch interaction</li> <li>Full functionality preserved</li> </ul>"},{"location":"concepts/dashboard-groups/","title":"Dashboard Groups","text":""},{"location":"concepts/dashboard-groups/#overview","title":"Overview","text":"<p>Dashboard Groups allow administrators to organize Superset dashboards into logical collections and assign them to users within a specific Data Domain. Instead of granting access to dashboards individually for each user, administrators can create a group (e.g. \"Finance Reports\", \"HR Analytics\"), add the relevant dashboards to it, and then assign users to the group. All members of a group automatically receive viewer access to the dashboards contained within that group.</p> <p>This mechanism simplifies user management \u2014 especially in environments with many dashboards and users \u2014 by providing a single point of control for dashboard access.</p>"},{"location":"concepts/dashboard-groups/#key-concepts","title":"Key Concepts","text":""},{"location":"concepts/dashboard-groups/#what-is-a-dashboard-group","title":"What is a Dashboard Group?","text":"<p>A Dashboard Group is a named collection that brings together:</p> <ul> <li>A set of dashboards \u2014 selected from the dashboards available in a given Data Domain</li> <li>A set of users \u2014 selected from users who have an eligible role in that Data Domain</li> </ul> <p>Each group is scoped to exactly one Data Domain (identified by <code>contextKey</code>). A Data Domain can have many groups, and each group manages its own list of dashboards and members independently.</p>"},{"location":"concepts/dashboard-groups/#how-does-it-work","title":"How does it work?","text":"<ol> <li>An administrator creates a new Dashboard Group within a chosen Data Domain.</li> <li>The administrator selects which dashboards should belong to the group.</li> <li>The administrator selects which users should be members of the group.</li> <li>Once the group is saved, the system automatically synchronizes the dashboard permissions to Apache Superset \u2014 members of the group receive viewer access to all dashboards in the group.</li> </ol> <p>When the group is later modified (dashboards added/removed, users added/removed) or deleted, the system re-synchronizes the affected users so that their Superset permissions always reflect the current state of the groups.</p>"},{"location":"concepts/dashboard-groups/#access-control","title":"Access Control","text":""},{"location":"concepts/dashboard-groups/#who-can-manage-dashboard-groups","title":"Who can manage Dashboard Groups?","text":"<p>Dashboard Group management is restricted to users with the <code>DASHBOARD_GROUPS_MANAGEMENT</code> authority. This is typically granted to administrators responsible for managing data access within the portal.</p> <p>All operations \u2014 listing, creating, editing, deleting groups, as well as viewing eligible users \u2014 require this authority.</p>"},{"location":"concepts/dashboard-groups/#which-users-can-be-added-to-a-group","title":"Which users can be added to a group?","text":"<p>Only users with one of the following Data Domain roles are eligible for group membership:</p> Role Description DATA_DOMAIN_VIEWER Standard viewer with read-only access to domain resources DATA_DOMAIN_BUSINESS_SPECIALIST Business specialist with access to dashboards in the domain <p>Users with higher roles (e.g. <code>DATA_DOMAIN_ADMIN</code>) are not listed as eligible group members because they typically already have broader access to all dashboards.</p>"},{"location":"concepts/dashboard-groups/#what-happens-when-a-users-role-changes","title":"What happens when a user's role changes?","text":"<p>If a user's Data Domain role is changed to a role that is not <code>DATA_DOMAIN_VIEWER</code> or <code>DATA_DOMAIN_BUSINESS_SPECIALIST</code>, the user is automatically removed from all Dashboard Groups in that Data Domain. This ensures that group memberships stay consistent with the user's actual role.</p>"},{"location":"concepts/dashboard-groups/#dashboard-access-direct-vs-group-based","title":"Dashboard Access: Direct vs. Group-Based","text":"<p>HelloDATA supports two ways of granting dashboard access to users:</p> Method Description Direct assignment An administrator assigns individual dashboards directly to a user Dashboard Group A user receives dashboard access through membership in one or more groups <p>Both methods coexist. When determining a user's final set of dashboards for Superset synchronization, the system merges direct assignments with group-based assignments. If the same dashboard appears in both a direct assignment and a group, it is included only once \u2014 the user still gets viewer access.</p>"},{"location":"concepts/dashboard-groups/#validation-rules","title":"Validation Rules","text":"<p>The system enforces the following rules when creating or updating Dashboard Groups:</p> <ul> <li>Name is required and must be between 3 and 150 characters.</li> <li>Name must be unique within the same Data Domain (case-insensitive). Attempting to create or rename a group to a name that already exists in that Data Domain will result in a conflict error.</li> <li>Name must start with a letter or a digit.</li> <li>A Data Domain must be selected \u2014 every group is scoped to exactly one Data Domain.</li> </ul>"},{"location":"concepts/dashboard-groups/#synchronization-with-superset","title":"Synchronization with Superset","text":"<p>Dashboard Groups are tightly integrated with the Superset permission synchronization pipeline:</p> <ul> <li>On group creation \u2014 if the group contains both dashboards and users, all member users are synchronized.</li> <li>On group update \u2014 the system detects which users were added or removed, and whether the dashboard list changed. Only affected users are re-synchronized. If the dashboard list changed, all current and former members are synchronized.</li> <li>On group deletion \u2014 all users who were members of the deleted group are re-synchronized so that their excess permissions are revoked.</li> </ul> <p>Synchronization happens asynchronously after the database transaction is committed, ensuring data consistency.</p>"},{"location":"concepts/dashboard-groups/#automatic-cleanup","title":"Automatic Cleanup","text":"<p>When dashboards are removed or become unavailable in a Data Domain (e.g. deleted from Superset), the system automatically cleans up stale entries from Dashboard Groups. Any dashboard entry referencing a dashboard that no longer exists is removed from all groups in the affected Data Domain. This prevents groups from containing references to non-existent dashboards.</p>"},{"location":"concepts/dashboard-groups/#batch-import-support","title":"Batch Import Support","text":"<p>Dashboard Groups can also be assigned to users via the CSV batch import mechanism. The CSV file supports an optional <code>Dashboard Group</code> column where group names can be specified (comma-separated within the column). During import, group names are resolved to their IDs, and users are assigned to the corresponding groups. If a referenced group name does not exist in the target Data Domain, the import will report an error.</p>"},{"location":"concepts/dashboard-groups/#api-reference","title":"API Reference","text":"<p>All Dashboard Group operations are exposed through the Portal REST API under the <code>/dashboard-groups</code> endpoint. The entire endpoint requires the <code>DASHBOARD_GROUPS_MANAGEMENT</code> authority.</p> Method Path Description <code>GET</code> <code>/dashboard-groups</code> List groups for a Data Domain (paginated, with search) <code>GET</code> <code>/dashboard-groups/{id}</code> Get a single group by ID <code>GET</code> <code>/dashboard-groups/eligible-users</code> Get users eligible for group membership in a Data Domain <code>POST</code> <code>/dashboard-groups</code> Create a new Dashboard Group <code>PUT</code> <code>/dashboard-groups</code> Update an existing Dashboard Group <code>DELETE</code> <code>/dashboard-groups/{id}</code> Delete a Dashboard Group <p>Query parameters for listing:</p> <ul> <li><code>contextKey</code> (required) \u2014 the Data Domain key</li> <li><code>page</code>, <code>size</code> \u2014 pagination</li> <li><code>sort</code> \u2014 sorting field</li> <li><code>search</code> \u2014 free-text search filtering by group name or dashboard title</li> </ul>"},{"location":"concepts/data-publisher/","title":"Data Publisher","text":"<p>With the Data Publisher, OGD data (Open Government Data) can be made available on the public Internet as CSV, XML, or JSON. This data can then be downloaded via a URL on a public website. </p>"},{"location":"concepts/data-publisher/#benefits-of-data-publisher","title":"Benefits of Data Publisher","text":"<p>The Data Publisher can be used to create greater transparency and make public data easily available. This fulfills the growing need for Open Government Data (OGD). </p>"},{"location":"concepts/data-publisher/#data-publisher-architecture","title":"Data Publisher architecture","text":"<p>The architecture of the Data Publisher is structured as follows: </p> <p></p> <ul> <li>Synchroniser:\u202fThe Synchroniser reads the data from the Datamart and converts it into a structured format (CSV, XLM, JSON). In a further step, this is stored on a filestorage.\u202f </li> <li>Filestorage:\u202f The filestorage serves as storage for various structured file formats. The filestorage is created within a Kubernetes cluster.\u202f </li> <li>Web server:\u202f The web server accesses the data stored on the filestorage. It also ensures that the data can be accessed via the https format and displayed/downloaded via a browser.\u202f </li> </ul>"},{"location":"concepts/data-publisher/#data-publisher-process","title":"Data Publisher process","text":"<p>The end-to-end process of the Data Publisher is as follows: </p> <p></p> <p>The blue boxes represent the current process that is already provided by HelloData BE with the help of DAGs. The orange boxes represent the tasks that are performed by the Data Publisher. </p> <ol> <li>Read out data: This process is done via Airflow and Python. </li> <li>Transforming data: Data transformation is carried out via DBT, and the data is integrated into a predefined target schema (e.g., UDM layer). </li> <li>Extract data from the datamart: All data is extracted from the defined target schema. </li> <li>Transform data into a file format: The data is transformed into one or more file formats (e.g. JSON, XML). </li> <li>Save data to a public repository: The data is stored on a publicly accessible repository on the Internet and a URL is provided for downloads. </li> </ol>"},{"location":"concepts/data-publisher/#configure-the-data-publisher","title":"Configure the data publisher","text":"<p>To configure the data publisher, the data owner must define the required tables and columns to be made available as OGD data. Once these have been defined, the data engineer or data analyst must configure the DAG so that the defined OGD data is written to a separate schema (usually the UDM layer). Once this has been set up, the Synchroniser can be configured as to which schema the tables should be exported from. This process with the corresponding responsibilities is shown in the illustration:  </p> <p></p>"},{"location":"concepts/data-publisher/#technical-process","title":"Technical process","text":"<p>In the first step, the DAG is executed either manually or in a planned manner. This process takes place on the HelloDATA platform, whereby the OGD data is written to a defined target schema, usually the UDM layer.  </p> <p>The Data Publisher process runs in parallel and is executed automatically at defined intervals. The timing of these queries can be customised by the administrator. In this step, the Data Publisher reads the data from the target schema, transforms it into one or more formats such as XML, JSON, or CSV, and saves it to the file store. The export formats can also be defined by the administrator.  </p> <p>Using a web server, the data can then be made available on the file repository via HTTPS. This allows the end user to be provided with a URL via which they can download the data. </p> <p></p>"},{"location":"concepts/showcase/","title":"Showcase: Animal Statistics (Switzerland)","text":""},{"location":"concepts/showcase/#what-is-the-showcase","title":"What is the Showcase?","text":"<p>It's the demo cases of HD-BE, it's importing animal data from an external source and loading them with Airflow, modeled with dbt, and visualized in Superset.</p> <p>It hopefully will show you how the platform works and it comes pre-installed with the docker-compose installation.</p>"},{"location":"concepts/showcase/#how-can-i-get-started-and-explore-it","title":"How can I get started and explore it?","text":"<p>Click on the data-domain <code>showcase</code> and you can explore pre-defined dashboards with below described Airflow job and dbt models.</p>"},{"location":"concepts/showcase/#how-does-it-look","title":"How does it look?","text":"<p>Below the technical details of the showcase are described. How the airflow pipeline is collecting the data from an open API and modeling it with dbt.</p>"},{"location":"concepts/showcase/#airflow-pipeline","title":"Airflow Pipeline","text":"<ul> <li>data_download   The source files, which are in CSV format, are queried via the data_download task and stored in the file system.</li> <li>create_tables   Based on the CSV files, tables are created in the LZN database schema of the project.</li> <li>insert_data   After the tables have been created, in this step, the source data from the CSV file is copied into the corresponding tables in the LZN database schema.</li> <li>dbt_run   After the preceding steps have been executed and the data foundation for the DBT framework has been established, the data processing steps in the database can be initiated using   DBT scripts. (described in the DBT section)</li> <li>dbt_docs   Upon completion of generating the tables in the database, a documentation of the tables and their dependencies is generated using DBT.</li> <li>dbt_docs_serve   For the visualization of the generated documentation, it is provided in the form of a website.</li> </ul>"},{"location":"concepts/showcase/#dbt-data-modeling","title":"DBT: Data modeling","text":""},{"location":"concepts/showcase/#fact_breeds_long","title":"fact_breeds_long","text":"<p>The fact table fact_breeds_long describes key figures, which are used to derive the stock of registered, living animals, divided by breeds over time.</p> <p>The following tables from the [lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cats_breeds</li> <li>cattle_breeds</li> <li>dogs_breeds</li> <li>equids_breeds</li> <li>goats_breeds</li> <li>sheep_breeds</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_beefiness_fattissue","title":"fact_cattle_beefiness_fattissue","text":"<p>The fact table fact_catle_beefiness_fattissue describes key figures, which are used to derive the number of slaughtered cows by year and month. Classification is done according to CH-TAX (Trading Class Classification CHTAX System | VIEGUT AG)</p> <p>The following tables from the [lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cattle_evolbeefiness</li> <li>cattle_evolfattissue</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_popvariations","title":"fact_cattle_popvariations","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the increase and decrease of the cattle population in the Animal Traffic Database (https://www.agate.ch/) over time (including reports from Liechtenstein). The key figures are grouped according to the following types of reports:</p> <ul> <li>Birth</li> <li>Slaughter</li> <li>Death</li> </ul> <p>The following table from the [lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_popvariations</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_pyr_wide-fact_cattle_pyr_long","title":"fact_cattle_pyr_wide\u00a0&amp;\u00a0fact_cattle_pyr_long","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the distribution of registered living cattle by age class and gender.</p> <p>The following table from the [lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_pyr</li> </ul> <p>The fact table fact_cattle_pyr_long pivots all key figures from fact_cattle_pyr_wide.</p> <p></p>"},{"location":"concepts/showcase/#superset","title":"Superset","text":""},{"location":"concepts/showcase/#database-connection","title":"Database Connection","text":"<p>The data foundation of the Superset visualizations in the form of Datasets, Dashboards, and Charts is realized through a Database Connection.</p> <p>In this case, a database connection to a database is established, which refers to a PostgreSQL database in which the above-described DBT scripts were executed.</p>"},{"location":"concepts/showcase/#datasets","title":"Datasets","text":"<p>Datasets are used to prepare the data foundation in a suitable form, which can then be visualized in charts in an appropriate way.</p> <p>Essentially, modeled fact tables from the UDM database schema are selected and linked with dimension tables.</p> <p>This allows facts to be calculated or evaluated at different levels of professional granularity.</p> <p></p>"},{"location":"concepts/showcase/#interfaces","title":"Interfaces","text":""},{"location":"concepts/showcase/#tierstatistik","title":"Tierstatistik","text":"Source Description https://tierstatistik.identitas.ch/de/ Website of the API provider https://tierstatistik.identitas.ch/de/docs.html Documentation of the platform and description of the data basis and API tierstatistik.identitas.ch/tierstatistik.rdf API and data provided by the website"},{"location":"concepts/workspaces-troubleshoot/","title":"Workspace Troubleshooting","text":""},{"location":"concepts/workspaces-troubleshoot/#kubernetes","title":"Kubernetes","text":"<p>If you haven't turned on Kubernetes, you'll get an error similar to this: <code>urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='kubernetes.docker.internal', port=6443): Max retries exceeded with url: /api/v1/namespaces/default/pods?labelSelector=dag_id%3Drun_boiler_example%2Ckubernetes_pod_operator%3DTrue%2Cpod-label-test%3Dlabel-name-test%2Crun_id%3Dmanual__2024-01-29T095915.2491840000-f3be8d87f%2Ctask_id%3Drun_duckdb_query%2Calready_checked%21%3DTrue%2C%21airflow-worker (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0xffff82c2ab10&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</code></p> <p>Full log: <pre><code>[2024-01-29, 09:48:49 UTC] {pod.py:1017} ERROR - 'NoneType' object has no attribute 'metadata'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n                       ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 363, in connect\n    self.sock = conn = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPSConnection object at 0xffff82db3650&gt;: Failed to establish a new connection: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 583, in execute_sync\n    self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 545, in get_or_create_pod\n    pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n\n....\n\n\nairflow.exceptions.AirflowException: Pod airflow-running-dagster-workspace-jdkqug7h returned a failure.\nremote_pod: None\n[2024-01-29, 09:48:49 UTC] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=run_boiler_example, task_id=run_duckdb_query, execution_date=20210501T000000, start_date=20240129T094849, end_date=20240129T094849\n[2024-01-29, 09:48:49 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 3 for task run_duckdb_query (Pod airflow-running-dagster-workspace-jdkqug7h returned a failure.\nremote_pod: None; 225)\n[2024-01-29, 09:48:49 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1\n[2024-01-29, 09:48:49 UTC] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check\n</code></pre></p>"},{"location":"concepts/workspaces-troubleshoot/#docker-image-not-build-locally-or-missing","title":"Docker image not build locally or missing","text":"<p>If your name or image is not available locally (check <code>docker image ls</code>), you'll get an error on Airflow like this:</p> <pre><code>[2024-01-29, 10:10:14 UTC] {pod.py:961} INFO - Building pod airflow-running-dagster-workspace-64ngbudj with labels: {'dag_id': 'run_boiler_example', 'task_id': 'run_duckdb_query', 'run_id': 'manual__2024-01-29T101013.7029880000-328a76b5e', 'kubernetes_pod_operator': 'True', 'try_number': '1'}\n[2024-01-29, 10:10:14 UTC] {pod.py:538} INFO - Found matching pod airflow-running-dagster-workspace-64ngbudj with labels {'airflow_kpo_in_cluster': 'False', 'airflow_version': '2.7.1-astro.1', 'dag_id': 'run_boiler_example', 'kubernetes_pod_operator': 'True', 'pod-label-test': 'label-name-test', 'run_id': 'manual__2024-01-29T101013.7029880000-328a76b5e', 'task_id': 'run_duckdb_query', 'try_number': '1'}\n[2024-01-29, 10:10:14 UTC] {pod.py:539} INFO - `try_number` of task_instance: 1\n[2024-01-29, 10:10:14 UTC] {pod.py:540} INFO - `try_number` of pod: 1\n[2024-01-29, 10:10:14 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:15 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:16 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:17 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:10:18 UTC] {pod_manager.py:348} WARNING - Pod not yet started: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:12:15 UTC] {pod.py:823} INFO - Deleting pod: airflow-running-dagster-workspace-64ngbudj\n[2024-01-29, 10:12:15 UTC] {taskinstance.py:1935} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 594, in execute_sync\n    self.await_pod_start(pod=self.pod)\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 556, in await_pod_start\n    self.pod_manager.await_pod_start(pod=pod, startup_timeout=self.startup_timeout_seconds)\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py\", line 354, in await_pod_start\n    raise PodLaunchFailedException(msg)\nairflow.providers.cncf.kubernetes.utils.pod_manager.PodLaunchFailedException: Pod took longer than 120 seconds to start. Check the pod events in kubernetes to determine why.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 578, in execute\n    return self.execute_sync(context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 617, in execute_sync\n    self.cleanup(\n  File \"/usr/local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 746, in cleanup\n    raise AirflowException(\nairflow.exceptions.AirflowException: Pod airflow-running-dagster-workspace-64ngbudj returned a failure.\n\n\n...\n\n[2024-01-29, 10:12:15 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1\n[2024-01-29, 10:12:15 UTC] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check\n</code></pre> <p>If you open a kubernetes Monitoring tool such as Lens or k9s, you'll also see the pod struggling to pull the image:</p> <p></p> <p>Another cause, in case you haven't created the local PersistentVolume, you'd see something like \"my-pvc\" does not exist. Then you'd need to create the pvc first.</p>"},{"location":"concepts/workspaces/","title":"Data Engineering Workspaces","text":"<p>On this page, we'll explain what workspaces in the context of HelloDATA-BE are and how to use them, and you'll create your own based on a prepared starter repo.</p> <p>Info</p> <p>Also see the step-by-step video we created that might help you further.</p>"},{"location":"concepts/workspaces/#what-is-a-workspace","title":"What is a Workspace?","text":"<p>Within the context of HelloDATA-BE, data, engineers, or technical people can\u00a0develop their dbt, airflow, or even bring their tool, all packed into a separate git-repo and run as part of HelloDATA-BE where they enjoy the benefits of persistent storage, visualization tools, user management, monitoring, etc.</p> <p><pre><code>graph TD\n    subgraph \"Business Domain (Tenant)\"\n        BD[Business Domain]\n        BD --&gt;|Services| SR1[Portal]\n        BD --&gt;|Services| SR2[Orchestration]\n        BD --&gt;|Services| SR3[Lineage]\n        BD --&gt;|Services| SR5[Database Manager]\n        BD --&gt;|Services| SR4[Monitoring &amp; Logging]\n    end\n    subgraph \"Workspaces\"\n        WS[Workspaces] --&gt;|git-repo| DE[Data Engineering]\n        WS[Workspaces] --&gt;|git-repo| ML[ML Team]\n        WS[Workspaces] --&gt;|git-repo| DA[Product Analysts]\n        WS[Workspaces] --&gt;|git-repo| NN[...]\n    end\n    subgraph \"Data Domain (1-n)\"\n        DD[Data Domain] --&gt;|Persistent Storage| PG[Postgres]\n        DD[Data Domain] --&gt;|Data Modeling| DBT[dbt]\n        DD[Data Domain] --&gt;|Visualization| SU[Superset]\n    end\n\n    BD --&gt;|Contains 1-n| DD\n    DD --&gt;|n-instances| WS\n\n    %% Colors\n    class BD business\n    class DD data\n    class WS workspace\n    class SS,PGA subsystem\n    class SR1,SR2,SR3,SR4 services\n\n    classDef business fill:#96CD70,stroke:#333,stroke-width:2px;\n    classDef data fill:#A898D8,stroke:#333,stroke-width:2px;\n    classDef workspace fill:#70AFFD,stroke:#333,stroke-width:2px;\n    %% classDef subsystem fill:#F1C40F,stroke:#333,stroke-width:2px;\n    %% classDef services fill:#E74C3C,stroke:#333,stroke-width:1px;</code></pre> A schematic overview of workspaces are embedded into HelloDATA-BE.</p> <p>A workspace can have n-instances within a data domain. What does it mean? Each team can deal with its requirements to develop and build their project independently.</p> <p>Think of an ML engineer who needs heavy tools such as Tensorflow, etc., as an analyst might build simple dbt models. In contrast, another data engineer uses a specific tool from the Modern Data Stack.</p>"},{"location":"concepts/workspaces/#when-to-use-workspaces","title":"When to use Workspaces","text":"<p>Workspaces are best used for development, implementing custom business logic, and modeling your data. But there is no limit to what you build as long as it can be run as a DAG as an Airflow data pipeline.</p> <p>Generally speaking, a workspace is used whenever someone needs to create a custom logic yet to be integrated within the HelloDATA BE Platform.</p> <p>As a second step - imagine you implemented a critical business transformation everyone needs - that code and DAG could be moved and be a default DAG within a data domain. But the development always happens within the workspace, enabling self-serve.</p> <p>Without workspaces, every request would need to go over the HelloDATA BE Project team. Data engineers need a straightforward way isolated from deployment where they can add custom code for their specific data domain pipelines.</p>"},{"location":"concepts/workspaces/#how-does-a-workspace-work","title":"How does a Workspace work?","text":"<p>When you create your workspace, it will be deployed within HelloDATA-BE and run by an Airflow DAG. The Airflow DAG is the integration into HD. You'll define things like how often it runs, what it should run, the order of it, etc.</p> <p>Below, you see an example of two different Airflow DAGs deployed from two different Workspaces (marked red arrow): </p>"},{"location":"concepts/workspaces/#how-do-i-create-my-own-workspace","title":"How do I create my own Workspace?","text":"<p>To implement your own Workspace, we created a hellodata-be-workspace-starter. This repo contains a minimal set of artefacts in order to be deployed on HD.</p>"},{"location":"concepts/workspaces/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Install latest Docker Desktop</li> <li>Activate Kubernetes feature in Docker Desktop (needed to run Airflow DAG as an Docker-Image): <code>Settings -&gt; Kubernetes -&gt; Enable Kubernetes</code></li> </ul>"},{"location":"concepts/workspaces/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Clone hellodata-be-workspace-starter.</li> <li>Add your own custom logic to the repo, update Dockerfile with relevant libraries and binaries you need.</li> <li>Create one or multiple Airflow DAGs for running within HelloDATA-BE.</li> <li>Build the image with <code>docker build -t hellodata-ws-boilerplate:0.1.0-a.1 .</code> (or the name of choice)</li> <li>Start up Airflow locally with Astro CLI (see more below) and run/test the pipeline</li> <li>Define needed ENV-Variables and deployments needs (to be set-up by HD-Team initially once)</li> <li>Push the image to a DockerHub of choice</li> <li>Ask HD Team to deploy initially</li> </ol> <p>From now on whenever you have a change, you just build a new image and that will be deployed on HelloDATA-BE automatically. Making you and your team independent.</p>"},{"location":"concepts/workspaces/#boiler-plate-example","title":"Boiler-Plate Example","text":"<p>Below you find an example structure that help you understand how to configure workspaces for your needs.</p>"},{"location":"concepts/workspaces/#boiler-plate-repo","title":"Boiler-Plate repo","text":"<p>The repo helps you to build your workspace by simply clone the whole repo and adding your changes.</p> <p>We generally have these boiler plate files: <pre><code>\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 build-and-push.sh\n\u251c\u2500\u2500 deployment\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 deployment-needs.yaml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 airflow\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 .astro\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Dockerfile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Makefile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 README.md\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 airflow_settings.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .airflowignore\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 boiler-example.py\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 include\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 .kube\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 packages.txt\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 plugins\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 duckdb\n        \u2514\u2500\u2500 query_duckdb.py\n</code></pre></p>"},{"location":"concepts/workspaces/#important-files-business-logic-dag","title":"Important files: Business logic (DAG)","text":"<p>Where as <code>query_duckdb.py</code> and the <code>boiler-example.py</code> DAG are in this case are my custom code that you'd change with your own code. </p> <p>Although the Airflow DAG can be re-used as we use <code>KubernetesPodOperator</code> that works works within HD and locally (check more below). Essentially you change the name and the schedule to your needs, the image name and your good to go.</p> <p>Example of a Airflow DAG: <pre><code>from pendulum import datetime\nfrom airflow import DAG\nfrom airflow.configuration import conf\nfrom airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (\n    KubernetesPodOperator,\n)\nfrom kubernetes.client import models as k8s\nimport os\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_past\": False,\n    \"start_date\": datetime(2021, 5, 1),\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"retries\": 1,\n}\n\nworkspace_name = os.getenv(\"HD_WS_BOILERPLATE_NAME\", \"ws-boilerplate\")\nnamespace = os.getenv(\"HD_NAMESPACE\", \"default\")\n\n# This will use .kube/config for local Astro CLI Airflow and ENV variable for k8s deployment\nif namespace == \"default\":\n    config_file = \"include/.kube/config\"  # copy your local kube file to the include folder: `cp ~/.kube/config include/.kube/config`\n    in_cluster = False\nelse:\n    in_cluster = True\n    config_file = None\n\nwith DAG(\n    dag_id=\"run_boiler_example\",\n    schedule=\"@once\",\n    default_args=default_args,\n    description=\"Boiler Plate for running a hello data workspace in airflow\",\n    tags=[workspace_name],\n) as dag:\n    KubernetesPodOperator(\n        namespace=namespace,\n        image=\"my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\",\n        image_pull_secrets=[k8s.V1LocalObjectReference(\"regcred\")],\n        labels={\"pod-label-test\": \"label-name-test\"},\n        name=\"airflow-running-dagster-workspace\",\n        task_id=\"run_duckdb_query\",\n        in_cluster=in_cluster,  # if set to true, will look in the cluster, if false, looks for file\n        cluster_context=\"docker-desktop\",  # is ignored when in_cluster is set to True\n        config_file=config_file,\n        is_delete_operator_pod=True,\n        get_logs=True,\n        # please add/overwrite your command here\n        cmds=[\"/bin/bash\", \"-cx\"],\n        arguments=[\n            \"python query_duckdb.py &amp;&amp; echo 'Query executed successfully'\",  # add your command here\n        ],\n    )\n</code></pre></p>"},{"location":"concepts/workspaces/#dag-how-to-test-or-run-a-dag-locally-before-deploying","title":"DAG: How to test or run a DAG locally before deploying","text":"<p>To run locally, the easiest way is to use the Astro CLI (see link for installation). With it, we can simply <code>astro start</code> or <code>astro stop</code> to start up/down.</p> <p>For local deployment we have these requirements:</p> <ul> <li>Local Docker installed (either native or Docker-Desktop)</li> <li>make sure Kubernetes is enabled</li> <li>copy you local kube-file to astro: <code>cp ~/.kube/config src/dags/airflow/include/.kube/</code></li> <li>attention, under Windows you find that file most probably under: <code>C:\\Users\\[YourIdHere]\\.kube\\config</code> </li> <li>make sure docker image is available locally (for Airflow to use it) -&gt; <code>docker build</code> must have run (check with <code>docker image ls</code></li> </ul> <p>The <code>config</code> file is used from astro to run on local Kubernetes. Se more infos on Run your Astro project in a local Airflow environment.</p>"},{"location":"concepts/workspaces/#install-requirements-dockerfile","title":"Install Requirements: <code>Dockerfile</code>","text":"<p>Below is the example how to install requirements (here <code>duckdb</code>) and copy my custom code <code>src/duckdb/query_duckdb.py</code> to the image.</p> <p>Boiler-plate example: <pre><code>FROM python:3.10-slim\n\nRUN mkdir -p /opt/airflow/airflow_home/dags/\n\n# Copy your airflow DAGs which will be copied into bussiness domain Airflow (These DAGs will be executed by Airflow)\nCOPY ../src/dags/airflow/dags/* /opt/airflow/airflow_home/dags/\n\nWORKDIR /usr/src/app\n\nRUN pip install --upgrade pip\n\n# Install DuckDB (example - please add your own dependencies here)\nRUN pip install duckdb\n\n# Copy the script into the container\nCOPY src/duckdb/query_duckdb.py ./\n\n# long-running process to keep the container running \nCMD tail -f /dev/null\n</code></pre></p>"},{"location":"concepts/workspaces/#deployment-deployment-needsyaml","title":"Deployment: <code>deployment-needs.yaml</code>","text":"<p>Below you see an an example of a deployment needs in <code>deployment-needs.yaml</code>, that defines:</p> <ul> <li>Docker image</li> <li>Volume mounts you need</li> <li>a command to run </li> <li>container behaviour</li> <li>extra ENV variables and values that HD-Team needs to provide for you</li> </ul> <p>This part is the one that will change most likely</p> <p>All of which will be eventually more automated. Also let us know or just add missing specs to the file and we'll add the functionallity on the deployment side. </p> <pre><code>spec:\n  initContainers:\n    copy-dags-to-bd:\n      image:\n        repository: my-docker-registry.com/hellodata-ws-boilerplate\n        pullPolicy: IfNotPresent\n        tag: \"0.1.0\"\n      resources: {}\n\n      volumeMounts:\n        - name: storage-hellodata\n          type: external\n          path: /storage\n      command: [ \"/bin/sh\",\"-c\" ]\n      args: [ \"mkdir -p /storage/${datadomain}/dags/${workspace}/ &amp;&amp; rm -rf /storage/${datadomain}/dags/${workspace}/* &amp;&amp; cp -a /opt/airflow/airflow_home/dags/*.py /storage/${datadomain}/dags/${workspace}/\" ]\n\n  containers:\n    - name: ws-boilerplate\n      image: my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\n      imagePullPolicy: Always\n\n\n#needed envs for Airflow\nairflow:\n\n  extraEnv: |\n    - name: \"HD_NAMESPACE\"\n      value: \"${namespace}\"\n    - name: \"HD_WS_BOILERPLATE_NAME\"\n      value: \"dd01-ws-boilerplate\"\n</code></pre>"},{"location":"concepts/workspaces/#example-with-airflow-and-dbt","title":"Example with Airflow and dbt","text":"<p>We've added another demo dag called <code>showcase-boiler.py</code> which is an DAG that download data from the web (animal statistics, ~150 CSVs), postgres tables are created, data inserted and a dbt run and docs is ran at the end.</p> <p></p> <p>In this case we use multiple task in a DAG, these have all the same image, but you could use different one for each step. Meaning you could use Python for download, R for transformatin and Java for machine learning. But as long as images are similar, I'd suggest to use the same image.</p>"},{"location":"concepts/workspaces/#volumes-pvc","title":"Volumes / PVC","text":"<p>Another addition is the use of voulmes. These are a persistent storage also called <code>pvs</code> in Kubernetes, which allow to store intermediate storage outside of the container. Downloaded CSVs are stored there for the next task to pick up from that storage.</p> <p>Locally you need to create such a storage once, there is a script in case you want to apply it to you local Docker-Desktop setup. Run this command: <pre><code>kubectl apply -f src/volume_mount/pvc.yaml\n</code></pre></p> <p>Be sure to use the same name, in this example we use <code>my-pvc</code> in your DAGs as well. See in the <code>showcase-boiler.py</code> how the volumnes are mounted like this: <pre><code>volume_claim = k8s.V1PersistentVolumeClaimVolumeSource(claim_name=\"my-pvc\")\nvolume = k8s.V1Volume(name=\"my-volume\", persistent_volume_claim=volume_claim)\nvolume_mount = k8s.V1VolumeMount(name=\"my-volume\", mount_path=\"/mnt/pvc\")\n</code></pre></p>"},{"location":"concepts/workspaces/#conclusion","title":"Conclusion","text":"<p>I hope this has illustrated how to create your own workspace. Otherwise let us know in the discussions or create an issue/PR.</p>"},{"location":"concepts/workspaces/#troubleshooting","title":"Troubleshooting","text":"<p>If you enconter errors, we collect them in Troubleshooting.</p>"},{"location":"manuals/role-authorization-concept/","title":"Roles and authorization concept","text":""},{"location":"manuals/role-authorization-concept/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>Authentication and authorizations within the various logical contexts or domains of the HelloDATA system are handled as follows.\u00a0 Authentication is handled via the OAuth 2 standard. In the case of the Canton of Bern, this is done via the central KeyCloak server. Authorizations to the various elements within a subject or Data Domain are handled via authorization within the HelloDATA portal. To keep administration simple, a role concept is applied. Instead of defining the authorizations for each user, roles receive the authorizations and the users are then assigned to the roles. The roles available in the portal have fixed defined permissions.</p>"},{"location":"manuals/role-authorization-concept/#business-domain","title":"Business Domain","text":"<p>In order for a user to gain access to a Business Domain, the user must be authenticated for the Business Domain. Users without authentication who try to access a Business Domain will receive an error message. The following two logical roles are available within a Business Domain:</p> <ul> <li>HELLODATA_ADMIN</li> <li>BUSINESS_DOMAIN_ADMIN</li> </ul>"},{"location":"manuals/role-authorization-concept/#hellodata_admin","title":"HELLODATA_ADMIN","text":"<ul> <li>Can act fully in the system.</li> </ul>"},{"location":"manuals/role-authorization-concept/#business_domain_admin","title":"BUSINESS_DOMAIN_ADMIN","text":"<ul> <li>Can manage users and assign roles (except HELLODATA_ADMIN).</li> <li>Can manage dashboard metadata.</li> <li>Can manage announcements.</li> <li>Can manage the FAQ.</li> <li>Can manage the external documentation links.</li> </ul> <p>BUSINESS_DOMAIN_ADMIN is automatically DATA_DOMAIN_ADMIN in all Data Domains within the Business Domain (see Data Domain Context).</p>"},{"location":"manuals/role-authorization-concept/#data-domain","title":"Data Domain","text":"<p>A Data Domain encapsulates all data elements and tools that are of interest for a specific issue. HalloDATA supports 1 - n Data Domains within a Business Domain.</p> <p>The resources to be protected within a Data Domain are:</p> <ul> <li>Schema of the Data Domain.</li> <li>Data mart tables of the Data Domain.</li> <li>The entire DWH environment of the Data Domain.</li> <li>Data lineage documents of the DBT projects of the Data Domain.</li> <li>Dashboards, charts, datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> </ul> <p>The following three logical roles are available within a Data Domain:</p> <ul> <li>DATA_DOMAIN_VIEWER \u00a0 \u00a0</li> <li>DATA_DOMAIN_EDITOR</li> <li>DATA_DOMAIN_ADMIN</li> </ul> <p>Depending on the role assigned, users are given different permissions to act in the Data Domain. A user who has not been assigned a role in a Data Domain will generally not be granted access to any resources of that Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#data_domain_viewer","title":"DATA_DOMAIN_VIEWER","text":"<ul> <li>The DATA_DOMAIN_VIEWER role is granted potential read access to dashboards of a Data Domain.</li> <li>Which dashboards of the Data Domain a DATA_DOMAIN_VIEWER user is allowed to see is administered within the user management of the HelloDATA portal.</li> <li>Only assigned dashboards are visible to a DATA_DOMAIN_VIEWER.</li> <li>Only dashboards in \"Published\" status are visible to a DATA_DOMAIN_VIEWER. A DATA_DOMAIN_VIEWER can view all data lineage documents of the Data Domain.</li> <li>A DATA_DOMAIN_VIEWER can access the links to external dashboards associated with its Data Domain. It is not checked whether the user has access in the systems outside the HelloDATA system boundary.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_editor","title":"DATA_DOMAIN_EDITOR","text":"<p>Same as DATA_DOMAIN_VIEWER plus:</p> <ul> <li>The DATA_DOMAIN_EDITOR role is granted read and write access to the dashboards of a Data Domain. All dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All charts used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All data sets used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR.</li> <li>A DATA_DOMAIN_EDITOR can create new dashboards.</li> <li>A DATA_DOMAIN_EDITOR can view the data marts of the Data Domain.</li> <li>A DATA_DOMAIN_EDITOR has access to the SQL lab in the superset.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_admin","title":"DATA_DOMAIN_ADMIN","text":"<p>Same as DATA_DOMAIN_EDITOR plus:</p> <p>The DATA_DOMAIN_ADMIN role can view the airflow DAGs of the Data Domain. A DATA_DOMAIN_ADMIN can view all database objects in the DWH of the Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#extra-data-domain","title":"Extra Data Domain","text":"<p>Beside the standard Data Domains there are also extra Data Domains An Extra Data Domain provides additional permissions, functions and database connections such as :</p> <ul> <li>CSV uploads to the Data Domain.</li> <li>Read permissions from one Data Domain to additional other Data Domain(s).</li> <li>Database connections to Data Domains of other databases.</li> <li>Database connections via AD group permissions.</li> <li>etc.</li> </ul> <p>These additional permissions, functions or database connections are a matter of negotiation per extra Data Domain. The additional permissions, if any, are then added to the standard roles mentioned above for the extra Data Domain.</p> <p>Row Level Security settings on Superset level can be used to additionally restrict the data that is displayed in a dashboard (e.g. only data of the own domain is displayed).</p>"},{"location":"manuals/role-authorization-concept/#system-role-to-portal-role-mapping","title":"System Role to Portal Role Mapping","text":"System Role Portal Role Portal Permission Menu / Submenu / Page in Portal Info HELLODATA_ADMIN SUPERUSER ROLE_MANAGEMENT Administration / Portal Rollenverwaltung MONITORING Monitoring DEVTOOLS Dev Tools USER_MANAGEMENT Administration / Benutzerverwaltung FAQ_MANAGEMENT Administration / FAQ Verwaltung EXTERNAL_DASHBOARDS_MANAGEMENT Unter External Dashboards Kann neue Eintr\u00e4ge erstellen und verwalten bei Seite External Dashboards DOCUMENTATION_MANAGEMENT Administration / Dokumentationsmanagement ANNOUNCEMENT_MANAGEMENT Administration/ Ank\u00fcndigungen DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration BUSINESS_DOMAIN_ADMIN BUSINESS_DOMAIN_ADMIN USER_MANAGEMENT Administration / Portal Rollenverwaltung FAQ_MANAGEMENT Dev Tools EXTERNAL_DASHBOARDS_MANAGEMENT Administration / Benutzerverwaltung DOCUMENTATION_MANAGEMENT Administration / FAQ Verwaltung ANNOUNCEMENT_MANAGEMENT Unter External Dashboards DASHBOARDS Administration / Dokumentationsmanagement Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Administration/ Ank\u00fcndigungen Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_ADMIN DATA_DOMAIN_ADMIN DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_EDITOR EDITOR DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DOMAIN_VIEWER VIEWER DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat"},{"location":"manuals/role-authorization-concept/#system-role-to-superset-role-mapping","title":"System Role to Superset Role Mapping","text":"System Role Superset Role Info No Data Domain role Public User should not get access to Superset functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER BI_VIEWER plus\u00a0roles forDashboards he was granted access to i. e. the slugified dashboard names with prefix \"D_\" Example: User is \"DATA_DOMAIN_VIEWER\" in a Data Domain. We grant the user acces to the \"Hello World\" dashboard. Then user gets the role \"BI_VIEWER\" plus the role \"D_hello_world\" in Superset. DATA_DOMAIN_EDITOR BI_EDITOR Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions. DATA_DOMAIN_ADMIN BI_EDITOR plus\u00a0BI_ADMIN Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions."},{"location":"manuals/role-authorization-concept/#system-role-to-airflow-role-mapping","title":"System Role to Airflow Role Mapping","text":"System Role Airflow Role Info HELLO_DATA_ADMIN Admin User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles.User additionally gets the Admin role. BUSINESS_DOMAIN_ADMIN User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles. No Data Domain role Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_EDITOR Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_ADMIN AF_OPERATOR plus\u00a0role corresponding to his Data Domain Key with prefix \"DD_\" Example: User is \"DATA_DOMAIN_ADMIN\" in a Data Domain with the key \"data_domain_one\". Then user gets the role \"AF_OPERATOR\" plus the role \"DD_data_domain_one\" in Airflow."},{"location":"manuals/user-manual/","title":"User Manual","text":""},{"location":"manuals/user-manual/#goal","title":"Goal","text":"<p>This use manual should enable you to use the HelloDATA platform and illustrate the features of the product and how to use them.</p> <p>\u2192 More about the Platform and its architecture you can find onArchitecture &amp; Concepts.</p>"},{"location":"manuals/user-manual/#navigation","title":"Navigation","text":""},{"location":"manuals/user-manual/#portal","title":"Portal","text":"<p>The entry page of HelloDATA is the Web Portal.</p> <ol> <li>Navigation to jump to the different capabilities of HelloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in user.4. Overview of your dashboards</li> </ol> <p></p>"},{"location":"manuals/user-manual/#business-data-domain","title":"Business &amp; Data Domain","text":"<p>As explained in Domain View, a key feature is to create business domains with n-data domains. If you have access to more than one data domain, you can switch between them by clicking the <code>drop-down</code> at the top and switch between them.</p> <p></p>"},{"location":"manuals/user-manual/#dashboards","title":"Dashboards","text":"<p>The most important navigation button is the dashboard links. If you hover over it, you'll see three options to choose from.</p> <p>You can either click the dashboard list in the hover menu (2) to see the list of dashboards with thumbnails, or directly choose your dashboard (3).</p> <p></p>"},{"location":"manuals/user-manual/#data-lineage","title":"Data-Lineage","text":"<p>To see the data lineage (dependencies of your data tables), you have the second menu option. Again, you chose the list or directly on \"data lineage\" (2).</p> <p>Button 2 will bring you to the project site, where you choose your project and load the lineage. </p> <p>Once loaded, you see all sources (1) and dbt Projects (2). On the detail page, you can see all the beautiful and helpful documentation such as:</p> <ul> <li>table name (3)</li> <li>columns and data types (4)</li> <li>which table and model this selected object depends on (5)</li> <li>the SQL code (6)<ul> <li>as a template or complied</li> </ul> </li> <li>and dependency graph (7)<ul> <li>which you can expand to full view (8) after clicking (7)</li> <li>interactive data lineage view (9)</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#data-marts-viewer","title":"Data Marts Viewer","text":"<p>This view let's you access the universaal data mart (udm) layer:</p> <p></p> <p>These are cleaned and modeled data mart tables. Data marts are the tables that have been joined and cleaned from the source tables. This is effectively the latest layer of HelloDATA BE, which the Dashboards are accessing. Dashboards should not access any layer before (landing zone, data storage, or data processing).</p> <p>We use CloudBeaver for this, same as the DWH Viewer later. </p>"},{"location":"manuals/user-manual/#data-engineering","title":"Data Engineering","text":""},{"location":"manuals/user-manual/#dwh-viewer","title":"DWH Viewer","text":"<p>This is essentially a database access layer where you see all your tables, and you can write SQL queries based on your access roles with a provided tool (CloudBeaver).</p>"},{"location":"manuals/user-manual/#create-new-sql-query","title":"Create new SQL Query","text":"<p>o</p>"},{"location":"manuals/user-manual/#choose-connection-and-stored-queries","title":"Choose Connection and stored queries","text":"<p>You can chose pre-defined connections and query your data warehouse. Also you can store queries that other user can see and use as well. Run your queries with (1).</p> <p></p>"},{"location":"manuals/user-manual/#settings-and-powerful-features","title":"Settings and Powerful features","text":"<p>You can set many settings, such as user status, and many more.</p> <p> Please find all setting and features in the CloudBeaver Documentation.</p>"},{"location":"manuals/user-manual/#orchestration","title":"Orchestration","text":"<p>The orchestrator is your task manager. You tell Airflow, our orchestrator, in which order the task will run. This is usually done ahead of time, and in the portal, you can see the latest runs and their status (successful, failed, etc.).</p> <ul> <li>You can navigate to DAGs (2) and see all the details (3) with the DAG name, owner, runs, schedules, next run and   recent.</li> <li>You can also dive deeper into Datasets, Security, Admin or similar (4)</li> <li>Airflow offers lots of different visualization modes, e.g. the Graph view (6), that allows you to see each step of   this task.<ul> <li>As you can see, you can choose calendar, task duration, Gantt, etc.</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#helper-library-for-scheduling-jobs-on-kubernetes","title":"Helper Library for Scheduling Jobs on Kubernetes","text":"<p>To unlock the full power of airflow on kubernetes, you will need to run your jobs in containers on the cluster. To make this a bit easier, we provide a (preinstalled helper library)[https://github.com/bedag/hellodata-be-airflow-pod-operator-params] for you to use.</p>"},{"location":"manuals/user-manual/#library-description","title":"Library description","text":"<p>The helper library consists mainly of a function, that returns properly formatted parameters to use with (airflows kubernetes pod operator)[https://airflow.apache.org/docs/apache-airflow/1.10.10/_api/airflow/contrib/operators/kubernetes_pod_operator/index.html]. It is named <code>hellodata_be_airflow_pod_operator_params</code> and can be imported with <code>import hellodata_be_airflow_pod_operator_params</code>. The two public objects are the function <code>get_pod_operator_params</code> and the class <code>EphemeralVolume</code>.</p> <p>Call the function <code>get_pod_operator_params</code> to get a dictionary with parameters to be passed to <code>kubernetes_pod_operator</code>.</p> Parameter Type Required Default Description <code>image</code> <code>str</code> <code>true</code> - The Docker image to use for the pod <code>namespace</code> <code>str</code> <code>false</code> <code>\"default\"</code> The Kubernetes namespace in which to create the pod <code>image_pull_secrets</code> <code>Optional[List[str]]</code> <code>false</code> <code>None</code> List of image pull secrets for private registries <code>secrets</code> <code>Optional[List[str]]</code> <code>false</code> <code>None</code> List of Kubernetes secret names to mount in the pod as environment variables <code>configmaps</code> <code>Optional[List[str]]</code> <code>false</code> <code>None</code> List of Kubernetes configmap names to mount in the pod as environment variables <code>cpus</code> <code>float</code> <code>false</code> <code>1.0</code> Number of CPU cores to allocate to the pod <code>memory_in_Gi</code> <code>float</code> <code>false</code> <code>1.0</code> Amount of memory in GiB to allocate to the pod <code>local_ephemeral_storage_in_Gi</code> <code>float</code> <code>false</code> <code>1.0</code> Amount of local ephemeral storage in GiB to allocate to the pod <code>startup_timeout_in_seconds</code> <code>int</code> <code>false</code> <code>120</code> Timeout in seconds for the pod to start up <code>large_ephemeral_storage_volume</code> <code>Optional[EphemeralVolume]</code> <code>false</code> <code>None</code> Large ephemeral storage volume to allocate to the pod <code>env_vars</code> <code>Optional[Dict[str, str]]</code> <code>false</code> <code>None</code> Additional environment variables to set in the pod Parameter Type Description <code>name</code> <code>str</code> The name of the ephemeral volume. <code>size_in_Gi</code> <code>float</code> The size of the volume in GiB (Gibibytes). <code>mount_path</code> <code>str</code> The file system path inside the pod where the volume is mounted. <code>storage_class</code> <code>str</code> The Kubernetes storage class to use for provisioning the volume."},{"location":"manuals/user-manual/#example-usage","title":"Example usage","text":"<p>The following python code contains an Airflow DAG that makes full usage of the library to schedule a pod on airflow.</p> <pre><code>from datetime import timedelta\nfrom pendulum import datetime\nfrom airflow import DAG\nfrom airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (\n    KubernetesPodOperator,\n)\n\nfrom hellodata_be_airflow_pod_operator_params import (\n    get_pod_operator_params,\n    EphemeralVolume,\n)  # library import\n\noperator_params = get_pod_operator_params(\n    \"alpine:latest\",\n    namespace=\"my-namespace\",\n    secrets=[\"my-secret\"],\n    configmaps=[\"my-configmap\"],\n    cpus=0.5,\n    memory_in_Gi=0.5,\n    local_ephemeral_storage_in_Gi=1,\n    startup_timeout_in_seconds=10 * 60,\n    large_ephemeral_storage_volume=EphemeralVolume(\n        \"my-storage\", 5, \"/app/large_ephemeral_storage\", \"my-storage-type\"\n    ),\n    env_vars={\"key\": \"value\"},\n)\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depend_on_past\": False,\n    \"start_date\": datetime(2025, 8, 1, tz=\"Europe/Zurich\"),\n}\n\nwith DAG(\n    dag_id=\"example_dag\",\n    schedule=\"@once\",\n    default_args=default_args,\n    max_active_runs=1,\n    dagrun_timeout=timedelta(minutes=60 * 5),\n) as dag:\n\n    my_task = KubernetesPodOperator(\n        **operator_params,\n        name=\"my_task\",\n        task_id=\"my_task\",\n        arguments=[\n            \"\"\"\n    echo \"I run on kubernetes and have the following env vars\" &amp;&amp;\n    printenv\n    \"\"\"\n        ],\n    )\n</code></pre>"},{"location":"manuals/user-manual/#default-dag-hellodata-monitoring","title":"Default DAG: HelloDATA Monitoring","text":"<p>This is a DAG provided by us that gives you a summary of DAG runs. It will send you an email reporting which DAGs have failed since the monitoring DAG last ran, which have run successfully, which have not run and which are still running.</p> <p></p> <p>The email contains three sections: 1. Monitored DAGs \u2013 A table with an overview of DAG runs tagged as <code>monitored</code>. 2. Changes to DAGs \u2013 Lists DAGs that have been paused/unpaused, are new, deleted, newly monitored (added the <code>monitored</code> tag) or newly unmonitored. 3. General Overview \u2013 A table with all DAG runs.</p> <p>You can modify the behavior of the DAG using environment variables on the Airflow worker:</p> Variable Name Default Value Effect <code>MONITORING_DAG_STATE_FILE</code> <code>/opt/airflow/dag_state_cache.json</code> Path to a file where the state is saved. On Kubernetes, this could be on a PVC to ensure it persists after a pod restart. <code>MONITORING_DAG_NOTIFY_EMAIL</code> <code>moiraine@tarvalon.org,rand.althor@aielwaste.net</code> Comma-separated list of email addresses to send the report to. Airflow's mail server settings are used for sending the email. <code>MONITORING_DAG_AIRFLOW_LINK</code> <code>your administrator has forgotten to set the MONITORING_DAG_AIRFLOW_LINK env variable</code> Value used to generate direct links to the DAG runs. <code>MONITORING_DAG_INSTANCE_NAME</code> <code>HelloDATA</code> Used to generate the email title: <code>&lt;MONITORING_DAG_INSTANCE_NAME&gt; monitoring, &lt;date and time&gt; - DAG monitoring report</code>. <code>MONITORING_DAG_RUNTIME_SCHEDULE</code> <code>0 5 * * *</code> Cron expression for when to run the DAG."},{"location":"manuals/user-manual/#prebuilt-mechanism-for-logging-dag-runs","title":"Prebuilt Mechanism for logging DAG runs","text":"<p>HelloDATA offers an easy way to log your DAG run stats to your DWH database through the <code>log_dag_run</code> function in the preinstalled python package <code>hellodata_be_dag_logs</code>. The library is opensource, of course. You can have a look at the code on GitHub.</p>"},{"location":"manuals/user-manual/#log_dag_run-function","title":"<code>log_dag_run</code> function","text":"<p>The <code>log_dag_run</code> function logs statistics for all tasks in the current Airflow DAG run, excluding any specified task IDs, and inserts this data into a database table. This helps in monitoring and analyzing DAG performance over time.</p> <p>Return type: <code>None</code> The function does not return a value; it performs logging and database insertion as side effects.</p> <p>These are the input parameters.</p> Parameter Type Default Value Usage <code>kwargs</code> <code>dict[str, Any]</code> required Airflow context dictionary containing information about the current DAG run. <code>exclude_task_ids</code> <code>list[str]</code> <code>[]</code> List of task IDs to exclude from logging and database insertion. <code>connection_id</code> <code>str</code> <code>\"default_connection\"</code> Airflow connection ID used to connect to the target database. <code>schema_name</code> <code>str</code> <code>\"public\"</code> Name of the database schema where the task statistics table resides. <code>table_name</code> <code>str</code> <code>\"dag_runs_stats\"</code> Name of the table where task statistics will be inserted."},{"location":"manuals/user-manual/#notes","title":"Notes","text":"<ul> <li>Ensure your Airflow connection (<code>connection_id</code>) is correctly configured for your database.</li> <li>The function should be called after the DAG run to capture accurate statistics.</li> <li>Adjust parameters as needed for your environment.</li> <li>You might want to ignore the task that logs the stats for the logs (see parameter <code>exclude_task_ids</code>)</li> </ul>"},{"location":"manuals/user-manual/#example-usage_1","title":"Example usage","text":"<p>The following example demonstrates how to use the <code>log_dag_run</code> function within an Airflow DAG. It defines several simple tasks and a logging task that records DAG run statistics to a database. The logging task is configured to run after the main tasks, ensuring that all relevant information is captured.</p> <pre><code>import pendulum\nfrom airflow.decorators import dag, task\nfrom hellodata_be_dag_logs import log_dag_run\n\n@dag(\n    schedule=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example\"],\n)\ndef tutorial_taskflow_api():\n    @task(task_id=\"dt1\")\n    def dt1():\n        return \"Some dummy task 1\"\n\n    @task(task_id=\"dt2\")\n    def dt2():\n        return \"Some dummy task 2\"\n\n    @task(task_id=\"dt3\")\n    def dt3():\n        return \"Some dummy task 3\"\n\n    @task(task_id=\"dt4\")\n    def dt4():\n        return \"Some dummy task 4\"\n\n    @task(task_id=\"hd_log_dag_run\", provide_context=True)\n    def log_stats(**kwargs):\n        log_dag_run(\n            kwargs,\n            connection_id=\"your-connection-id\",\n            schema_name=\"udm\",\n            table_name=\"dag_run_stats\",\n            exclude_task_ids=[\"hd_log_dag_run\"],\n        )\n\n    dt1_task = dt1()\n    dt2_task = dt2()\n    dt3_task = dt3()\n    dt4_task = dt4()\n    print_context_task = log_stats()\n    dt1_task &gt;&gt; dt2_task\n    [dt2_task, dt3_task] &gt;&gt; print_context_task\n\ntutorial_taskflow_api()\n</code></pre>"},{"location":"manuals/user-manual/#jupyter-notebooks-jupyter-hub","title":"Jupyter Notebooks (Jupyter Hub)","text":"<p>If you have one of the roles of <code>HELLODATA_ADMIN</code>, <code>BUSINESS_DOMAIN_ADMIN</code>, or <code>DATA_DOMAIN_ADMIN</code>, you can access Jupyter Hub and its notebooks with:</p> <p></p> <p>That opens up Jupyter Hub where you choose the base image you want to start with. E.g. you choose Data Science to do ML workloads, or R if you solely want to work with R. This could look like this:</p> <p></p> <p>After you can start creating notebooks with <code>File -&gt; New -&gt; Notebook</code>:</p> <p> After you choose the language (e.g. Python for Python notebooks, or R).</p> <p>After you can start running commands like you do in Jupyter Notebooks.</p> <p></p> <p>See the official documentation for help or functions.</p>"},{"location":"manuals/user-manual/#connect-to-hd-postgres-db","title":"Connect to HD Postgres DB","text":"<p>By default, a connection to your own Postgres DB can be made.</p> <p>The default session time is 24h as of now and can be changed with ENV <code>HELLODATA_JUPYTERHUB_TEMP_USER_PASSWORD_VALID_IN_DAYS</code>.</p>"},{"location":"manuals/user-manual/#how-to-connect-to-the-database","title":"How to connect to the database","text":"<p>This is how to get a db-connection:</p> <pre><code>from hello_data_scripts import connect # import the function\nconnection = connect() # use function, it fetches the temp user creds and establishes the connection\n</code></pre> <p><code>connection</code> can be used to read from postgres.</p>"},{"location":"manuals/user-manual/#example","title":"Example","text":"<p>This is a more extensive example of querying the Postgres database. Imagine <code>SELECT version();</code> as your custom query or logic you want to do.</p> <pre><code>import sys\n#import psycopg2 -&gt; this is imported through the below hello_data_scripts import\nfrom hello_data_scripts import connect  \n\n# Get the database connection\nconnection = connect()\n\nif connection is None:\n    print(\"Failed to connect to the database.\")\n    sys.exit(1)\n\ntry:\n    # Create a cursor object\n    cursor = connection.cursor()\n\n    # Example query to check the connection\n    cursor.execute(\"SELECT version();\")\n    db_version = cursor.fetchone()\n    print(f\"Connected to database. PostgreSQL version: {db_version}\")\n\nexcept psycopg2.Error as e:\n    print(f\"An error occurred while performing database operations: {e}\")\n\nfinally:\n    # Close the cursor and connection\n    cursor.close()\n    connection.close()\n    print(\"Database connection closed.\")\n</code></pre>"},{"location":"manuals/user-manual/#administration","title":"Administration","text":"<p>Here you manage the portal configurations such as user, roles, announcements, FAQs, and documentation management.</p> <p></p>"},{"location":"manuals/user-manual/#benutzerverwaltung-user-management","title":"Benutzerverwaltung / User Management","text":""},{"location":"manuals/user-manual/#adding-user","title":"Adding user","text":"<p>First type your email and hit enter. Then choose the drop down and click on it. </p> <p>Now type the Name and hit <code>Berechtigungen setzen</code> to add the user: </p> <p>You should see something like this:</p> <p></p>"},{"location":"manuals/user-manual/#changing-permissions","title":"Changing Permissions","text":"<ol> <li>Search the user you want to give or change permission</li> <li>Scroll to the right</li> <li>Click the green edit icon</li> </ol> <p>Now choose the <code>role</code> you want to give:</p> <p></p> <p>And or give access to specific data domains:</p> <p></p> <p>See more in role-authorization-concept.</p>"},{"location":"manuals/user-manual/#portal-rollenverwaltung-portal-role-management","title":"Portal Rollenverwaltung / Portal Role Management","text":"<p>In this portal role management, you can see all the roles that exist.</p> <p>Warning</p> <p>Creating new roles are not supported, despite the fact \"Rolle erstellen\" button exists. All roles are defined and hard coded.</p> <p></p>"},{"location":"manuals/user-manual/#creating-a-new-role","title":"Creating a new role","text":"<p>See how to create a new role below: </p>"},{"location":"manuals/user-manual/#ankundigung-announcement","title":"Ank\u00fcndigung / Announcement","text":"<p>You can simply create an announcement that goes to all users by <code>Ank\u00fcndigung erstellen</code>: </p> <p>Then you fill in your message. Save it.</p> <p> You'll see a success if everything went well: </p> <p>And this is how it looks to the users \u2014 It will appear until the user clicks the cross to close it. </p>"},{"location":"manuals/user-manual/#faq","title":"FAQ","text":"<p>The FAQ works the same as the announcements above. They are shown on the starting dashboard, but you can set the granularity of a data domain:</p> <p></p> <p>And this is how it looks: </p>"},{"location":"manuals/user-manual/#dokumentationsmanagement-documentation-management","title":"Dokumentationsmanagement / Documentation Management","text":"<p>Lastly, you can document the system with documentation management. Here you have one document that you can document everything in detail, and everyone can write to it. It will appear on the dashboard as well:</p> <p></p>"},{"location":"manuals/user-manual/#monitoring","title":"Monitoring","text":"<p>We provide two different ways of monitoring:</p> <ul> <li>Status:- Workspaces</li> </ul> <p></p>"},{"location":"manuals/user-manual/#status","title":"Status","text":"<p>It will show you details information on instances of HelloDATA, how is the situation for the Portal, is the monitoring running, etc. </p>"},{"location":"manuals/user-manual/#data-domains","title":"Data Domains","text":"<p>In Monitoring your data domains you see each system and the link to the native application. You can easily and quickly observer permission, roles and users by different subsystems (1). Click the one you want, and you can choose different levels (2) for each, and see its permissions (3).</p> <p></p> <p></p> <p>By clicking on the blue underlined <code>DBT Docs</code>, you will be navigated to the native dbt docs. Same is true if you click on a Airflow or Superset instance.</p>"},{"location":"manuals/user-manual/#devtools","title":"DevTools","text":"<p>DevTools are additional tools HelloDATA provides out of the box to e.g. send Mail (Mailbox) or browse files ( FileBrowser).</p> <p></p>"},{"location":"manuals/user-manual/#mailbox","title":"Mailbox","text":"<p>You can check in Mailbox (we useMailHog) what emails have been sending or what accounts are updated.|</p> <p></p>"},{"location":"manuals/user-manual/#filebrowser","title":"FileBrowser","text":"<p>Here you can browse all the documentation or code from the git repos as file browser. We use SFTPGo here. Please use with care, as some of the folder are system relevant.</p> <p>Log in</p> <p>Make sure you have the login credentials to log in. Your administrator should be able to provide these to you.</p> <p></p>"},{"location":"manuals/user-manual/#more-know-how","title":"More: Know-How","text":"<ul> <li>More help for Superset<ul> <li>Superset Documentation</li> </ul> </li> <li>More help for dbt:<ul> <li>dbt Documentation</li> <li>dbt Developer Hub</li> </ul> </li> <li>More about Airflow<ul> <li>Airflow Documentation</li> </ul> </li> <li>More about SFTPGo<ul> <li>SFTPGo Documentation</li> </ul> </li> </ul> <p>Find further important references, know-how, and best practices on HelloDATA Know-How.</p>"},{"location":"more/changelog/","title":"Changelog: HD-BE Documentation","text":""},{"location":"more/changelog/#2024-08-28-added-new-features","title":"2024-08-28 Added new features","text":"<ul> <li>Added Juypter Notebooks / Jupyter Hub on the data stack page.</li> <li>Added Data Publisher to the concept page.</li> <li>Updated User Manual related to the new features.</li> <li>Updated Roadmap.</li> </ul>"},{"location":"more/changelog/#2023-11-22-concepts","title":"2023-11-22 Concepts","text":"<ul> <li>Added workspaces on the concepts page.</li> <li>Added showcase main category to explain the demo that comes with HD-BE</li> </ul>"},{"location":"more/changelog/#2023-11-20-changed-corporate-design","title":"2023-11-20 Changed corporate design","text":"<ul> <li>Changed primary color to KAIO style guide: color red (#EE0F0F), and font: Roboto (was already default font)</li> </ul>"},{"location":"more/changelog/#2023-11-06-switched-architecture-over","title":"2023-11-06 Switched architecture over","text":"<ul> <li>Switched the architecture over to mkdocs</li> <li>Updated vision</li> <li>Updated user manual</li> </ul>"},{"location":"more/changelog/#2023-09-29-initial-version","title":"2023-09-29 Initial version","text":"<ul> <li>Created the template for documentation with mkdocs and the popular theme mkdocs-material.</li> </ul>"},{"location":"more/faq/","title":"FAQ","text":""},{"location":"more/glossary/","title":"Glossary","text":"<ul> <li>HD: HelloDATA</li> <li>KAIO: Amt f\u00fcr Informatik und Organisation des Kantons Bern (KAIO)</li> </ul>"},{"location":"test/examples/","title":"Examples","text":""},{"location":"test/examples/#code-annotation-examples","title":"Code Annotation Examples","text":""},{"location":"test/examples/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"test/examples/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"test/examples/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"test/examples/#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#admonitions-call-outs","title":"Admonitions / Call-outs","text":"<p>Note</p> <p>this is a note</p> <p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Supported types:</p> <ul> <li>note</li> <li>abstract</li> <li>info</li> <li>tip</li> <li>success</li> <li>question</li> <li>warning</li> <li>failure</li> <li>danger</li> <li>bug</li> <li>example</li> <li>quote</li> </ul>"},{"location":"test/examples/#diagrams","title":"Diagrams","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"test/examples/#sequence-diagram","title":"Sequence diagram","text":"<p>Sequence diagrams describe a specific scenario as sequential interactions between multiple objects or actors, including the messages that are exchanged between those actors:</p> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"test/examples/#state-diagram","title":"State diagram","text":"<p>State diagrams are a great tool to describe the behavior of a system, decomposing it into a finite number of states, and transitions between those states: <pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre></p>"},{"location":"test/examples/#class-diagram","title":"Class diagram","text":"<p>Class diagrams are central to object oriented programing, describing the structure of a system by modelling entities as classes and relationships between them:</p> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre>"},{"location":"test/examples/#entity-relationship-diagram","title":"Entity-relationship diagram","text":"<p>An entity-relationship diagram is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge:</p> <pre><code>erDiagram\n  CUSTOMER ||--o{ ORDER : places\n  ORDER ||--|{ LINE-ITEM : contains\n  LINE-ITEM {\n    string name\n    int pricePerUnit\n  }\n  CUSTOMER }|..|{ DELIVERY-ADDRESS : uses</code></pre>"},{"location":"test/examples/#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"vision/roadmap/","title":"Feature Roadmap HD-BE - August 2024","text":"Feature Roadmap"},{"location":"vision/vision-and-goal/","title":"Our Vision and Goal","text":"<p>The Open-Source Enterprise Data Platform in a Single Portal</p> <p>HelloDATA BE is an enterprise data platform built on top of open source. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p>"},{"location":"vision/vision-and-goal/#vision","title":"Vision","text":"<p>In a fast-moving data engineering world, where every device and entity becomes a data generator, the need for agile, robust, and transparent data platforms is more crucial. HelloDATA BE is not just any data platform; it's the bridge between open-source innovation and enterprise solutions' demanding reliability. </p> <p>HelloDATA BE handpicked the best tools like dbt, Airflow, Superset, and Postgres and integrated them into a seamless, enterprise-ready data solution. Empowering businesses with the agility of open-source and the dependability of a tested, unified platform. </p>"},{"location":"vision/vision-and-goal/#the-goal-of-hellodata-be","title":"The Goal of HelloDATA BE","text":"<p>Our goal at HelloDATA BE is clear: to democratize the power of data for enterprises. </p> <p>As digital transformation and data expand, the challenges with various SaaS solutions, vendor lock-ins, and fragmented data sources become apparent. </p> <p>HelloDATA BE trying to provide an answer to these challenges. We aim to merge the world's best open-source tools, refining them for enterprise standards ensuring that every organization, irrespective of size or niche, has access to top-tier data solutions. By fostering a community-driven approach through our open-source commitment, we envision a data future that's inclusive, robust, and open to innovation.</p>"}]}